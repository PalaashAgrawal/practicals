<!DOCTYPE html>
<html lang="en" dir="ltr">

<head>
  <meta name="generator" content="Hugo 0.68.3" />
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="Neural Networks  Before Starting, we will change the accelerator of this notebook from a CPU to a GPU, since we&rsquo;ll be running our Neural Networks on the GPU.
  Go to Runtime &ndash;&gt; Change Runtime Type &ndash;&gt; GPU
 Welcome to the 8th practical session on Machine Learning. Today we&rsquo;re going to start learning about the most important and widely applied avenue of Machine Learning - Deep Learning. Deep Learning is nothing but a fancy word for Machine Learning applications with Neural Networks.">
<meta name="theme-color" content="#FFFFFF"><meta property="og:title" content="" />
<meta property="og:description" content="" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://palaashagrawal.github.io/practicals/docs/08-neural-networks/" />

<title>08 Neural Networks | Practical Machine Learning</title>
<link rel="manifest" href="https://palaashagrawal.github.io/practicals/manifest.json">
<link rel="icon" href="https://palaashagrawal.github.io/practicals/favicon.png" type="image/x-icon">
<link rel="stylesheet" href="https://palaashagrawal.github.io/practicals/book.min.2dc4d2afa8da6ac78d76671c21e140952e7a84846fed47ed02a1a2d68166d992.css" integrity="sha256-LcTSr6jaaseNdmccIeFAlS56hIRv7UftAqGi1oFm2ZI=">
<script defer src="https://palaashagrawal.github.io/practicals/en.search.min.04bc9d34bf913a8e7fd2e243405da2424c17970b1407e6733478ce542eb58cae.js" integrity="sha256-BLydNL&#43;ROo5/0uJDQF2iQkwXlwsUB&#43;ZzNHjOVC61jK4="></script>
<link rel="alternate" type="application/rss+xml" href="https://palaashagrawal.github.io/practicals/docs/08-neural-networks/index.xml" title="Practical Machine Learning" />
<!--
Made with Book Theme
https://github.com/alex-shpak/hugo-book
-->

  
</head>

<body dir="ltr">
  <input type="checkbox" class="hidden toggle" id="menu-control" />
  <input type="checkbox" class="hidden toggle" id="toc-control" />
  <main class="container flex">
    <aside class="book-menu">
      <div class="book-menu-content">
        
  <nav>
<h2 class="book-brand">
  <a href="https://palaashagrawal.github.io/practicals/"><span>Practical Machine Learning</span>
  </a>
</h2>


<div class="book-search">
  <input type="text" id="book-search-input" placeholder="Search" aria-label="Search" maxlength="64" data-hotkeys="s/" />
  <div class="book-search-spinner hidden"></div>
  <ul id="book-search-results"></ul>
</div>











  



  
  <ul>
    
      
        <li>
          
  
  

  
    <a href="https://palaashagrawal.github.io/practicals/docs/00-basics-of-python-and-introduction-to-machine-learning/" class="">00 Basics of Python and Introduction to Machine Learning</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://palaashagrawal.github.io/practicals/docs/01-linear-regression/" class="">01 Linear Regression</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://palaashagrawal.github.io/practicals/docs/02-logistic-regression/" class="">02 Logistic Regression</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://palaashagrawal.github.io/practicals/docs/03-singular-value-decomposition-and-principal-component-analysis/" class="">03 Singular Value Decomposition and Principal Component Analysis</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://palaashagrawal.github.io/practicals/docs/04-bayesian-learning/" class="">04 Bayesian Learning</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://palaashagrawal.github.io/practicals/docs/05-clustering-algorithms-in-machine-learning/" class="">05 Clustering Algorithms in Machine Learning</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://palaashagrawal.github.io/practicals/docs/06-support-vector-machines/" class="">06 Support Vector Machines</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://palaashagrawal.github.io/practicals/docs/07-decision-trees-and-random-forests/" class="">07 Decision Trees and Random Forests</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://palaashagrawal.github.io/practicals/docs/08-neural-networks/" class=" active">08 Neural Networks</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://palaashagrawal.github.io/practicals/docs/09-convolution-neural-networks/" class="">09 Convolution Neural Networks</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://palaashagrawal.github.io/practicals/docs/10-advanced-computer-vision-applications/" class="">10 Advanced Computer Vision Applications</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://palaashagrawal.github.io/practicals/docs/11-advanced-computer-vision-applications-continued/" class="">11 Advanced Computer Vision Applications Continued</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>















</nav>




  <script>(function(){var menu=document.querySelector("aside.book-menu nav");addEventListener("beforeunload",function(event){localStorage.setItem("menu.scrollTop",menu.scrollTop);});menu.scrollTop=localStorage.getItem("menu.scrollTop");})();</script>


 
      </div>
    </aside>

    <div class="book-page">
      <header class="book-header">
        
  <div class="flex align-center justify-between">
  <label for="menu-control">
    <img src="https://palaashagrawal.github.io/practicals/svg/menu.svg" class="book-icon" alt="Menu" />
  </label>

  <strong>08 Neural Networks</strong>

  <label for="toc-control">
    
    <img src="https://palaashagrawal.github.io/practicals/svg/toc.svg" class="book-icon" alt="Table of Contents" />
    

    
  </label>
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$','$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
  
    window.addEventListener('load', (event) => {
        document.querySelectorAll("mjx-container").forEach(function(x){
          x.parentElement.classList += 'has-jax'})
      });
  
  </script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script type="text/javascript" id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</div>


  
  <aside class="hidden clearfix">
    
  
<nav id="TableOfContents">
  <ul>
    <li><a href="#what-are-neural-networks">What are Neural Networks?</a></li>
    <li><a href="#building-a-neural-network-with-pytorch">Building a Neural Network with PyTorch</a>
      <ul>
        <li><a href="#converting-the-data-into-pytorch-tensors">Converting the data into PyTorch tensors</a></li>
      </ul>
    </li>
    <li><a href="#datasets-and-dataloaders">Datasets and Dataloaders</a></li>
    <li><a href="#creating-our-own-neural-network">Creating our own Neural Network</a>
      <ul>
        <li><a href="#activation-functions">Activation Functions</a></li>
      </ul>
    </li>
    <li><a href="#gradient-descent-on-neural-networks">Gradient Descent on Neural Networks.</a></li>
    <li><a href="#refactoring-the-entire-code-to-a-convenient-class">Refactoring the entire code to a convenient class</a></li>
    <li><a href="#implementing-nnsequential-from-scatch">Implementing nn.Sequential from scatch</a></li>
    <li><a href="#review">Review</a></li>
    <li><a href="#exercise">Exercise</a></li>
  </ul>
</nav>



  </aside>
  
 
      </header>

      
      
  <article class="markdown"><h1 id="neural-networks">Neural Networks</h1>
<blockquote>
<p>Before Starting, we will change the accelerator of this notebook from a CPU to a GPU, since we&rsquo;ll be running our Neural Networks on the GPU.</p>
</blockquote>
<blockquote>
<p>Go to Runtime &ndash;&gt; Change Runtime Type &ndash;&gt; GPU</p>
</blockquote>
<p>Welcome to the 8th practical session on Machine Learning. Today we&rsquo;re going to start learning about the most important and widely applied avenue of Machine Learning - <em>Deep Learning</em>. Deep Learning is nothing but a fancy word for Machine Learning applications with <em>Neural Networks</em>.</p>
<p>But why does Deep Learning get to have a whole phrase dedicated to it? That&rsquo;s because Deep Learning is so flexible and efficient, that it can solve almost any problem in the modern age. Neural Networks have reached a point, where they can process almost any kind of data - videos, images, audio, text, tabular data, etc. And in a lot of applications, Neural Networks have been claimed to perform better than human performance. (<a href="https://www.eff.org/ai/metrics">This</a> notebook provides a comparison of a lot of models on various benchmark problems along with human performance. You would get a general idea about the current state of Deep Learning).</p>
<p>That&rsquo;s crazy if you think about it, since Artificial Intelligence literally aspires to be as good as human intelligence. We consider human intelligence to be the most superior, and every form of intelligence, including animal intelligence, and Artificial Intelligence is compared to human intelligence. So, you would now understand why there is so much hype about Deep Learning. Almost all research (including corporate research too) in Machine Learning is about Deep Learning now.</p>
<pre><code>
</code></pre><h2 id="what-are-neural-networks">What are Neural Networks?</h2>
<p>You may have seen a visual representation of Neural Networks before, something like the picture below.</p>
<p><img src="https://drive.google.com/uc?id=1oXydloc3m8yNmM17Z4CvI1L8F4APRJiN" alt="" /></p>
<p>This is nothing but an augmented version of the Linear Model we learned about in the Linear/Logistic Regression sessions.</p>
<p>In the Linear Regression Model, we fed an input tensor (with different features $x_1$,$x_2$,&hellip;$x_n$ into the model $f(x) = w_1.x_1 + w_2.x_2 + w_3.x_3 + &hellip;.. + b$. (If you don&rsquo;t remember this, head back to the Linear Regression and Logisitic Regression sessions, and come back once you&rsquo;re clear with that concept!)</p>
<p>Neural Networks are nothing but multiple Linear functions stacked on top of each other.</p>
<p>So the output of linear model is fed to the next, and this is done multiple times. Finally we get an output just like we did in Linear/Logistic Regression models.</p>
<p>But wait, why do we even need to stack multiple Linear models on top of each other?</p>
<p>So, if you remember the Linear Regression session, you would know that the &ldquo;model&rdquo; is nothing but some mathematical function in the space of the feature vectors ($x_1$, $x_2$ and so on). Our job is to find that mathematical function that best solves a task (like a Cat vs Dog classification problem).</p>
<p>But most of the times, the model is not necessarily as simple as a single Linear Function. Neural Networks is a series of linear functions, that apparantly, can represent <em>any</em> complexity of mathematical functions in space. The more the number of <em>layers</em>, or the number of linear stacks (in other words, the <em>deeper</em> the network) (Hence the name <em>Deep</em> Learning), the more complexity the model can handle. Isn&rsquo;t that an interesting approach?</p>
<p>While Deep Learning may have gained popularity in the past decade only, Neural Networks are definitely not a new concept. They existed as back as in 1940s. There is a wonderful article by Stanford University on the history of Neural Nets. <a href="https://cs.stanford.edu/people/eroberts/courses/soco/projects/neural-networks/History/history1.html">Check it out</a> if you want to know more!</p>
<p><img src="https://drive.google.com/uc?id=1zSbF-sQtR_-Ddf85vzjnfqCetOgAgJ2y" alt="" /></p>
<p>This is how a Neural Networks work. Remember? in the Linear/Logistic Regression Model we fed an input, and passed it through the linear function, and based on the output, made our predictions! Here too, we feed the input through multiple linear functions, and based on the final output, make our predictions. We will look at this in more detail below!</p>
<p>In this session, we will learn how to build our own Neural Networks with PyTorch. We&rsquo;ll be learning how to use PyTorch&rsquo;s inbuilt functionality to vreate Neural Networks with minimal effort. Then we&rsquo;ll slowly breakdown each component and learn how to write them in python from scratch. That being said, this session has some really good lessons on python development, especially Object Oriented Programming concepts, which will help you develop better Machine Learning code for the rest of your life. Most Deep Learning practitioners are not good software developers, and so it is a valuable skill to master in order to develop efficient tools and techniques.</p>
<h2 id="building-a-neural-network-with-pytorch">Building a Neural Network with PyTorch</h2>
<p>Let us pick a problem and solve it using Neural Networks. For the purpose of this session, we&rsquo;ll be using the <a href="https://www.kaggle.com/moltean/fruits"><em>Fruits Dataset</em></a>, which contains images of 44 fruits and vegetables, which we wish to classify. For simplicity, we&rsquo;ll only be classifying bwtween any 2 categories for now.</p>
<p>Lets us dowload this data from kaggle. First of all, upload your <em>kaggle.json</em> file.</p>
<pre><code>%cd 
from google.colab import files
uploaded = files.upload()
for fn in uploaded.keys():
  print('User uploaded file &quot;{name}&quot; with length {length} bytes'.format(
      name=fn, length=len(uploaded[fn])))
!mkdir -p ~/.kaggle/ &amp;&amp; mv kaggle.json ~/.kaggle/ &amp;&amp; chmod 600 ~/.kaggle/kaggle.json 

!kaggle datasets download -d moltean/fruits
!unzip fruits.zip &gt;./tmp
%cd fruits-360
!ls
</code></pre><p>The following are the classes that we are dealing with.</p>
<pre><code>!cd Training &amp;&amp; ls
</code></pre><p>This dataset contains 100x100 pixel images for any category. So, for example, let us look at two categories  - Pears and Cauliflowers. Just fr visualization purposes, we&rsquo;ll be using the PIL library</p>
<pre><code>from PIL import Image
</code></pre><pre><code>im=Image.open('Training/Cauliflower/236_100.jpg')
print(im.size)
im
</code></pre><pre><code>im=Image.open('Training/Pear/236_100.jpg')
print(im.size)
im
</code></pre><h3 id="converting-the-data-into-pytorch-tensors">Converting the data into PyTorch tensors</h3>
<p>As you may remember, PyTorch processes all data in the form of <em>tensors</em>, which are nothing but N-dimensional arrays. Our first job is to find a way to convert images to tensors and vice versa (just in case we wish to visualize tensors as images).</p>
<p>A quick google search tells us that the <em>torchvision</em> module of the PyTorch library functions contains some classes that can do this for us. Let us create objects of these classes, namely <code>img2tensor</code> and <code>tensor2img</code>. The names are pretty self explanatory!</p>
<blockquote>
<p>Lessons in Python:</p>
</blockquote>
<blockquote>
<p><em>Class</em></p>
</blockquote>
<blockquote>
<p>A class is a collection of different things, all under one roof. These things can be anything - data or functions  (these are the two broad categories that everything in programming can be put into, if you think about it). The data can be anything like - numbers, strings, <em>objects</em> of other classes *we&rsquo;l look at <em>objects</em> next!), and functions are the peices of code that operate on this data, to provide us users with some functionality. (These functions within a class are also called as methods).</p>
</blockquote>
<blockquote>
<p>So, if you remember building models with SKLearn, we always used classes to build models. We first named our class something, and first gave it some <em>data</em> (usually our x&rsquo;s and y&rsquo;s) and then used the <code>.fit()</code> method (or function) to train the model!</p>
</blockquote>
<blockquote>
<p>Objects are nothing but the real world implementations of classes. Classes are only templates about how data and functions (attributes and methods, to be more precise) can exist together and function together to provide usefulness to the user! An object is like the physical version of a class, that Actually does tasks and stores data physically.</p>
</blockquote>
<blockquote>
<p>If you&rsquo;re not aware of the concept of classes and objects at all, it would be useful to spend some time learning about them. <a href="https://www.youtube.com/watch?v=n-DVyV2RjiY">Here</a> is a video to help you understand the intuition behind them. The video teaches this concept in JAVA, which is a really old (classic) but popular language to learn object oriented programming (which is nothing but the style of programming, which contains lots and lots of classes and objects).</p>
</blockquote>
<blockquote>
<p>One we dive a little deeper into classes in python, we&rsquo;ll provide with more resources to learn about the more technical details.</p>
</blockquote>
<p>Python is a fully object oreiented programming language, meaning everything in python is an object of some class. Turns out, that objects are essential in data sciences, because of the flexibility they provide in bundling different data and functionality together, which is also one of the reasons why python is so famous for data sciences.</p>
<p>Let us start with creating <code>img2tensor</code> and <code>tensor2img</code>, which are objects of the classes <code>ToTensor</code> and <code>ToPILImage</code>.</p>
<pre><code>from torchvision.transforms import ToTensor,ToPILImage
img2tensor = ToTensor()
tensor2img=ToPILImage()
</code></pre><p>Let us see these classes in action!</p>
<pre><code>img2tensor(im) #gives us a tensor
</code></pre><pre><code>img2tensor(im).shape
</code></pre><p><code>img2tensor</code> takes in a PIL image as input, and gives a tensor as an output.</p>
<p>Notice how <code>img2tensor</code> is an object of a class, but is used like function. This is a special feature of python. We have the liberty to define the functionality of an object, when it is used like a function. (like <code>output = object_name(input)</code> We will learn about this in more detail below.</p>
<p>But first let us move ahead. Next, we need to go to the location where all images are, and convert each image to a tensor so that we can get the final <code>x</code> and <code>y</code> for our model!. To handle paths, we use the <code>pathlib</code> library. And to navigate through directories, we use the <code>os</code> library in python.</p>
<pre><code>import os
from pathlib import Path
Path.ls =lambda x: os.listdir(x) # this funcitonality gives us the contents of a directory, in a LINUX style command (&quot;ls&quot;)
</code></pre><p>Next, we&rsquo;ve written a function that takes in the paths of all the directories where are images are (for example, if our model contains data on cauliflowers and pears, we will pass in the paths where cauliflower and pear images are). And the model returns the <code>x</code> tensors and the corresponding ground truth labels <code>y</code>.</p>
<p>Now, since we are dealing with tensors, we need the PyTorch library (<code>torch</code>), to handle tensors. So we import that!</p>
<pre><code>import torch
</code></pre><pre><code>def get_tensors_from_folder(folder_paths:list=None,normalize_x=True,preprocess_y=True):
    f&quot;&quot;&quot;
    Takes in the paths of all the categories that need to be considered, and returns a collective tensor for the input tensors and the target tensors. 
    normalize_x: Default True. If True, the function will automatically Gaussian normalize the input tensor x.
    preprocess_y: Default True. If True, the function will convert the labels to a series of numbers ranging from 0 to N-1, where N is the number of classes(categories). 
                                This is important if your classes are strings, or unordered. Computers cannot understand anything other than numbers. More than that, 
                                Neural Networks, as you will see, only understand categories that go as 0,1,2,....N-1 (if there are N categories to be classified in total.)
    &quot;&quot;&quot;
    x = None 
    y = []
    folder_paths=list(folder_paths)

    for folder_path in folder_paths:
        for img_path in folder_path.ls():
            x=torch.cat((img2tensor(Image.open(folder_path/img_path))[None],x)) if x is not None else img2tensor(Image.open(folder_path/img_path))[None]
            # y=torch.cat((torch.Tensor(folder_path.stem),y)) if y is not None else torch.Tensor(folder_path.stem)
            y.append(folder_path.stem)

    if normalize_x: 
        x.sub_(x.mean()).div_(x.std())
    if preprocess_y:
        proc={v:k for k,v in enumerate(sorted(set(y)))}
        y=[proc[v] for v in y]

    y=torch.Tensor(y).long()
    return x.reshape(x.shape[0],-1),y
</code></pre><p>So, now that we&rsquo;ve defined the function to retrieve the relevant tensors, we will use it to derive the training <code>x</code> and <code>y</code> tensors and the testing <code>x</code> and <code>y</code> tensors.</p>
<pre><code>path_cauliflowers = Path('Training/Cauliflower')
path_pears=Path('Training/Pear')

x_train,y_train=get_tensors_from_folder([path_cauliflowers,path_pears])
x_train.shape, y_train.shape
</code></pre><pre><code>path_cauliflowers = Path('Test/Cauliflower')
path_pears = Path('Test/Pear')

x_test,y_test = get_tensors_from_folder([path_pears,path_cauliflowers])
x_test.shape,y_test.shape
</code></pre><h2 id="datasets-and-dataloaders">Datasets and Dataloaders</h2>
<p>Next, we need to learn about 2 important concepts in datasciences - Dataset and Dataloaders.</p>
<p>Dataset: A Dataset is nothing but a class which bundles the <code>x</code> and <code>y</code>. This is important, because its very inefficient to have to specify x and y separately everytime. They always go together. So the dataset class makes our life easier, by simply bundling x and y. You can simply retrieve the individual x and y by calling the attributes <code>dataset.x</code> and <code>dataset.y</code>.</p>
<p>Below is a simple implementation of the Dataset class.</p>
<p>Any dataset class should have three properties</p>
<ul>
<li>It should have <code>x</code> and <code>y</code> as its attributes.</li>
<li>We should be able to get the length of the dataset (with the <code>__len__</code> (pronounced dunder len) method)</li>
<li>We should be able to get a specific value by index (with the <code>__getitem__</code> method)</li>
</ul>
<pre><code>class Dataset(): # to wrap x and y together
    def __init__(self, x,y):self.x,self.y=x,y # __init__ is pythons way to initiate classes into objects. self.x is basically the process of setting x as an attribute of the object
    def __len__(self): return self.x.shape[0]
    def __getitem__(self,i): return self.x[i], self.y[i]
</code></pre><pre><code>#Let us derive the training dataset
train_ds=Dataset(x_train,y_train)
len(train_ds) # we can use len on train_ds, because of the __len__ method
</code></pre><pre><code>train_ds[0] # __getitem__
</code></pre><pre><code>#similarly let us derive the Validation Dataset
valid_ds=Dataset(x_test,y_test)
len(valid_ds)
</code></pre><p>Lessons in Python.
You may have used len and list_name[i] very frequently in the case of lists. Even list is a class in python, which ultimately implements the <code>__len__</code> and <code>__getitem__</code> methods</p>
<pre><code>my_list=[1,2,3,4,5]
len(my_list), my_list.__len__()
</code></pre><pre><code>my_list[0], my_list.__getitem__(0)
</code></pre><p>The next concept is dataloaders.
A dataloader is a convenient class in python that does 2 things.</p>
<ol>
<li>It loads the data into the model (that we are going to build). Sometimes, it is not a wise idea to feed all the data into the model at once, because of memory constraints, so the dataloader can also feed data to the model in batches.</li>
<li>The dataloader can also make sure that the data is loaded onto the correct device (CPU or GPU). By default, everything is on the CPU, so you need to explicitly load the data on the GPU, if you wish so. So, dataloaders can save us from this unnecessary effort!</li>
</ol>
<blockquote>
<p>What is a GPU?</p>
</blockquote>
<blockquote>
<p>A GPU stands for Graphical Processing Unit (Just like CPU stands for Central Processing Unit). Its also known as a graphics card. If anyone of you is interested in gaming, you would know what a graphics card is! A GPU is like a CPU, but multitudes faster (sometimes millions of times faster). A GPU can do matrix multiplications and additions very fast, which is exactly what the graphics of any video game are - lots and lots of matrix operations. A CPU is much more capable of handling more complex operations than matrix operations, but a GPU, even though less capable of doing complex operations, is very fast at doing these basic operations. Turns out, that Deep Learning is nothing but a lot of matrix multiplications and additions, which is why we use GPUs for Deep Learning.</p>
</blockquote>
<blockquote>
<p>There&rsquo;s a newer type of device, called the TPU, or the Tensor Processing Unit, which is a GPU, but meant specifically for Tensor Operations. So, a TPU can ideally handle much less complexity than a GPU, but can perform Deep Learning related operations at a much faster rate. They are still not commercially used, because</p>
</blockquote>
<ol>
<li>They are still very costly</li>
<li>Because the community awareness is still low, there isnt much support for TPUs in deep learning libraries. It will take a few years, before TPUs become mainstream.</li>
</ol>
<p>PyTorch implements DataLoaders for us. We will use that!</p>
<pre><code>from torch.utils.data import DataLoader
</code></pre><pre><code>train_dl=DataLoader(train_ds,batch_size=64,shuffle=True)
valid_dl=DataLoader(valid_ds,batch_size=128,shuffle=True) #batch_size double the batch_size of train_dl since valid_dl doesnt require gradients, so less space will be used, and thus batch_size can be increased. 
</code></pre><p>As you can see that, the DataLoader takes in a dataset, along with <code>batch_size</code> and an option to specify whether the data should be returned in random order, or in the same order as given in the dataset.</p>
<p>This dataloader now works just like an iterator.</p>
<pre><code>len(train_dl)
</code></pre><pre><code>for x,y in train_dl: print(x.shape, end=' ')
</code></pre><p>We can simply feed in one batch of size 64 to the model at once.</p>
<p>Now that we are set up with our data, let us move onto building our Neural Networks</p>
<h2 id="creating-our-own-neural-network">Creating our own Neural Network</h2>
<p>PyTorch provides us with a really easy way to define Neural Networks, by simply stacking the different components (or layers) in a function called <code>Sequential</code>.</p>
<p>PyTorch provides a module called <code>nn</code> which contains all Neural Network architecture related stuff</p>
<pre><code>from torch import nn
</code></pre><pre><code>def model(in_features,hidden_features,out_features): 
    return nn.Sequential(nn.Linear(in_features,hidden_features), nn.ReLU(), nn.Linear(hidden_features,out_features),nn.Sigmoid())
</code></pre><p>What does</p>
<p><code>nn.Sequential(nn.Linear(in_features,hidden_features), nn.ReLU(), nn.Linear(hidden_features,out_features), nn.Sigmoid())</code></p>
<p>do ?</p>
<p>First of all, what are nn.Linear, nn.ReLU, and nn.Sigmoid?</p>
<p>nn.Linear is nothing but a linear model class. It is the implementation of the linear model we defined in the Linear/Logistic Regression session. That&rsquo;s the basic building block of a Neural Network.</p>
<p>nn.ReLU is the implementation of the <em>ReLU</em> Activation function, or the Rectified Linear Unit. We&rsquo;ve seen the sigmoid activation function. The ReLU activation function looks like this.</p>
<p><img src="https://drive.google.com/uc?id=1YzqTmE-VLvLRzAj19Hde1vlcXv1JJNBr" alt="" /></p>
<p>And is simply defined as</p>
<p>$$ReLU(z) = max(0,z)$$</p>
<p>What is an activation function anyways?</p>
<h3 id="activation-functions">Activation Functions</h3>
<p>As you know, a Neural Network is nothing but a stack different linear functions. But if you analyze a simple linear stack mathematically, as follows&hellip;</p>
<p>$$y_1=w_1.x + b_1$$
$$y_2 = w_2.y_1 + b_2 $$</p>
<p>$y_2$ can be simplified as:</p>
<p>$$y_2 = w_2(w_1.x_1 + b_1) + b_2 $$
$$ y_2 = w_2.w_1.x_1 + (w_2.b_1 + b_2)$$</p>
<p>which is nothing but another linear function</p>
<p>$$ y_2 = W.x_1 + B $$</p>
<p>which is exactly what $y_1$ could have represented. Then what was the point of stacking multiple linear layers? The ultimate point of stacking linear layers was because a single linear layer could not represent the complexity of functions in space.</p>
<p>You can try stacking 3,4, or even more linear layers together, and write them mathematically, then simplify them, and see, that the final result will still be equivalent to one linear layer.</p>
<p>So, how do we overcome this problem? This is where Activation Functions come into use. Activation Functions are non-linearities put in between of two linear layers. It turns out that that because of this non-linearity, the resulting model can achieve any level of complexity, with as little as 2 layers. Infact, a research proposed that with just 2 layers of Neural Networks (excluding the output layer), with a non-linearity activation function, can achieve arbitrarily any level of model complexity. Obviously, this is only in theory, and in practice you need more than 2 layers to achieve a level of complexity, which is why many Neural Nets have 100s, maybe even 1000s of layers.</p>
<p>For now, we&rsquo;ll be focussing on only 2 layers (the input layer, and the hidden layer). (Hidden Layer is basically any linear layer in between the output and the input layer, both excluded. Refer to the figure in the beginning of this notebook).</p>
<p>There are many activation functions. Even Sigmoid is an activation function. Infact, in the earlier days, sigmoid was used in place of ReLU. But now, we only use ReLU as the activation function.</p>
<hr>
<p>Finally nn.Sigmoid is the implementation of the sigmoid function. Just like in the logistic regression model, we use sigmoid to scale the output between 0 and 1. This helps us interpret the model output better, in terms of probability.</p>
<p>nn.Sequential stacks all these components together.
When we write</p>
<p><code> nn.Sequential(nn.Linear(in_features,hidden_features), nn.ReLU(), nn.Linear(hidden_features,out_features),nn.Sigmoid())</code></p>
<p>When you feed data into this, the data fill be fed into the first layer of the stack, ie nn.Linear. The output of this Linear layer is fed into ReLU, the output of the ReLU is fed into the second Linear Layer, and finally the output of the Linear layer is fed to the Sigmoid function.</p>
<p>The output of the Sigmoid function is our final output function, and we will make predictions based on that</p>
<pre><code>my_model = model(x_train.shape[-1],50,2)
my_model #prints out all the modules # uses the __repr__ method
</code></pre><p>You can see the different layers in <code>my_model</code>.</p>
<p>nn.Sequential has some wonderful properties. It provides methods like the <code>.parameters</code> to return the list of learnable parameters in the model. So, if you remember the gradient descent algorithm from the Linear/Logistic Regression session, you would know that we need to update parameters. So nn.Sequential makes it easy for the model to recognize which data are the parameters.</p>
<pre><code>for p in my_model.parameters(): print(p.shape)
</code></pre><p>And we can simply use my_model as a function (even though its a class object). As seen below. This comes by implementing the <code>__call__</code> method, which tells the object to do something, if the object is directly used as a function. So the <code>__call__</code> method of the nn.Sequential must be implementing the functionality to pass the input through all the layers in the model and returning the output</p>
<pre><code>xb,yb=next(iter(train_dl))
pred = my_model(xb)
</code></pre><pre><code>??my_model.__call__
# You can see, that there is some implementation of __call__, in the form of _call_impl . Details dont matter as of now
</code></pre><p>What do the predictions from the model look like? Let us analyze the <code>pred</code> tensor</p>
<pre><code>pred.shape
</code></pre><p><code>pred</code> is basically a tensor of 2 values for every one of the 64 inputs. These 2 outputs per input is basically the prediction of the probability of each one of the 2 categories (pears and cauliflowers). As we saw earlier, whichever neuron has the highest value, is basically the prediction of the model for that input.</p>
<pre><code>pred
</code></pre><h2 id="gradient-descent-on-neural-networks">Gradient Descent on Neural Networks.</h2>
<p>Next, we will simply apply the <strong>Gradient Descent</strong> Algoithm to optimize our Neural Nets. If you don&rsquo;t remember what Gradient Descent is, head back to the Linear Regression and Logistic Regression sessions to understand it, and then come back!</p>
<p>The steps are the same.</p>
<ol>
<li>Get predictions from the model</li>
<li>Calculate loss function</li>
<li>Derive Gradients of parameters with respect to the loss function</li>
<li>Update the parameters by a specific step size (called as learning rate)</li>
<li>Calculate the accuracy on the validation/test dataset to find out how good your model is performing</li>
<li>Repeat this process for many iterations (or <em>epochs</em>)</li>
</ol>
<p>We implemented this from scratch earlier. But now, we will use PyTorch&rsquo;s built in Gradient Descent Algorithm.</p>
<pre><code>learning_rate = 0.001
optimizer = torch.optim.SGD(my_model.parameters(), lr=learning_rate)
</code></pre><p>And we will also use the Cross Entropy Loss function inbuilt in PyTorch. If you dont remember what the Cross Entropy loss function does, head back to the Logisitic Regression session!</p>
<pre><code>loss_fn=torch.nn.CrossEntropyLoss()
</code></pre><p>Next, we will pass the input through the model, and calculate the loss. Passing the data through the model is called a <strong>forward pass</strong> through the model.</p>
<pre><code># FORWARD PASS
pred=my_model(xb) # we only pass one batch at a time. Because many a times, the entire dataset is too large for the model to handle at once
loss=loss_fn(pred,yb)
</code></pre><p>And we calculate the gradients after this. You don&rsquo;t have to implement the gradient calculations on your own. PyTorch does that for you!</p>
<p>You would remember from the Linear/Logistic Regression session that we simply wrote <code>loss.backward</code> to calculate the *gradient of all parameters with respect to the loss`. (Now you understand why nn.Sequential&rsquo;s .parameter method is useful. With this, the model keeps track of where parameters and present, so that gradients can be calculated and they can be updated).</p>
<p>The method is called <code>.backward</code> because traditionally, the process of calculating gradients is called a <strong>backward pass</strong>. Its nothing but an implementation of the chain rule for derivative calculations.</p>
<pre><code># BACKWARD PASS
loss.backward()
</code></pre><p>So, just to understand the state of the model at this point - We haven&rsquo;t trained the Neural Net at all right now. So let us see how the model is performing on the dataset right now, and we will compare it with the performance after we train the model.</p>
<p>So for that, we define an accuracy function, which basically checks how many predictions are the same as the target value (ground truth value).</p>
<p>The neural Network has 2 neurons at the output layer, corresponding to the two classes (pears and cauliflowers). Whichever neuron has the highest value, is the prediction for the given input. <code>torch.argmax</code> basically returns the index of the neuron with gives the highest value.</p>
<pre><code>def accuracy(pred,targ): return (torch.argmax(pred,dim=1)==targ).float().mean() #understand this line of code. Search on google for the functionality of a term, if you dont understand
</code></pre><pre><code>accuracy(pred,yb)
</code></pre><p>Now, if we update the parameters once, using the <code>optimizer.step</code> method of the <code>optimizer</code> class, let us see how the accuracy increases.</p>
<pre><code>optimizer.step()
optimizer.zero_grad()
</code></pre><pre><code>accuracy(my_model(xb),yb)
</code></pre><p>You can see that the accuracy has increased!</p>
<p>What is <code>optimizer.zero_grad()</code>? We mentioned this in the earlier sessions too, when we were discussing gradient descent, but we didnt describe it back then. Let us understand it now.</p>
<p>The idea is simple. By default, PyTorch <em>accumulates</em> (or keeps adding up ) gradients if you keep calculating gradients. This is not a bad thing, infact, its quite helpful when you have very small batch sizes, and you wish to accumulate gradients of multiple batches, (more the data used for parameter update, the better generalisation occurs in the final output). At the end, you have to manually <em>clear</em> the gradients, so that fresh gradients can be calculated. <code>.zero_grad</code> does exactly that. It replaces the gradients with a zero value. So the next time <code>.backward</code> is called, gradients are added to 0, which basically means, that fresh gradients are calculated.</p>
<p>This is it. You&rsquo;ve built your own Neural Network!</p>
<h2 id="refactoring-the-entire-code-to-a-convenient-class">Refactoring the entire code to a convenient class</h2>
<p>The above code is quite scattered and clunky. We will refactor the code to make the entire Neural Network more understandable and easy to work with.</p>
<p>Now is a good time to understand what <code>__init__</code> does. <a href="https://www.youtube.com/watch?v=ZDa-Z5JzLYM">Here</a> is a great video by Cory Schafer on YouTube that explains this.  Actually, Cory Schafer has one of the best tutorials on object oriented programming in Python. So go check it out if you&rsquo;re interested!</p>
<pre><code>class NeuralNetwork():
    def __init__(self,train_dl,valid_dl,model,loss_fn=nn.CrossEntropyLoss(),optimizer=None):
        f&quot;valid_dl can be None. In that case, the model will be evaluated on the training set only&quot;

        self.train_dl = train_dl
        self.valid_dl = valid_dl
        self.model = model
        self.loss_fn=loss_fn
        self.optimizer = torch.optim.SGD(self.model.parameters(),lr=0.1) if not optimizer else optimizer
        
    def train(self,epochs=1,lr=None,loss_fn=None):
        self.train_loss,self.valid_loss=[],[]
        if lr: self.opt.defaults['lr']=learning_rate
        if loss_fn: self.loss_fn=loss_fn

        for epoch in range(epochs):
            # self.model=self.model.train()
            for x,y in train_dl:                     
                loss=self.loss_fn(self.model(x),y)
                loss.backward()
                self.optimizer.step()
                self.optimizer.zero_grad()
                self.train_loss.append(loss)

            if self.valid_dl: print(f'epoch {epoch+1}:validation accuracy - {self.get_validation_accuracy()}')
            else: print(print(f'epoch {epoch+1}: training accuracy - {self.get_validation_accuracy(validation=False)}'))

    def get_validation_accuracy(self,validation=True):
        dl=self.valid_dl if validation else self.train_dl
        with torch.no_grad(): acc=[accuracy(self.model(x),y) for x,y in dl]
        return torch.Tensor(acc).mean().float()
</code></pre><pre><code>my_model = model(x_train.shape[-1],50,2) # reinitializing the model
optimizer=torch.optim.Adam(my_model.parameters(),lr=0.0001)
my_network = NeuralNetwork(train_dl,valid_dl,my_model,loss_fn=loss_fn,optimizer=optimizer)
</code></pre><pre><code>my_network.get_validation_accuracy() # checking the model performance before training, for comparison
</code></pre><pre><code>my_network.train(1)
</code></pre><p>You can see that with just one epoch, the model has improved significantly.</p>
<p>Try writing the above class from scratch, without looking at it. You&rsquo;ll be surprised by how much you learn about python and Neural Nets with just this one class definition. It should take you about 3-4 hours to write it. Peeking at the original function once in a while is okay, but your aim should be to understand everything, and being able to write it from scratch.</p>
<h2 id="implementing-nnsequential-from-scatch">Implementing nn.Sequential from scatch</h2>
<p>We&rsquo;ve implemented a model using nn.Sequential from scratch. But lets see what goes on under the hood. Let&rsquo;s actually look at the source code of the nn.Sequential function.</p>
<p>In Jupyter notebooks, you can look up the source code of any function using the <code>??</code> sign.</p>
<p>So let us first go to the source code of the model function which we defined above.</p>
<pre><code>??model
</code></pre><p>As expected, it would show us the definition of the model, as defined above. We used nn.Sequential within the <code>model</code> function.</p>
<p>So next, let us look up the source code of the nn.Sequential class.</p>
<pre><code>??nn.Sequential
</code></pre><p>First of all, notice that nn.Sequential <em>inherits</em> from nn.Module.</p>
<p><code>nn.Module</code> is the base class for all functions in the <code>nn</code> (torch.nn). It contains basic information about how nn functions should behave. The <code>__call__</code> for all functions is implemented in nn.Module .</p>
<pre><code>class Sequential(Module):
</code></pre><pre><code>??nn.Module.__call__
</code></pre><p>There&rsquo;s a lot of technical code over there, but look for this main line of code.</p>
<pre><code>result = self.forward(*input, **kwargs)
</code></pre><p>So, behind the scenes, nn functions call a function called <code>forward</code>. This is nothing but the function corresponding to the forward pass. Now you know why you can do a forward pass by simply calling the object like a function, with input as the argument to the function.</p>
<p>This forward function is all you need to define for any nn function. Even nn.Linear, nn.ReLU, nn.Signmoid define only a forward function, and the rest is done by the <code>__call__</code> method.</p>
<p>Next, lets take a look at the <code>forward</code> function of the nn.Sequential function</p>
<pre><code>??nn.Sequential.forward
</code></pre><p>You would see this.</p>
<pre><code>def forward(self, input):
        for module in self:
            input = module(input)
        return input
</code></pre><p>what is a module? A module is the basic unit in nn.Sequential function. In our case, nn.Linear, nn.ReLU, nn.Sigmoid, etc are the basic units. Let us run this ourself to understand this better</p>
<pre><code>#my_model is a nn.Sequential object
for module in my_model: print(module)
</code></pre><p>As you can see, module is nothing but the different building layers of our model.
So what nn.Sequential.forward is doing, is that it iterates through each layer, and passes the output from the previous layer to the next.</p>
<p>Now that we know how it works, let a model using this behaviour from scratch!</p>
<pre><code>class model(nn.Module):
    def __init__(self,layers):
        super().__init__()
        self.layers=layers
        for k,v in enumerate(self.layers): self.add_module(f&quot;layer {k}&quot;,v) #nn.Sequential automatically added each layer as a module. We didnt have to do that. 
                                                                           #But now we have to do it ourselves. nn.Module has a function called add_module that can add any layer as a module of the class.
</code></pre><p>Now all we need to do is, define a forward function in this model class. But before that, lets see, if the model behaves properly</p>
<pre><code>in_features=x_train.shape[-1]
hidden_features=50
out_features=2

layers=[nn.Linear(in_features,hidden_features), nn.ReLU(), nn.Linear(hidden_features,out_features),nn.Sigmoid()]
my_model=model(layers)
</code></pre><pre><code>my_model
</code></pre><p>Yes, it does. You can see, that all the building blocks have been added as modules. Now we can create a forward function.</p>
<p>Let us actually verify that this model class will use the <code>forward</code> function when called</p>
<pre><code># this will show an error at this point because forward is not implemented
# my_model(x_train)    #Uncomment to run
</code></pre><p>You can see that the error occurs at the line <code>result = self.forward(*input, **kwargs)</code>. Meaning, all we need to do is, define a forward function, and nn.Module will take care of the rest.</p>
<pre><code>class model(nn.Module):
    def __init__(self,layers):
        super().__init__()
        self.layers=layers
        for k,v in enumerate(self.layers): self.add_module(f&quot;layer {k}&quot;,v)
    
    def forward(self,x):
        for layer in self.layers: x=layer(x)
        return x
</code></pre><pre><code>my_model=model(layers)
pred=my_model(x_train)
pred.shape
</code></pre><p>Yes, the model prediction is working correctly now!</p>
<h2 id="review">Review</h2>
<p>That&rsquo;s it for this session. In the next session, we&rsquo;ll look at more advanced concepts in Neural Networks, and following that, we&rsquo;ll dive into specific applications in Deep Learning, like Computer Vision, NLP and so on.</p>
<p>In this session, we learnt how to build our own Neural Networks from scratch. We dived deep into PyTorch&rsquo;s elegant API and even learnt a lot new stuff in python.</p>
<p>You are not expected to learn all this stuff in one go. There were too many concepts involved in this session, so its okay to take some time to understand all this stuff. Go through the whole notebook again if you need to, and make sure you understand everything well.</p>
<p>We built Neural Networks the practical way - no math involved. Thats because libraries that exist today make sure we don&rsquo;t have to involve ourselves in unnecessary bookish mathematics. Infact, we saw how PyTorch provides us with functionalities to build our own Neural Networks in just one line of code - <code>nn.Sequential(...)</code>. Yes, we went much deeper than that. The sole reason behind that was to show you how things work at the foundational level of code.</p>
<p>This is important when you want to modify the functionality of a code. If you simply don&rsquo;t know how nn.Sequential works, you cannot make it work in any other way than the default way. However, once you know how things work at the grassroot level,  you can make your code work in whatever way you want!</p>
<h2 id="exercise">Exercise</h2>
<p>Your task is to define the nn.Linear, nn.Sigmoid and nn.ReLU from scratch. You only have to implement the <code>forward</code> function in each of these classes. Once you do that, build a nn.Sequential object using not nn.Linear, etc, but the functions you define below. Run the Pear vs Cauliflower Classifier and make sure you get the same results.</p>
<p>You can use any python or pytorch inbuilt function if needed, but you cannot directly copy paste the code from the repective forward definitions of nn.Linear, etc.</p>
<pre><code>import math
</code></pre><pre><code>class linear(nn.Module):
    def __init__(self, in_features, out_features):
        super().__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))
        self.bias = nn.Parameter(torch.Tensor(out_features))
        self.reset_parameters() #used to kaiming initialize parameters
    
    def reset_parameters(self): #implementation of the kaiming initialization. IGNORE THIS
        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))
        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)
        bound = 1 / math.sqrt(fan_in)
        nn.init.uniform_(self.bias, -bound, bound)

    def forward(self,x):
        raise NotImplementedError
</code></pre><pre><code>class relu(nn.Module):
    def __init__(self):
        super().__init__()
        
    def forward(self,x):
        raise NotImplementedError
</code></pre><pre><code>class sigmoid(nn.Module):
    def __init__(self):
        super().__init__()
    
    def forward(self,x):
        raise NotImplementedError
</code></pre><pre><code>in_features=x_train.shape[-1]
hidden_features=50
out_features=2

layers=[linear(in_features,hidden_features), relu(), linear(hidden_features,out_features),sigmoid()]
my_model=nn.Sequential(*layers)
</code></pre><pre><code>my_model
</code></pre><pre><code>optimizer=torch.optim.Adam(my_model.parameters(),lr=0.0001)
</code></pre><pre><code>mynet=NeuralNetwork(train_dl,valid_dl,my_model,optimizer=optimizer)
</code></pre><pre><code>mynet.get_validation_accuracy() #checking accuracy before the training cycle
</code></pre><pre><code>mynet.train()
</code></pre><pre><code>
</code></pre></article>
 
      

      <footer class="book-footer">
        
  <div class="flex flex-wrap justify-between">





</div>

 
        
      </footer>

      
  
  <div class="book-comments">

</div>
  
 

      <label for="menu-control" class="hidden book-menu-overlay"></label>
    </div>

    
    <aside class="book-toc">
      <div class="book-toc-content">
        
  
<nav id="TableOfContents">
  <ul>
    <li><a href="#what-are-neural-networks">What are Neural Networks?</a></li>
    <li><a href="#building-a-neural-network-with-pytorch">Building a Neural Network with PyTorch</a>
      <ul>
        <li><a href="#converting-the-data-into-pytorch-tensors">Converting the data into PyTorch tensors</a></li>
      </ul>
    </li>
    <li><a href="#datasets-and-dataloaders">Datasets and Dataloaders</a></li>
    <li><a href="#creating-our-own-neural-network">Creating our own Neural Network</a>
      <ul>
        <li><a href="#activation-functions">Activation Functions</a></li>
      </ul>
    </li>
    <li><a href="#gradient-descent-on-neural-networks">Gradient Descent on Neural Networks.</a></li>
    <li><a href="#refactoring-the-entire-code-to-a-convenient-class">Refactoring the entire code to a convenient class</a></li>
    <li><a href="#implementing-nnsequential-from-scatch">Implementing nn.Sequential from scatch</a></li>
    <li><a href="#review">Review</a></li>
    <li><a href="#exercise">Exercise</a></li>
  </ul>
</nav>


 
      </div>
    </aside>
    
  </main>

  
</body>

</html>












