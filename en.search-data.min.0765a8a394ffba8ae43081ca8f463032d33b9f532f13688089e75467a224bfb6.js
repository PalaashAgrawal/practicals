'use strict';(function(){const indexCfg={cache:true};indexCfg.doc={id:'id',field:['title','content'],store:['title','href','section'],};const index=FlexSearch.create('balance',indexCfg);window.bookSearchIndex=index;index.add({'id':0,'href':'/docs/00-basics-of-python-and-introduction-to-machine-learning/','title':"00 Basics of Python and Introduction to Machine Learning",'section':"Docs",'content':"Basics of Python and Introduction to Machine Learning Welcome to the first hands-on session on Practical Machine Learning. In this session we\u0026rsquo;ll learn some basics of Python and Introduce ourselves to Machine Learning with Python.\nPython is THE go-to language for Machine Learning, and Data Sciences. This is because of the highly dynamic nature of python, which makes it very efficient to handle data. The language is also very intuitive, so one does not have to learn a lot of unnecessary syntax (such as brackets and semicolons). Another factor that has led to the popularity of Python is the large community. A large community means that platforms and libraries constantly grow, and bugs are solved very fast. A large community also offers support to beginners and professionals alike, which attracts them towards itself. Infact, Python has reached a point, where it is the only self-sufficient language to exist today, wiping all its competition in the past few years (such as R, Caffe, Java, etc).\nTo give you a deeper insight, a lot of frameworks have been developed on top of Python (meaning, a lot of libraries have been developed using Python, with the aim to make programmers\u0026rsquo; lives easier, so that they dont have to write the code from scratch). As you will see yourself, you can create a State-of-the-Art AI model that can differentiate between people (such as in a facial recognition system) or an object detection system (such as in a self-driving-car) in as few as 5 lines of Python code.\nThe Environment: Jupyter Notebooks The platform on which this tutorial has been developed is called a Jupyter Notebook. Here, you can write python code very efficiently. However, you are not required to use Jupyter Notebooks, although it is recommended. If you are comfortable using text editors such as VSCode, vim or PyCharm, that works too. You can find a lot of online tutorials to work with text editors. Any text editor will work!\nThere are a few perks to using Jupyter Notebooks.\n It is highly interactive. You can see, how we can even write notes and text inside the same document as your code. If you come across a bug in the code, you don\u0026rsquo;t need to rerun the entire code. You can simply fix the bug and continue writing code. Whereas in a text editor, you need to compile and run the code everytime you make a change and wish to run it. You will learn more about it as you go through the code.  Introduction to Jupyter Notebooks We write Python code in cells, as you can see below.\nWe\u0026rsquo;ve written a simple line of code that can print \u0026lsquo;Hello World\u0026rsquo;. Simply click on the code and run it. You can run it by either clicking the triangle on the left of the code, or pressing Shift + Enter\nprint('Hello World! This is a cell')\rBasics of Python Note 1: This is for those who are unfamiliar with Python and/or programming. If you are familiar with Python, you can skip this section. If you\u0026rsquo;re not writing the notebook from scratch, and simply running the notebook that has been provided to you through GitHub, you can simply click on the small arrow besides the to minimize or expand the entire subsection. So if you don\u0026rsquo;t want to view this subsection, for example, you can simply minimize it, and it will become easier for you to read. Note 2: The best way to learn any programming language is to learn over time. You should not try to learn the entire language before using it. The best way to learn is to learn through trial and error. So don\u0026rsquo;t be afraid to make errors. The more you make errors, the more you will learn. Having said that, we won\u0026rsquo;t teach you everything in this notebook - infact, we\u0026rsquo;ll only give you the basic tools to get started. And over time, we will introduce you to concepts as and when required. And you will learn a lot of stuff on your own as well! Variables and Variable Types Let us introduce you to a variable in Python. A variable is a container which can contain any value, like a number, a string, a list of strings, or even other variables. There are no restrictions to what type of value a variable may contain in python, unlike some other programming languages, where you can only put certain types of values in a particular variable. For example, in the language C, a variable that is designed to contain numbers, cannot contain a string of characters.\nYou can name your variable anything. The only things to keep in mind are -\n the variables are case sensitive. For example, \u0026lsquo;A\u0026rsquo; and \u0026lsquo;a\u0026rsquo; are two different variable names in Python. The name of the variable cannot start with a number. You can have both letters and numbers. You can also have the underscore \u0026quot; _ \u0026quot; in your variable name anywhere. There are certain words that Python uses internally. You cannot name your variable as one of those words. A list of these words can be found here.  Let us define a variable. You can edit the code and write anything instead of the string \u0026lsquo;Python\u0026rsquo;. Try writing your name in place of \u0026lsquo;Python\u0026rsquo; (make sure your value is contained within quotes. In python, single quotes and double quotes mean the same thing).\nYou also see a hash sign in the code below, followed by a statement. This is a comment, and python ignores everything that is written in a line after a hash sign. It is for you, the user, to write your comments and understand the code better. We\u0026rsquo;ll be adding comments throughout the code for you to understand the code better.\nmy_variable = 'Python' print(my_variable)\rNow go back to the above cell (by using the up arrow key, and instead of your name, type a number (anything - a whole number, a decimal number, etc) and rerun the cell (using shift+enter ), and now the code prints that number. So you can see, the same variable can take any value, and any type of value - no restrictions.\nThis is a good time to introduce you to the types of values in Python. Infact, this is common to most of the programming languages, so it is a good piece of information to know. As for now, let us introduce ourselves to three types of values -\n  Numbers: These are the values on which you can do arithemetic operations on. Like, addition, subtraction, multiplication, division, exponential, negation, etc. Numbers can be either\n Integers (represented by int) (like, -1,0,1,2\u0026hellip;and so on) Floating point numbers (represented by float) (like -1.0, 0.0, 1.00002, etc)  These are all the numbers that you would need most of the times. But you can also have complex numbers (represented by complex. For example 1 + 3j is a valid number in python.\n  String: These are the values on which you cannot perform arithemetic operations on. They usually represent names of things. Like your name, the letters of the alphabet, and so on.\n  booleans: True and False are called boolean values. Its a method to check a condition. For example: is 2 greater than 1? The answer is True.\n  Any value that does not fit in these categories is called None type. It represents an empty variable. In better words, the memory block in your computer is empty, if I assign it the value None.\nLet us explore these data types\n#writing multiple print statements would write all the values on different lines\rprint(1)\rprint(1.000)\rprint(1 + 5)\rprint(1.0 + 5.0)\rprint(type(1), type(1.0)) # you can write multiple things inside a print command, and it will print both the objects side by side.\r# type(...) returns the type of the item inside it. So, it tells that 1 is an int, and 1.0 is a float! #Excercise: Try printing the sum of the first non-zero digits (Starting from 1). Write your code below this line\r#let us try it with variables\rvariable_1 = 5\rvariable_2 = 6\rprint(variable_1 + variable_2)\rprint(variable_1*variable_2)\rprint(-variable_1)\r#Exercise for you! Declare variable variable_3 with another value. Now show 2 different ways in which #shifting brackets gives different results (because of BODMAS). Write your code below.\r#strings\rvariable_1='Lorem ipsum dolor sit amet'\rvariable_2='Duis aute irure dolor in reprehenderit'\rprint(type(variable_1))\r#you cant add two variables. For example 'a' + 'b' makes no absolute sense. But in Python, you can operate two strings with a '+' sign.\r#It doesnt add the two strings, but instead joins them (or in other words, concatenates them)\rprint(variable_1 + ' ' + variable_2)\rLists Lists are one of the most frequently used concepts in python. Lists are basically a collection of values, that is given a seperate name. Lists are represented by [ ]. For example:\nmy_list = [10,20,30,40,50] # the list of numbers 1 through 5\rprint(my_list)\rprint(type(my_list))\rRemember not to name your list as \u0026lsquo;list\u0026rsquo;, because \u0026lsquo;list\u0026rsquo; is a python keyword, that is reserved by the language.\nIn the context of Data Sciences, lists can be use to store all the data in one variable.\nYou can then isolate a particular element of the list. We do so by indexing, which is done by writing my_list[0] , for example. The zero is the index, which refers to the first element of the list, the value of which is 10, in this case.\nThe index can range from 0 to the number of elements in the list minus 1. This may be confusing at first - you may ask, why isnt the index simply from 1 to the number of elements in the list?! But that\u0026rsquo;s just how it is. Infact, this is used in many programming languages. An exception may be MATLAB.\nprint(my_list[0],my_list[1])\r#in addition to the range of the indices mentioned above, you can even write -1,-2 and so on. -1 simply means 'the last element of the list'\r#-2 would mean , the second last element and so on. print(my_list[-1])\rSlicing the lists:\nYou can slice lists, which means, you can choose a subsection of the list, and store it in another variable. This is done using [ : index ], where index is the index till which you want your elements to be sampled, the element at that index excluded.\nsub_list=my_list[:3] #sub_list will contain values till index number 2 (thats the third element, because, remember? indexing starts from 0)\r#index number 3, and all subsequent values are excluded\rprint(sub_list)\rSo in this method, you slice a list from the beginning. But you can also do from the end. So if we slice using [index_1:], we get a subsection, that contains values from index index_1 (included in this case), till the end\nsub_list_1 = my_list[3:] # we get values from index=3 till the end\rprint(sub_list_1)\rWhat if we want a subsection that is in the middle of the list (ie, does not contain either the end, or the beginning). We do so by [ index_1 : index_2 ] . As you may have guessed, we will get values from index_1 to index_2. index_1 WILL be included, but index_2 will not be included.\nOne way to memorize this is - imagine [ index_1 : index_2 ] as\n index_1 : + : index_2\n So, if you recall, index_1 : includes the index, while : index_2 excluded the index.\nsub_list_2=my_list[1:4]\rprint(sub_list_2) #the indices included are - 1,2,3\rLists can also contain strings, other variables, other lists and any other type of data. Infact, in a single list, all of these types of values can coexist\nmy_list_1 = [1,None,True,'hello',1.55]\rprint(my_list_1) # a list with different types of values\rFunctions You would know what a function is, from high school, or even first year calculus.\n y = f(x)\nA function f takes an input x and returns an output y.\nwe can define a function in python using the keyword def.\ndef f(x): return x**2\rWhat does this function do? First of all, notice the syntax of the function - it takes a value (say x). It returns x**2 (which means x to the power of 2, or x squared, in other words). The word return is the word which gives back the calculated value.\ny=f(5)\ry #Jupyter notebooks allow you to print the value of a variable without having to write print explicitly. #The only condition is, that you should write the name of the variable as the last piece of code in the cell. This is a comment,\r#so it doesnt count. But if you write any other code after the 'y', the y will not be printed unless you explicitly write print(y)\rBesides calculations, a function can do anything which may or may not needed to be returned. For example -\ndef f(x):\rprint('Hi, this function calculates the square of a number') #this is totally unrelated to the function, #but the function still carries it out\rreturn x**2 print(f(5)) #yes, you can put functions inside print. Infact, print itself is a function! #So the input to the print function is the output of the f function.\rHow about a function with more than one inputs?\ndef add_two_variables(x1,x2):\rreturn x1+x2+1\rsum_of_two_variable = add_two_variables(3,4) #sum_of_two_variables is a variable, to which we assign the value of the output of the function\rsum_of_two_variable\rEverything we do, we will do in terms of functions. Right now, we\u0026rsquo;ve only written very short peices of code. But your functions can be very long too. So, functions are an efficient way to repeat a task multiple times, without having to write down the code again and again. It also helps you to understand the code better. Its a concise way to express code.\n# Exercise for you: write a function f that takes 2 numbers, and returns two outputs: the sum of the two numbers, and the quotient. #the function should also print a statement that prints the difference of the two numbers. The order of the numbers in the operations does not matter\r#eg: def f(x1,x2). #y1,y2=f(number1,number2). In python, division is done using the '/' sign. (eg, num1/num2) If/else conditions What if you want to check a condition. Like, if you want to check if your variables value is greater than a number? This is how we check conditions.\nvariable_1=5 #try different values of the variable. But make sure the variable is a number. Because you cant compare a string with a number.\rif variable_1\u0026gt;3: print('yes, the value is greater than 3')\relse: print('no, the value is Less than 3')\rAlso, try making the value of the variable - a string. You will get an error! Thats okay. As we mentioned - errors are the things that teach you, and make you a better programmer! Change the value again, and run it again!\nFYI, the else statement is optional. if you only write if \u0026lt;condition\u0026gt;: \u0026lt;action\u0026gt;, it is still correct. Else basically means, if the above condition is not met, do something. So obviously, if you dont write the else part: if the above condition is not met, do nothing!\n  At this point, let us introduce you to comparison operators. In the above code, we did an inequality comparison (Greater than/less than). Some other operators are:\n x==y To check if x and y are equal x!=y to check if x and y are NOT equal x \u0026gt; y to check if x is greater than y x \u0026lt; y to check if x is less than y x \u0026gt;= y to check if x is greater than or equal to y x \u0026lt;= y to check if x is lesser than or equal to y  # Exercise for you!\r# Write a FUNCTION, that takes in two strings! within the function, # assign a variable the value True if the two strings are the same, # and false otherwise. Return the value of this variable\rfor loops A loop literally means to do something again and again. So for example, we want to compute the square of numbers from 1 to 20, we can write a loop function, which runs again and again, taking different inputs everytime.\nThe syntax is as follows\ninputs=[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\rfor i in inputs: print(i**2)\ri is a dummy variable - meaning a temporary variable which takes the values of the elements in the inputs list. Its actually just like plain english. You can simply guess what for i in inputs means. For a number (which, lets call it i for the sake of simplicity) in the list called inputs, do something! Isnt that simple to understand?\nActually, if you simply want all the numbers within a range - there is this function in python, called range.\nrange(20) returns all the numbers from 0 to 19. If you want your numbers between a particular range, say 1 and 20, we would write range(1,21). You would have guessed! just like before, the first index is included, but the last index is not!\nSo instead of for i in inputs, you can write for i in range(1,21), and you will get the same results.\n#Excercise for you! Instead of directly writing what to do inside the for loop, write a function (outside of the for loop), and use the function inside the loop\r#Your task? write a function which only prints the value if the value is even. Hence, your function will have no return value. So you can skip that line.\r# so, for all the values in the given range (say 1 to 20): if a value is even: print the value, otherwise - do nothing.\rThat is all the basics you need for Python. As we said, we would introduce you to important concepts in python as we proceed. We encourage you to explore the language on your own - and learn. You can never learn enough! And don\u0026rsquo;t be afraid to make mistakes. There\u0026rsquo;s no mistake that cannot be corrected!\nOne another important piece of advice is - to not spend too much time remembering syntaxes. More often than not, you will have access to the internet, and you can simply go online and search for the syntax. Infact, you can also find the code others may have written - so it just makes life easier for you. Ofcourse, treat this as a learning opportunity, and not as an excuse to not learn at all. Because if you don\u0026rsquo;t know your language and concepts well - you would never be able to fully utilize your knowledge.\nAnd finally, the language itself is not knowledge. It is only a tool to gain knowledge and experience. You will use python to build Machine Learning models, and so, its only fair to think of it as a stepping ladder towards your goal!\nMachine Learning The Practical Meaning of Machine Learning You must have come across a lot of theoretical definitions of Machine Learning. But a lot of those definitions are loaded with jargon. What do those definitions mean between the lines? What does Machine Learning actually mean in practice?\nMachine Learning is a fast evolving branch of study, which relates to, as the name suggests, making machines (computers) - \u0026ldquo;learn\u0026quot;, or \u0026ldquo;think\u0026rdquo; about information. If we can teach machines to \u0026ldquo;think\u0026rdquo;, it would certainly make our lives easier, since we don\u0026rsquo;t have to do the thinking.\nWe are surrounded by Machine Learning even in today\u0026rsquo;s world. You must have heard about glorified stories of self driving cars and self-aware robots, but we often overlook the ways in which Machine Learning already impacts our lives, in the most subtle ways possible. For example\u0026hellip;\n You don\u0026rsquo;t have to memorize directions anymore, because the Maps Application on your phone does that for you. Your GMail automatically sends spam email to the Spam section, and doesn\u0026rsquo;t let it appear in our inbox, so we don\u0026rsquo;t have to take the effort to do so. You don\u0026rsquo;t have to know the spelling of words to be able to type in today\u0026rsquo;s world. Your keyboard autocorrects any obvious mistakes, with what it thinks would best fit in the context. There are even companies based on this, like Grammarly. So, if you never thought about it, even though autocorrects may be annoying sometimes, its amazing how these systems are aware of the context of what you\u0026rsquo;re typing, and give you suggestions based on that.  Most modern phone keyboards also suggest next words based on context. They also learn your behaviours. For example, if you\u0026rsquo;re a person who uses a lot of emojis, the keyboard will learn when to suggest that you type emojis.\n  You don\u0026rsquo;t have to learn the local language of a place to be able to visit it. Google can translate between your language and the local language very efficiently, both ways. And its getting better day by day.\n  We dont need to take the effort to type anymore, we can simply say \u0026ldquo;Hey Google\u0026rdquo; or \u0026ldquo;Hey Siri\u0026rdquo; or \u0026ldquo;Hey Alexa\u0026rdquo; (depending on the device you have) and ask it to do something for you. And not just petty tasks, you can even ask it to do important tasks, like paying your bills, place orders, book appointments, etc. It even does not matter if you speak in a different accent, or even if you stutter, fumble or there\u0026rsquo;s noise in the background.\n  Do you observe a pattern in all the highlighted words? You would observe that all these reflect the idea of gathering knowledge about different things. We have somehow been able to teach Machines to think and do tasks for us, so that we don\u0026rsquo;t have to do them. The ultimate goal of Machine Learning is to be able to teach machines to do tasks that we do everyday, so that humans can focus on more intellectual tasks.\nThis is why AI and automation are called the 4th Industrial Revolution (The first three being The Steam Engine (1768\u0026rsquo;s), Oil and Electricity (and hence the Industrialization that followed)(1850\u0026rsquo;s),and Computerization (1970\u0026rsquo;s))\nIf you think about it, there\u0026rsquo;s a reason that each of these is called an Industrial Revolution. The Revolution refers to a change in the goals of development of the entire humanity.\nFor example, Before the steam engine, Trade was only limited to local communities, because travel was simply impossible over long routes. We developed as a species when travel became fast with the steam engine. Travel times were cut massively, which implied faster development.\nWith The second Industrial Revolution, production of goods became fast and cheap. Humans did not have to hand-produce essential goods. We developed as a race, because humans could now focus on more important and intellectual things, like development of Technology, Economy, Politics, etc.\nWith the third Industrial Revolution, We discovered how to make computers do the tasks that we did everyday. Computers could now handle Computation, Information Processing, and information storage. Communication Systems also improved. And the Internet emerged too! Just 20 years ago, there were no cell phones widely available. Today, we have highly evolved platforms to communicate - over Calls, Social Media, the \u0026ldquo;traditional\u0026rdquo; media, like theTelevision, etc. This has not only helped us connect better, but allowed us to work on even more intellectual tasks, such as Research, both Scientific and Social, Large Scale Global Trade, Space Exploration, Global Economy, etc.\nWhat AI and autonomity will allow us to do, is to focus on even more intellectual tasks, by doing the everyday cognitive tasks for us. If we can save the time and energy to do these everyday tasks, we can, as a collective human race, focus on more important tasks.\nBut How do you make Machines Learn? Machines are not hard-coded to gather knowledge. Infact, that is simply not possible. No human can write a computer program that can differentiate between two different faces. We ourselves do not know how to represent such a task using an algorithm. Its just something we \u0026lsquo;know\u0026rsquo; how to do.\nThe way machines learn to do such tasks is through identifying patterns in the Data we provide to it. That is the gist of all of Machine Learning. And all of the theory of Machine Learning is to simply develop algorithms that can learn information about the data. In the image below, you can see, how we feed data into the system. The system combines all this information to get meaningful results.\nFor example, if our model was to differentiate between two different people, we would \u0026ldquo;train\u0026rdquo; our Machine Learning model over a collection of pictures of these two people (the data), to learn features that diffentiate the two people. For example, the model may learn that one person is of different skin tone than the other, or one has a particular hairstyle, which is different from the other person\u0026rsquo;s hairstyle.\nWhat is a Machine Learning Model? A Machine Learning Model is the entity which gives us meaningful results. It is, at core, a mathematical function, which takes in data as the input and gives us results as the ouput. For example, the images of two people are the inputs, and the classification of whether the picture is of person A or B, is the result.\nDeveloping our own Machine Learning models. Let us build our own Machine Learning Models. Even if you don\u0026rsquo;t know anything about Machine Learning, thats okay - We provide you with all the necessary tools. The idea behind this is to motivate you towards Machine Learning, and to help you understand the extent of applications of Machine Learning. This is also to tell you that, Machine Learning is not black magic! And it does not require one to have a PhD degree or years of experience to dive into Machine Learning. You might not be able to understand how it works in total depth, but it certainly does not need a lot of knowledge to run your models.\nThis approach is called the top-down approach of learning, and this is how all of coding should be learnt. We learn how to use applications first, and then learn the details gradually, rather than the other way round. We\u0026rsquo;ve seen a lot of Machine Learning beginners simply lose interest in Machine Learning when they\u0026rsquo;re overloaded with a lot of theory, without getting the opportunity to utilize the knowledge. We all like to assure ourselves that all the effort of learning is worth it. So, while you will learn a lot of theory in lectures, these labs will give you the tools to apply your knowledge in the real world. And we assure you, you won\u0026rsquo;t build dummy models, but actual state-of-the-art models.\nYour first Machine Learning Model \u0026hellip; with fastai\nLet us begin by creating a model that will tell you which breed a cat or a dog belongs to, just by looking at an image of a cat or a dog. Before that, let us install the necessary library. For this particular problem, we\u0026rsquo;ll be using a library called fastai, which is a widely used library for deep learning, and makes it possible to train models efficiently without having to go in a lot of details of the code.\nWhat is Deep Learning? Deep Learning is a subset of Machine Learning, and is probably the most famous type of Machine Learning. If you decide to pursue Machine Learning in greater depth outside this course, you will find yourself almost predominantly exploring Deep Learning. It uses something known as Neural Networks as models (remember, a model is nothing but a mathematical function which maps the input to the output). The reason it is so famous, is because of the great flexibility that Neural Networks provide. They can solve any complexity of problems easily. We\u0026rsquo;ll learn more about Deep Learning as the course proceeds. The idea, at this point, is to simply let you know about what Machine Learning is about in the practical world.\nBy the way, this is what a Neural Network looks like. You may or may not have seen this image before.\nLet us begin by installing the fastai library. Before that, you need to enable your GPU. If you\u0026rsquo;re using Colab, all you need to do is click on Runtime on the top, then Change Runtime Type, and change the Hardware Accelerator from None to GPU.\nIf you\u0026rsquo;re using a PC instead of Colab, check if your PC has a GPU, that is CUDA enabled.\nIf it seems a little intimidating at first, don\u0026rsquo;t worry. Remember, you don\u0026rsquo;t need to learn all these technicalities at this stage. We will learn of all these technicalities as we proceed in the course. Right now, we are only introducing you to what Machine Learning looks like.\nDisclaimer: Before We start, Please keep in mind. Don\u0026rsquo;t try to memorize/and or understand the code per se. Ofcourse, if you can figure it out - great!. If you cannot, that\u0026rsquo;s okay. We only ask you to appreciate the beauty of Machine Learning.\n!pip install fastai --upgrade \u0026gt;.tmp\rfrom fastai.vision.all import *\rWe will now download the dataset called Oxford-IIIT Pets, which contains 37 different categories of breeds of cats and dogs.\npath = untar_data(URLs.PETS) # this peice of code downloads the data from the internet, #unzips all the files, and also tells us that our data is #stored at a location which we store in the variable named path\rpath\rThe data is stored in the location that is written just above this line, the name of which we temporarily store in the variable named path. Let us now visualize this dataset.\ndls = ImageDataLoaders.from_name_re(path, get_image_files(path/'images'), pat=r'^(.*)_\\d+.jpg', item_tfms=Resize(224))\rdls.show_batch()\rYou can see different breeds of cats or dogs, with their breed written on top of the corresponding image. So Now that our dataset is ready, lets jump into training our model! It will take about 5 minutes to train our state-of-the-art Pet breed classifier.\nlearn = cnn_learner(dls, resnet34, metrics=accuracy)\rlearn.fine_tune(4,3e-3)\rThere\u0026rsquo;s a lot of jargon here, like epoch and train_loss. Let us skip all these details for now. Basically, a Neural Network is trained in stages, and it keeps getting better and better each stage. Focus on the column that says accuracy - which tells us how good the model is performing at each of these stages.\nThe final value of the column tells us the accuracy of your model - basically, out of all the pictures, how many times does the model identify the breed of the cat or dog correctly. Your model gets the breed right for than 90% of the time. How amazing is that?!\nWhat\u0026rsquo;s even more amazing is to imagine what the model must have learned. To identify one breed from another, it must have to identify the colour, fur texture, fluffiness, proportions, facial features, size, and a lot of other things, which our human brains can identify effortlessly, and which we take for granted. Our brain truly is amazing.\nNow for the most exciting part! Let\u0026rsquo;s try this out on our own very picture. If you have a photo of a dog or a cat on your PC, upload it using the button that appears after you run the code in the cell below. Again, the code won\u0026rsquo;t make sense to you. We don\u0026rsquo;t expect you to understand right now either!\nfrom fastai.vision.widgets import *\rbtn_upload=widgets.FileUpload()\rbtn_upload\r#this is your image!\rimg = PILImage.create(btn_upload.data[-1])\rimg\rNote: If you\u0026rsquo;re running your code on a text editor on your local PC: you don\u0026rsquo;t have to use this upload widget. You can simply enter the absolute path of the image by writing the code:\npath=Path('\u0026lt;write absolute path of the image here\u0026gt;')\rimg=PILImage.open(path)\rAnd let\u0026rsquo;s see what the model think this is!\npred=learn.predict(img)[0]\ris_cat=' cat' if pred[0].isupper() else ' dog'\rpred + is_cat\rA lot of you must have uploaded a picture of something which is neither a cat nor a dog. And found out, that the model still predicts it to be one of the breeds of a cat or dog. Turns out the model is not as smart as our human brain! But we\u0026rsquo;ll discuss the reason in depth when we cover Neural Networks.\nHaving said that, if you haven\u0026rsquo;t tried uploading a picture of something which is neither a cat or dog, do so! Why not?\nInfact, here\u0026rsquo;s a fun experiment. You must know that not every dog or cat is suitable for everyone. A person fits best with a pet that has the same traits as the person, and same \u0026lsquo;spirit\u0026rsquo; as the person. And a lot of times, the owners and pets even LOOK alike.\nSo let\u0026rsquo;s find yours! Go back to the upload button above, click a selfie and upload your photo. And run the code below it. You will get a prediction of a cat or a dog breed! That\u0026rsquo;s your spirit animal.\nOfcourse, it may not be true. But its worth looking into. Who knows you start liking that breed more now!\nSummary We introduced ourselves to python, and Machine Learning as a broad area of study. We studied the basics of Python and learned some important concepts that are essential for Machine Learning, and Data Sciences in general. We learned what Machine Learning means in the practical world, and how it has evolved to impact our lives for good. And we even built our first Machine Learning model, and experimented with it.\nIn the next session, we\u0026rsquo;ll look into Linear and Logisitic Regression!\n"});index.add({'id':1,'href':'/docs/01-linear-regression/','title':"01 Linear Regression",'section':"Docs",'content':"Linear Regression Welcome to the second lab session of the Machine Learning Course. In this session, let\u0026rsquo;s look at a fundamental topic in Machine Learning - Linear Regression\nBefore we start, let us see what \u0026lsquo;Regression\u0026rsquo; means.\nRegression Regression is a statistical concept, which refers to finding relationships between different quantities. This lies at the core of Data Sciences. Finding how different types of data are related are used to find meaningful conclusions and decisions.\nIn the context of Machine Learning, Regression refers to the prediction of a continuous mathematical real-numbered variable as the output, when the model is given some inputs, in the form of variables. This output value is predicted by observing patterns in the data you provide to it.\nFor example: look at this image below and observe any pattern that you can interpret visually.\nThis chart shows the daily Covid-19 Cases in the United States. Do you see some trends? You would observe that the number of daily cases rise and fall and rise again, almost like a wave.\nHospitals need to prepare resources, such as hospital beds, ventilators, oxygen supply, and even testing kits well in advance. Thus, an estimate of the expected number is surely helpful.\nObserve the part towards the end of the curve on the right. The chart tells us, that everyday since 29 December 2019, the number of cases have crossed 200,000. Moreover, since December 1, till Jan 15, the number of cases have crossed 200,000 on all days except some 7 odd days. But to compensate this, the number of new cases cases have even risen beyond 300,000 on one day, and more than 250,000 on many days.\nIf you were to predict the number of cases on the following day, it would be very reasonable to the number of Covid-19 positive patients to be above 200,000, because thats the trend. Ofcourse, you never know, the number may be lower or even extremely high - It may even cross 400,000. But chances are the number will be in this ballpark. This is what regression means - to observe the patterns in the data and predict a value with high confidence. Here, given the input, such as the number of new cases in the past month, the number of tests carried out, and the history of the events of the past few days (for example, public events that may have acted as super-spreader events), we predict how many cases there will be in the future. Infact, this is what statisticians do - they predict how worse the pandemic can get, and when the curve is expected to flatten, using these techniques.\nOne important concept to understand is - that we can only predict our results, with a probability (or, in other words, with high confidence), and so, there is always a chance that our prediction is wrong, or off by a large margin. Our job is to give the best possible results, based on the data that we have been provided with. And a larger probability (ie, higher confidence) is desirable, because we can be more sure that our results will be accurate (or close to the actual value).\nLet us begin with our discussion on Linear Regression!\nLinear Regression Given some data x, the results of which we know, and represent as y (which may be refered to as the target value, or the ground truth value) (keep in mind, the target values are not predictions - they are actual values which were factually observed in the real world), our simplest approach can be to map a function y=f(x) that maps the relation between the inputs (x) and the output (y).\nThe simplest function that can be made to \u0026ldquo;learn\u0026rdquo; these relations and patterns is a linear function. Given an input x, the output will be given as:\n f(x) = w.x + b #this is our model. '.' represents multiplication pred = f(x) #pred is the prediction of the output, given some input x The coefficient of x, ie, w, and the intercept term b are the parameters of our linear function. If the linear function is designed properly, meaning, if w and b are given proper values, then pred can be closer to the actual output y. Note that our parameters are real valued numbers.\nThis is essentially learning the patterns of the data. So now, when you want to predict the output, of which you do not know the ground truth output (for example, future prediction), the model will predict the output based on the data we had provided to it. This is because, we assume, that all data shows a common pattern. Since, we have learned that pattern by adjusting our parameters, our predictions are expected to give good results.\nAbove is shown the linear regression technique for ONE independent variable (x) which is to be mapped to ONE dependent variable (y). You may have multiple dependent variables (x1, x2, x3 and so on), which is to be mapped to the dependent variable (y) , and in that case, the model looks like:\nf(x1,x2,x3 ...) = w1.x1 + w2.x2 + w3.x3 + .... + b pred = f(x1,x2,x3 ...) In this picture, we fit a linear regression model to the data. Now for an unseen data, the model would predict a value that lies on this line. This is a fairly accurate representation of the data, since we observe that all the data more or less lies alone a straight line.\nAs an example, let us take a look at the price of the Google stock in the New York Stock Exchange, over the course of many years.\nCase Study: The New York Stock Exchange Setting up our Data* Let us begin by downloading the NYSE Dataset, which contains details of the . If you click on the hyperlink, you will be redirected to a website called kaggle.com. This website is one of the most famous websites for Data Science Practitioners. It provides access to many datasets, both small-scale and large-scale, and a platform for practitioners to discuss their solutions, share ideas, and even compete. Many competetions are even paid, meaning the winner of the competition wins a price money of thousands of US dollars, sometimes even in millions! Infact, many game-changing Machine Learning research papers have resulted from kaggle competitions. And many top kagglers (as they are called), called as kaggle grandmasters are known to have been offered high-paid positions as Data Scientists at top tech-companies and research institutes.\nTo download datasets from Kaggle, we need an account. So head over to the website and make an account! You can simply signup using your google account, without having to go through the entire procedure!\nKaggle provides us with an API (a function that saves us from a lot of technicalities) to pull datasets from the website. For that, we need to download a file which contains the mechanism to download datasets over the cloud. This file is goes by the name kaggle.json, and is unique to each kaggle account (This is done so that Kaggle knows who is downloading the dataset, because when a lot of data belongs to companies, which have their own legal requirements. So this is a method to hold accountability against any user who misuses the data. We don\u0026rsquo;t need to worry about it!)\nSo\u0026hellip;\n Head over to www.kaggle.com On the top right corner, click on your profile photo. Then go to My Profile Click on Account on the top menu bar. Click on Create new API Token. A file named kaggle.json will be downloaded!  Ideally, the API expects the file to be located in your computer at the location ~/.kaggle/kaggle.json.\n(So if you\u0026rsquo;re using your PC rather than this notebook, you can simply create a folder named .kaggle in your root directory, and put the kaggle.json file in that directory)\nFor those who are using this notebook, this poses a problem! We are not running this notebook using our own PC\u0026rsquo;s CPU/GPU, but rather running this notebook on a PC on a remote Google server, and we don\u0026rsquo;t have the interface to the PC on which this notebook is running. We only have access to this notebook. But, below we have provided you with a peice of code, that lets you upload the kaggle.json file from your computer, and send it to the remote PC on which we\u0026rsquo;re working. The source of the code can be found here.\nUpload the kaggle.json file using the Choose Files button after running the cell below. Make sure the name of the file is nothing else other than kaggle.json.\n#Code to upload kaggle.json from local PC to the remote server from google.colab import files uploaded = files.upload() for fn in uploaded.keys(): print('User uploaded file \u0026quot;{name}\u0026quot; with length {length} bytes'.format( name=fn, length=len(uploaded[fn]))) # Then move kaggle.json into the folder where the API expects to find it. !mkdir -p ~/.kaggle/ \u0026amp;\u0026amp; mv kaggle.json ~/.kaggle/ \u0026amp;\u0026amp; chmod 600 ~/.kaggle/kaggle.json Then, to download the dataset, we go to the dataset page, click on the three vertical dots on the right, and click on Copy API command.\nNow paste this command in the cell below. But to run this code, we need to add a ! at the beginning of the command. This is because this code is not a python code, but a terminal code. The ! tells that Jupyter Notebook that the code written following this, is a terminal code, and not in python. We\u0026rsquo;ve already written this for you\u0026hellip;\n!kaggle datasets download -d dgawlik/nyse Next, unzip the dataset file (which is downloaded in the form of a zip file). We also move the contents of the file to a new folder, which we name \u0026ldquo;nyse_dataset\u0026rdquo;.\nOnce we unzip the dataset, we take a look at the dataset folder using the ls terminal command\n!mkdir nyse_dataset #mkdir is the terminal command to make a new folder with a specific name (which here, is \u0026quot;nyse_dataset\u0026quot;) !unzip nyse.zip -d nyse_dataset #unzip the nyse.zip file and move the contents to the folder named \u0026quot;nyse_dataset\u0026quot; %cd /content/nyse_dataset !ls #These are the files inside the downloaded dataset That\u0026rsquo;s it regarding the setup of our dataset! The process may seem slightly overwhelming at first, but its really not. Its just 3 steps -\n Make sure your API key (the json file) is present. If you\u0026rsquo;re using your local PC to run this code, you dont even have to do this again and again. Download the dataset using the command that Kaggle provides you with Uzip it and visualize it!  It becomes almost effortless over time. So do not worry if you cannot wrap your head around everything at once. The important idea is to understand the workflow.\nNow lets actually start processing our data!\nWorking with our data We will be carrying out our analysis on the prices-split-adjusted.csv file. Notice that our file is a .csv file, which stands for \u0026ldquo;comma separated values\u0026rdquo;. Its similar to an Excel file containing data. If you open a csv file using any text editor, you would find data in a row separated with commas, and each line acting as a whole row.\npandas is one of the most widely used libraries in python to handle csv files. We will create a \u0026ldquo;dataframe\u0026rdquo; using this library, which is nothing but the data represented in the form of a table. This can be used to extract certain values in the data.\nimport pandas as pd # we call the pandas library and give it an alias (pd), which we will use as a shortcut to call pandas dataframe = pd.read_csv( 'prices-split-adjusted.csv', parse_dates=['date']) #store the dataset in the prices-split-adjusted.csv in the variable called dataframe dataframe.head() #.head() is a method to print the first 5 rows of the dataframe Let us understand this dataset. The first column of the dataframe is the date on which the stock prices are recorded. According to the dataset description on the kaggle page, the stocks are recorded from 2010 to 2016. The second column is the Stock Exchange symbol of the company. To find all the companies in the dataset, we use the unique() method, which finds all unique values in a column.\ndataframe['symbol'].unique() We\u0026rsquo;ll work with the dataset of only one company - Google (Now called the Alphabet Inc Company) (the symbol of which is \u0026lsquo;GOOG\u0026rsquo;.)\ngoogle_df = dataframe[dataframe['symbol']=='GOOG'].set_index('date') google_df.head() #notice, the dates act as the indices of the dataframe, ie the key that we can use to refer to a particular row len(google_df) #let us check how many rows of data do we have #additional information: #to extract one row of data, say of the date 2010-01-04, we use the loc method: google_df.loc['2010-01-04'] The remaining of the columns are details of the stock price and company, such as the opening price on any day, the closing price, the highest and lowest stock price on any day, and the volumes of trades done on a day. We are concerned with the closing price of the google stock on each of these days. We are given the stock prices between 2010 and 2016. Let us try to find what the stock price may be in the future. We will find the stock price at the end of each year from 2017 through 2020, and also try to predict what the stock price may be at the end of 2025, if the stock were to increase in value at the same rate.\nLet us start by visualizing the data. We use the matplotlib library to plot our data. It is The most widely used python library to plot and visulaize data. Again, we\u0026rsquo;ll be using an alias (plt) to avoid writing the entire phrase again and again.\nimport matplotlib.pyplot as plt plt.figure(figsize=(15, 7)) plt.plot(google_df.close) #the x axis is the date, the y axis is the closing price of the stock on any given day Can you guess which variable is the dependent variable and which is independent?\nThe time is the independent variable, and the stock price is the dependent variable. Let us put them in the variables x (for independent variable), and y (for dependent variable). But how do we use the date as an input? Remember, all values need to real numbers.\nOne approach is to use the number of days since the first day that is recorded in the dataset. Let us use this as the independent variable. We will make two different lists for x and y respectively.\nfirst_day = google_df.index.min() #remember, dates are indices to the dataframe first_day #this is the day relative to which we'll calculate the number of days passed. This refers to a specific time - 4 January 2010, at 00:00:00 hrs\nLet us define a function that takes in a date from the dataframe, and calculates the number of days since this date\ndef calculate_days_since(date,reference=first_day): return (date-reference).days #'date minus reference' would return the difference in terms of the exact time elapsed, till the exact milliseconds. #the .days method ignores everything except the number of days, which is what we require! You might be wondering? how can we subtract two dates (date and reference)? Actually, go back above, and notice the statement where we read the csv file. there\u0026rsquo;s an argument that says parse_dates: which tells pandas to not consider the column \u0026lsquo;dates\u0026rsquo; as a string, but as meaningful objects. By writing this argument, pandas now knows that these strings are dates, and provides us with methods to handle them. One such is that, we can subtract two dates to get the difference.\nInternally, all dates are converted into objects of another python library, called the datetime. So if we are to pass a date to this function, we need to first convert it to a datetime object. So let us import that library.\nfrom datetime import datetime Let us look at if the function we defined above works. Lets see, how many days have passed between the reference date (4 Jan 2010) and 1 Jan 2021.\ncalculate_days_since(datetime(2021,1,1)) #The syntax of datetime is datetime(year, month, day) Yes, that works perfectly. If it doesn\u0026rsquo;t, look if the syntax is correct (including, if brackets are opened and closed properly, and if the inputs are correct.\nLet us, finally, build our dataset (meaning, our x and y)\n x=list(map(calculate_days_since,google_df.index)) x[:10] #seeing the first 10 items of our x y=list(google_df.close) y[:10] assert len(x)==len(y) There\u0026rsquo;s some python jargon here. Let us look at these. Everytime we come across a new, important concept in Python, we\u0026rsquo;ll write a small note with the heading \u0026ldquo;LESSONS IN PYTHON\u0026rdquo;\n  Lessons in Python\n   x=list(map(calculate_days_since,google_df.index)) map is a function that can be used to apply a function over every value in a collection of values. Here, we wish to apply the calculate_days_since function over every value in the collection of dates, which is given by google_df.index (remember, the dates are the indices of the dataframe, and the .index method returns a collection of all indices).\nSo, this function takes values from google_df.index, and pass it through the calculate_days_since function, and returns a collection of the values as map object. Obviously, this map object is of no use to us, we want the result to be in the form of a list. So we pass the result of the map function to the function list(), which returns a list of the values.\nassert len(x)==len(y) assert is a sleek method in python to check a condition. If the condition is satisfied, it does * nothing *. If the condition is not satisfied, an error is raised. This immediately aborts all functionality, and can be used as an effective debugging tool, to find out where exactly in the code an error exists. And if no error exists, this method does not interfere with the functionality of the code. Here, we are making sure that the number of items in our dependent variable y is the same as the number of items in our independent variable x. So if no output is given, or no action takes place, it means that indeed, the condition is satisfied, and we\u0026rsquo;re good to go!\n   Let us visualize the data using the plt.plot(x_axis,y_axis).\nplt.plot(x,y) plt.ylabel('Stock price (closing)'); plt.xlabel('Days since 4 Jan 2010'); The Linear Regression Model We will look at two methods to build a Linear Regression Method:\n By using a direct formula. The Gradient Descent.  The idea of Linear Regression is to make sure that our predictions are as close to the target value as possible. To ensure this, we use the method of Least squares. In other words, our objective is: Minimize the difference between the predictions and the ground truth. Since we do not care about the sign of the difference, we square the difference, in order to ensure we always get a positive value.\nHence our goal can be written as:\n$$Minimize \\frac{1}{n} \\sum_1^n(pred - y)^2$$.\nWe call this the loss function. Smaller the value of the loss function, the better the model is considered. It is nothing but the average sum of squared errors.\nMethod 1: Direct Formula: Given some input x and output y, you can directly calculating the line of best fit using the formula described below. : (Disclaimer: You Don\u0026rsquo;t need to memorize these formulae!)\n$$ y= w_{1}x_1 + w_2x_2 + \u0026hellip;. + w_k x_k + b$$\n$$w_i = \\frac{\\sum_1^n(x_i - \\bar{x_i})(y - \\bar{y})}{\\sum_1^n(x_i - \\bar{x_i})}$$\n$$b= \\frac{1}{n} \\left {\\sum_{1}^{n} y - \\sum_{i=1}^{k} w_i (\\sum_{1}^{n}x_i) \\right }$$\nWhere n is the number of training points, k is the number of independent variables, and all other notations have standard mathematical meanings. These calculations come from solving a differential equation in order to minimize the square of the difference.\nThis seems quite confusing. Fortunately, there are many python libraries out there that do this calculation for us. One such library is the scikitlearn library. Let us import that library.\nfrom sklearn.linear_model import LinearRegression Here is the documentation of the function. The fit() method is used to find the parameters of the model (w and b). However, according to the documentation, the model expects each data point to be a separate list, and all these lists to be contained in one massive list.\n[[i] for i in x] does that for us. This method of making a list is called list comprehensions. Basically, for every value in x, which we denote as i, put a list containing i ([i]) inside the list.\nmodified_x = [[i] for i in x] print(modified_x[:10]) reg=LinearRegression().fit(modified_x,y) The weights w are stored in the coef_ method of the reg object, and the intercept term b is stored in the intercept_ method.\nw,b = reg.coef_ , reg.intercept_ w,b Let us see how good this performs: #we define a function that predicts the values according to this model def lin_model(x): return x*w + b plt.plot(x,y) plt.plot(x,[lin_model(o) for o in x]) plt.ylabel('Stock price (closing)'); plt.xlabel('Days since 4 Jan 2010'); Looks pretty accurate! Visually too, you can see that the line fits pretty well within the data.\nday=datetime(2017,12,31) days_elapsed= calculate_days_since(day) days_elapsed lin_model(days_elapsed) Method 2: Gradient Descent Gradient Descent is an algorithm which iteratively updates the values of the parameters until a goal is met, using derivatives. In our case, the goal is to minimize the loss function as we mentioned above.\nThe way Gradient Descent works is by minimization of the loss function through the concept of gradients. Below is a dummy example:\nConsider a function that needs to be reduced. Let this be a quadratic function for the sake of simplicity. Our job is to find the value of x (on the x axis), that makes y (the independent variable), minimum!\nSuppose you start from a random point on this graph, say (x,y) = (4,16). How do we, from here, proceed towards the minima of this graph?\nGradients provide the answer! Gradients, if you don\u0026rsquo;t know, is the slope of the curve. You must have studied about the method to calculate the gradient - rise over run. If you do not know what gradients are, head over to Khan Academy. Here and here are some videos to get you started!\nIf you now have some idea about gradients, let us proceed with our discussion on gradients. Gradients are real valued numbers, which represent how the values of the graph are changing.\nLet us start by assigning a totally random value to our parameter x. Now, at the point (4,16) (the red point), can you guess whether the gradient of y with respect to x, is positive or negative?\nIt is positive! Infact, if you know how to calculate the gradient of a quadratic function $$f(x)=x^2$$, it is simply $$f\u0026rsquo;(x) = 2x$$. So at x=4, the gradient is simply +8.\nGradient Descent continuously updates the parameter (x) by a value opposite to the sign of the gradient of the dependent variable(y) with respect to the parameter(x).\nIf the gradient of y with respect to x is positive, update x with a negative number. (Meaning, Reduce the value of x by a certain number.)\nIf the gradient with respect to the parameter(x) is negative, increase the parameter(x). If you continuously update the parameters using this method, you will reach to the minima of the function.\nOur parameter is continuously updated in steps that move towards the mimima. x goes from 4 to 3 to 2.2 to 1.5 and so on, until it reaches 0\nHow do we represent this practically?\n#Step 1: calculate gradients gradient(x) = 2*x #(gradient of the quadratic function x**2 is 2*x) #Step 2: update the parameters x -= step_size*gradient(x) #x-=number is the same as x=x-number. Let us understand the weight update step. You would understand why we are subtracting from x. This is because we are taking a step of the opposite sign of the gradient. If the gradient is positive, x increases by step_size*gradient(x), and if negative, x decreases by the magnitide step_size*gradient(x).\nWhat is step_size? Step_size is a small positive number, which lets us control how big a parameter update takes place. When we study Neural Networks, you will see why we need to control the size of the step. But you can get a basic intuition about the idea - if the step_size is too small, it\u0026rsquo;ll take forever to reach the minima. If the step_size is too large, you may overshoot the minima. Meaning, for example, if you\u0026rsquo;re currently at x=0.1, you need to update the parameter by 0.1 to reach x=0. But because of the high learning rate, you can only take a step bigger than this, and thus never reach the minima. The step_size is also known as the learning rate in Machine Learning. So keep this term in mind when we further study Gradient Descent, predominantly during Neural Networks!\nWhat happens when we finally reach x=0. We know that x=0 is the minima of this curve. But also notice, that the gradient with respect to x is also 0. So the weight update step does not update the parameter, and the value remains stagnant. At this point, we have fulfilled our goal! Notice how we started from a random value and still reached the minima using this algorithm!\n So how would Gradient Descent look like for our problem? What would be our model function (y)? and what would be the independent function (x)?\ny is the function which needs to be minimized, so this would be our Loss Function (the average sum of squared error between the target and model predictions). x would be our parameters which need to updated such that y (the dependent variable) is minimum. So we\u0026rsquo;ll be running the gradient descent separately for each of the parameters (w1,w2,w3\u0026hellip;.,wk,b)\nImplementing Gradient Descent To implement this library, we\u0026rsquo;ll be using PyTorch, a framework built on top of Python, which is known for efficiently handling Deep Learning (ie, the Machine Learning concerned with Neural Networks). The reason we are using PyTorch, is because PyTorch can calculate the gradients for us, without us having to tell the mathematical formula for the gradient. You can calculate the gradients for ANY function, however complex, and we don\u0026rsquo;t even have to know how to calculate the gradient. It may seem bizarre at first - you may ask, how would pytorch know what our model, or loss function is! And so, how would it know how to calculate the gradients? Does it know the gradients of all functions in this universe? We will see how this works. This mechanism is called autograd.\n#let us start by importing the PyTorch library, which is denoted by torch import torch We have 2 parameters, w and b, that we need to find values of, such that predictions are such that the loss function is minimum. (This sentence summarizes the entire concept of Linear Regression, so make sure you go through it again and understand it well). As we mentioned before, we will first start with random values for w and b.\nparams=torch.randn(2).requires_grad_() #initialize w and b with a random value of size 1. This is the dimension sizes. w,b=params w,b Notice that these parameters are in the form of tensors. What are tensors? Tensors are PyTorch specific data structures that can efficiently handle data. They are basically n-dimensonal matrix-like structure. For example, a one dimensional data structure is simply a list. A 2 dimensional structure is a 2-D matrix. A 3D structure is a 3-D Matrix (having height, width and depth), and so on. So you can have very complex data types within tensors. Tensors make operations very efficient!\nWe also added a .requires_grad_() in front of the random initialization of these parameters. This function is available only for tensors, so you cant randomly append this function to any other datatype. This is the pytorch function which tells PyTorch that these parameters need to be updated.\nSo, naturally, it needs gradients, in order to be updated (refer back to the parameter update formula). And once we tell PyTorch that these variables need gradients, it will keep track of all operations you do on those variables. So it knows how to calculate the gradient for this. For example, if your model is a quadratic function, PyTorch will keep track of this, and hence know internally, how to calculate the gradient of the quadratic function. We\u0026rsquo;ll be discussing this in more depth later.\nLet us now define our model:\ndef model(x,params=params): w, b= params return w*x +b Here comes another aspect of PyTorch. It expects all operations to be done between tensors only. So let us convert our data to tensors!\nx_tensor,y_tensor = torch.tensor(x).float() , torch.tensor(y).float() #PyTorch expects everything to be floats. You dont have to know this fact at this point x_tensor.shape, y_tensor.shape #Let us see if the model works on our data tensors... preds=model(x_tensor) preds.shape Let us run this model on our inputs:\nAnd let us finally calculate the loss function. Let us define this function. We use a predefined Mean Square Error Loss function, which is defined in PyTorch. If you ever want to find out how to import a functionality from a library - a quick Google search always helps. For example, you can type \u0026ldquo;PyTorch MSE loss\u0026rdquo;, and you would surely get some results. Even if a functionality doesn\u0026rsquo;t exist in the library, you may find discussions on some forum related to your query.\nBy searching, we find that MSE is defined at a specific submodule of the torch library - called the \u0026ldquo;nn\u0026rdquo;.\nloss_fn=torch.nn.MSELoss() #Pytorch already implements the Mean Square Error Loss function for us. #It is equivalent to mean((pred-target)**2) def loss_LinearRegression(pred,target): assert len(pred)==len(target) #we need to make sure that there are as many predictions as targets. If not, then there is some error in our code return loss_fn(pred,target) #let us test this function on our predictions and target values loss=loss_LinearRegression(preds,y_tensor) loss Now, the next step is to calculate the gradient of our parameters with respect to the loss function. Remember, moving in the direction opoosite to the sign of the gradient would minimize the function of which the gradients are calculated (in this case, the loss function). Calculating Gradients is as simple as typing the .backward() function\nloss.backward() #gradients have now been calculated. Let us check these gradients params.grad #the .grad method of tensors tell us the gradients of the loss function with respect to these parameters. #if you calculate the gradients by hand, you would get this value only! Finally, let us update our parameters.\nlr=1e-4 params.data-=lr*params.grad params #go back up and verify that the value of params have indeed changed! Let us also define a function that does one complete cycle.\nparams = torch.zeros(2).requires_grad_() #re-initializing our parameters def train_one_iteration(params,lr=1e-4): #In python 1e-4 is the same as saying 10^-4. Similarly, 2e5 would mean 2x(10^+5). And so on... preds = model(x_tensor, params) loss = loss_LinearRegression(preds, y_tensor) loss.backward() params.data -= lr * params.grad.data params.grad = None #The details of this peice of code will be discussed in the Neural Networks section. But note: it is an essential step Now let us finally run the Gradient Descent Algorithm. Remember, this is an iterative algorithm, and needs to be run multiple times. In each run, or iteration, the loss will reduce, and the predictions will hence improve! Let us first see how the model predicts initially, before training.\n#since we plan to visulaize the model in terms of a plot again and again, it is useful to write a function. #Remember, functions are an efficient way to avoid writing the same peices of code again and again def plot_linreg_model(params): plt.plot(x_tensor,y_tensor) pred=model(x_tensor,params).detach() plt.plot(x_tensor,pred) print('MSE Loss:',loss_LinearRegression(pred,y_tensor).item()) plot_linreg_model(params) for _ in range(50): train_one_iteration(params,lr=2e-7) #run this algorithm 50 times plot_linreg_model(params) The model has definitely improved! You can even see, the loss has improved. But if you go back to the section where we calculated the best fit line through the direct formula, you\u0026rsquo;d see that this fit is still off. So let us run it for some more iterations.\nfor _ in range(1000): train_one_iteration(params,lr=2e-7) plot_linreg_model(params) At this point, you\u0026rsquo;d see that the improvement is almost very small. This is because we are close to the minima already. When we\u0026rsquo;re close to the minima, the gradient becomes very small (you can visualize this by taking the example of the quadratic function example we introduced earlier). So the parameter update too is very small. So now that we know the model is not improving anymore, let us leave it at that.\nInference Now, let us see what this model predicts the stock price to be at the end of 2018, and compare it with the actual closing price of the Google stock at the end of 2018. Remember, our input x is the number of days passed since the reference date. So we use a function which we already defined, to find out the number of days between 31 Dec 2018, and the reference date.\nday=datetime(2018,12,31) days_elapsed= calculate_days_since(day) days_elapsed model(days_elapsed,params).item() Let us look at the actual price at the end of 2018.\nYou can see, that at the end of 2018, the stock price was about USD 1037. This is not very far from the prediction our own model!\nCongratulations, you have successfully built your Linear Regression Model!\nReview Below we\u0026rsquo;ve given an exercise for you to try out. But before that, there are a few tips and review points, that you should go through, think over, and make sure you understand each of them. These points are the most important points of this entire session - and not surprisingly, these are the concepts and ideas that will actually help you be a good Machine Learning practitioner.\nSome Tips:   Machine Learning is rapidly evolving to be more about efficiency, rather than depth of knowledge. Just having the skill to implement a model is far more valuable than the vast theory of someone who spent years reading books. Knowing your concepts is more important than the unnecessary details. For example, we didn\u0026rsquo;t have to know how to calculate gradients in order to do gradient descent. If you ask someone who has learnt Machine Learning the hard way, they would, almost anyday, say that it is not possible to do Gradient Descent without knowing enough calculus to calculate gradients. But clearly, that is not true anymore. This is the magic of the wonderful frameworks that have been developed by the AI community to facilitate learning. But, that does not mean you should skip the concepts comepletely. Not having to do something, and not knowing something at all are two completely different things. You cannot escape learning what gradients are, and how they are calculated. Just that, you don\u0026rsquo;t Need to calculate them. Its like, having a calculator to do your math is a luxury, but a person who does not know how to add numbers at all cannot thrive.\n  Having said that, this is the approach you should follow. You should think smart - all functionalities need not be written from scratch. Often, a pre-existing code might be the way to go, and maybe even more efficient. Ofcourse, this does not apply in every context (for example, you cannot simply use someone\u0026rsquo;s copy-righted code for commercial purposes, because of legal implecations), but wherever it applies, think in that direction.\n  Finally, the ability to search for your queries is The MOST valuable skill you will learn as a developer. Knowing what and how to Google search your queries is a skill, which comes over time, but is a wonderful skill to acquire. You need not memorize syntaxes of commands, or even know the complete functionality a function may have. You can always do a quick search over documentations and forums. But again, Do Not think of this as an excuse to not learn at all. Just till it doesnt not come to you naturally, you may use these tools as an aid.\n  Review Questions These are non-evaluative, but highly recommended to go through. Make sure you clearly know the answer to each of these concepts. The answers to all these questions are somewhere in this notebook, so if you find yourself unclear with a concept, go back up and find the answer!\n  What is a Machine Learning model in terms of code? Can you write the syntax of a model in a simple pseudo-code form?\n  We imported many important libraries in this notebook. Whenever a library was imported, the reason and the goal was mentioned. Can you enlist all the libraries used in this notebook, and why they were used?\n  Can you describe in 3-5 one-line statements, the steps to carry out Linear Regression?\n  what is a tensor?\n  Why are dataframes so popular with handling tabular data?\n  What is a loss function? Which loss function did we use for Linear Regression?\n  Exercise Problem 1: Building your own Linear Regression Stock Prediction Model. Can you carry out Linear Regression using Gradient Descent for 2 other stocks. Amazon (AMZN), and NASDAQ (NDAQ). You need to redefine all functions, and may use the functionalities that we have already defined. In addition to the number of the days passed since the reference date, also use the volume of trade as an input feature. (Hence, you now have 3 parameters, w1,w2 and b). Report the parameters and loss in terms of a dataframe. Also plot the linear regression model with the datapoints (as shown above) for both these stocks (stock price vs number of days passed). All the major steps will remain the same. You may need to do some minor tweaks to consider the extra input feature.\n Problem 2: Non Linear Regression We saw Linear Regression, which aimed at fitting a straight line into the dataset. But what if we wish to fit a non-linear curve into the data. This can be done by replacing a linear function with a polynomial function.\nInstead of:\ny= w.x + b We could use a non-linear function:\ny=w1.(x**3) + w2.(x**2) + w3.(x) + b So now, we have a cubic function, instead of a linear function. This can be used to fit a non-linear curve. Generally speaking this is more flexible than a linear curve. It can act as a cubic function, a quadratic function (if you set w1 as 0), a linear function (if w1 and w2 both are 0), or something in between these curves. The method of regression remains the same. Can you implement Non Linear Regression using the cubic function as the model?\nAdditional tips: Cubing a value exponentially increases it in magnitude. Computer CPUs cannot handle very large values. This is known as Model Explosion. So we first divide each input by 100, and then apply this model.\nWithin your model function: Do the following transformation on the input x\ninput=torch.div(x.detach(),100)\nThen use the input variable as the model input.\nw1*(input**3) + w2*(input**2) + w3*(input) +b .\n Use 20 iterations, and a learning rate of 2e-8.\nDoes this model perform better than the previous linear regression model?\nAdditionally, try different Learning Rates to see if you can improve the results. Show 5 different reasonable learning rates, and show the plots.\n "});index.add({'id':2,'href':'/docs/02-logistic-regression/','title':"02 Logistic Regression",'section':"Docs",'content':"Logistic Regression We studied about Linear Regression in the last session. Let us learn about another fundamental concept in Machine Learning: The Logistic Regression.\nSo let us first start by dissecting the terms and try to get an intuition of what logistic regression means.\nLogistic Regression: an Intuition You may recognize the term \u0026ldquo;Regression\u0026rdquo;. Do you recall what Regression means in Machine Learning?\nRegression is prediction of a value, given some inputs. This value is calculated by feeding the input to a function. This function is adjusted in a way, that it gives us the best predictions over the data that we provide to it.\nThen we studied about Linear Regression, which is a regression technique that uses a linear function as the model. We also studied about non-linear regression as an extension of linear regression.\nLet us now study about Logistic Regression.\nHowever, there\u0026rsquo;s a small catch here. Logistic Regression is not really Regression, but a classification technique. What is classification?\nClassification means to classify items into categories, or classes. For example, among a collection of pictures of dogs and cats, we would like our model to tell which one is which.\nWe\u0026rsquo;ll be looking at a lot of classification techniques over this course. Logistic Regression will be the first of these.\nSo let us first understand how logistic regression works, and then we\u0026rsquo;ll surely answer the question - Why is Logistic Regression called Regression, if its a classification technique?\nLogistic Regression If you were to break down what Linear Regression is - you would realize, it simply means - Regression using a Linear Model. By the same logic, Logistic Regression would relate to a Logistic Model (function).\nWhat is a Logistic function?\nLogistic Function The logistic function or the sigmoid function is defined as below:\n$$sigmoid(x) = \\frac{1}{1+e^{-x}}$$\nIt looks something like this: So, if this is a Logistic function, what would logistic regression mean? Naturally it would mean modeling the data using a logistic function. Ideally it would be helpful in a case where the data is distributed more or less over a sigmoid function.\nBut we don\u0026rsquo;t usually see data distributed in this fashion - there are almost no practical scenarios where data is distributed in this fashion. Actually, this function is not used for prediction of a value at all. As we said, it is used for classification. But how can we use a function for classification?\nBefore we start explaining that, let us point out a unique property of this function. Notice how the function strictly lies between 0 and 1. It approaches 1 as the input approaches $+\\infty$, and 0 as the input approaches $-\\infty$.\nBefore moving forward, let us make sure, we implement the function in python. Infact, we\u0026rsquo;ll try to implement whatever concept we introduce. And we\u0026rsquo;ll also tell you tricks, like when you don\u0026rsquo;t need to implement a function, and can simply use a pre-implemented version of the code.\nWe begin by importing the library PyTorch. As you know, our data would be in the form of Tensors, so PyTorch gives us great tools to handle Tensors. PyTorch is also a great numerical processing library (meaning, it can efficiently handle complex calculation on a large set of numbers simultaneously).\nimport torch\rdef sigmoid(x): return 1/(1+torch.exp(-x))\rtorch.exp(x) is the same as $e^x$.\nMinor Technical Detail: It does not take any values for input (x). PyTorch expects all inputs to be Tensors, so x needs to be a Tensor\nNow let\u0026rsquo;s test it on some values.\nsigmoid(torch.tensor((100.))) #sigmoid(100) is so close to 1, that the computer rounds it off to 1 itself. #But theoretically, yes, it would be very close to 1, but not exactly 1. The round-off does not matter practically!\rsigmoid(torch.tensor((-100.)))\rsigmoid(torch.tensor(0.))\rMoving ahead, as you would know, Computers can only understand numbers. (Even in the last session, you would remember, we had to convert dates into numbers, because computers do not understand dates).\nSo it doesn\u0026rsquo;t understand that our classes are \u0026lsquo;dogs\u0026rsquo; or \u0026lsquo;cats\u0026rsquo;. To make the computer understand which one is which - we assign a label to each of these classes. By convention, let us name them 0 (for dogs) and 1 (for cats). (You can interchange the labels - there\u0026rsquo;s nothing wrong in that).\nSo now we come to the core idea of Logistic Regression - The sigmoid function will always produce an output between the values 0 and 1. If the output is closer to 1 than it is to 0 (ie, more than 0.5), we will say, that the model predicts the output to be 1, and if it is closer to 0 than it is to 1 (i.e., less than 0.5), then we would say that the model predicts the output to be 1. Does it make sense logically to you?\nDon\u0026rsquo;t worry if the underlying idea is not yet clear. We would explore all the details in the following sections.\nSo this is how Logistic Regression is used for Classification.\nNow coming to the question - Then why is it called Regression anyways?\nThe answer would be clearer as we look into the details. What you should know now, is that, we would ultimately create a Regression Model - a model that predicts a value, which lies between 0 and 1. And then we would create a condition, that would classify the prediction as 0 or 1, based on whether the value of the prediction is less or more than the threshold of 0.5. So we\u0026rsquo;re trying to do classification through a regression model. Fascinating!\nSo how do we model a Logistic Regression Model? By this, what we actually mean is - given some data points, which have multiple features (x1,x2 and so on) and a target value (label , or categories , or classes), how do we form a logistic regression model?\nLet us take a look at the logistic function again, because we know have an idea that in the end, our output is going to be the result of the logistic function.\n$$sigmoid(a) = \\frac{1}{1+e^{-a}}$$\nWe only have one parameter in this equation - a, where a is a real number. But our model has multiple features (x1, x2, x3 and so on). How do we model a logistic function in terms of all these multiple features, if we have only one variable as the input?\nWe already learnt of a technique to combine multiple variables into one single value - the Linear Regression Model.\ny = w1*x1 + w2*x2 + w3*x3 + ... + b\ry is the variable that contains the characteristics of all input features.\nSo, now you can guess how the logistic regression model will be formulated.\n$$model(x_1,x_2,x_3\u0026hellip;.) = sigmoid(w_1.x_1 + w_2.x_2 + w_3.x_3 + \u0026hellip;.. + b) $$\nor in other words,\n$$model(x_1,x_2,x_3\u0026hellip;.) = \\frac{1}{1+e^{-(w_1x_1 + w_2x_2 + w_3x_3 + \u0026hellip; + b)}} $$\nThats it! That is the logistic regression model.\nOkay, so how does it work? What are the parameters of the model? Remember, that parameters are the variables that we change in order to make our model work. We do not have control over the exponential $e$, nor do we have control over the data (x1,x2\u0026hellip;). We only have control over w1,w2\u0026hellip;.,b. These are our parameters.\nWe would like to adjust our parameters in a way, that for each data point (having features x1,x2\u0026hellip;), the linear function w1*x1 + w2*x2 + .... + b predicts a value greater than 0 if the target class is a 1 (hence giving the output of the logistic function greater than 0.5, which means, we would say that the model predicts the model to be a 1).\nAnd if the actual target class is 0, we would want the linear function to return a value less than 0, so that the sigmoid (logistic) function returns a value less than 0.5, and thus we would say, that the model predicts the class to be a 0.\nPhew! That\u0026rsquo;s a lot to take in at once. Go back and read this again, and make sure you understand the logic. If needed, go back and study the logistic function, the linear regression model, and how they work, and how they all come together to form a classification model.\nWe would adjust the parameters w1,w2\u0026hellip;.,b using an algorithm we already learned about - the Gradient Descent!\n\rCase Study: Identifying Handwritten Digits Given an image of a handwritten digit, can we build a classifier that can identify what digit it is? Digits refers to integers from 0 to 9, both inclusive.\nIn the image above, notice, all the different styles in which any digit can be written. The model needs to \u0026ldquo;learn\u0026rdquo; the characteristics of digits. What differentiates a 1 from a 2, or a 5 from a 9? To be fair, it is difficult for even humans to express the idea of distinction in words. But we still Can give a vague answer to this question - each number has its own way of writing - for example, a 1 is not likely to have a curved line, but a 2 is likely to have a curve. Its quite amazing that a model can learn these ideas - and remember, all this is learnt in the form of parameters, which are nothing but numbers in themselves!\nThis dataset is called the MNIST dataset, which contains 28x28 pixel greyscale images for all digits. We however will look at only 2 digits (remember, logistic regression is meant for binary classification). (We will however, look at how to classify among multiple classes too, later on!)\nSo let us donwload this dataset. Last time, we used the Kaggle API to download the dataset. We can use it this time too! But let us look at another alternative! Its good to know of all possible options to carry out a task - good practitioners should know of their options, because each option, more often than not, has it\u0026rsquo;s pros and cons, and every method is not the best choice for a given task. But still, if you wish to use the Kaggle API, here is the link to the dataset. Follow the exact procedure as last session. Below is a commented out peice of code from last session to download the dataset. Remember, you need to upload the kaggle.json file, and then enter the Kaggle API command.\nIn this session, we\u0026rsquo;ll use the API provided by PyTorch. PyTorch provides us tools related to Computer Vision in a separate child library, called torchvision, which also includes some famous public datasets, MNIST being one of them. Here is the documentation for the datasets.\nimport torchvision\rmnist_train_ds = torchvision.datasets.MNIST(root='',download=True)\rmnist_test_ds = torchvision.datasets.MNIST(root='',train = False, download=True)\rlen(mnist_train_ds), len(mnist_test_ds)\rWe have 2 variables - mnist_train_ds and mnist_test_ds. These refer to the Training set and the Test set, which is basically the result of splitting the entire datasets into two subsets. What do they mean?\nSplitting Datasets into training and testing sets Imagine you have 1000 datapoints to train your model on. You successfully train your model. But now, the question is - How do you know that your model actually works well?\nYou would say - the model predicts $p%$ of the images in the dataset correctly, which is a good accuracy.\nBut here\u0026rsquo;s the problem - its very much possible, that the model might work well on data you\u0026rsquo;ve trained it on (The training dataset), but terrible on data that it hasn\u0026rsquo;t. This is called Improper Fitting of the model over the data. It basically means, that your model has not \u0026ldquo;learnt\u0026rdquo; the right things.\nThis means that the parameters are set in a manner, that do not represent the general idea of the category, but only the specific characteristics of the images in the training dataset. Naturally, the model performs worse on images it never trained on, because it would come across new settings, that it hasn\u0026rsquo;t learnt to identify.\nFor example, if your model predicts which image is that of a cat or a dog - you would expect your model to learn the general characterisitics of dogs and cats, and not features such as - a picture of a dog is likely to have a green background (grass), while a cat is not (because dogs like to go out of the house, cats don\u0026rsquo;t!). What if all your training images have dogs with a green background, and all cat images with an indoor background? Your model is likely to learn that too!\nNow, suppose you have a pet dog at home, and click a picture, and feed it to your model. This picture has an indoor setting, so the model is likely to predict it as a cat. That is how improper fitting works.\n How do we make sure that the model fits well through the data? Keep aside a small portion (say, 20%) of the dataset aside. Only train your model on the remaining data. And at the end, see how the model performs on the data that we kept aside (the data that the model has never seen). This would be an indicator of how well the model has learnt the general features of the data. Because if the model works well on data that it never came across, it has to be because the model has learnt the right characterisitics.\nThis dataset, that we keep aside is called the Validation Set, or the Test Dataset. These two essentially mean the same thing - a sub-dataset that is used to validate or test the correctness of the model. The model never trains on this dataset, and is used only to test the accuracy of the model.\nIf the model does not perform well on the validation dataset, we make changes in the model and training mechanism. More specifically, we change the hyperparameters of the model, which are values other than the parameters (w1,w2\u0026hellip;.), which cannot be learned by the model, but need to be manually set by us. Learning Rate in the Gradient Descent Algorithm is one such example. So you may adjust the learning rate until the validation set accuracy is good enough for our application.\n Note:\n  In many courses, validation set and test set are two different concepts. A validation set refers to a set which is specifically used to set hyperparameters. In doing this, there is a chance we have \u0026ldquo;memorized\u0026rdquo; the validation set too, because we manually set the hyperparameters (eg, the learning rate) as a value that works well only for the particular (validation) set. So, we use a test set, which is a set that we never use for either training or adjustment. It is completely unknown to the model. This is a stricter version of the idea we are trying to pursue (to keep aside a dataset that is not to be seen by the model during training).\nHowever, for small scale projects, you need not necessarily separate the validation and the test datasets. They can be combined into one dataset, and can be called either the validation set, or the test set. In these sessions, we will interchangeably use the terms validation set and the test set, unless we explicitly mention otherwise.\n  Improper fitting of the dataset can be of two types - Underfitting and Overfitting. You will learn more of this during the lectures. But if you are curious to know more about these concepts - here is great intuitive explanation of the difference between the two!\n  Why did we not use a validation set in the Linear Regression problem? In many courses, you will find a separate validation set being used for the Linear Regression Problem. This is technically right, but practically, we don\u0026rsquo;t need a validation set for Linear Regression problems. This is because you cannot underfit or overfit linear data. The only extra data you may need in a linear regression model, is to carry out inference (checking the performance of the final model) (testing of the model).\n  Let us look at how this dataset looks like. According to the documentation, you can index these dataset variables to get a tuple with the image, and the label\nmnist_train_ds[0]\rmnist_train_ds[0][0]\rBinary Classification in the MNIST dataset The MNIST dataset contains 10 labels (0 through 9), but if you remember, logisitic regression is meant to do binary classification. So let us take out two labels, say - 3\u0026rsquo;s and 5\u0026rsquo;s and try to differentiate between them.\nthrees_ds = [i for i in mnist_train_ds if i[1]==3]\rfives_ds = [i for i in mnist_train_ds if i[1]==5]\rlen(threes_ds), len(fives_ds)\r##Similarly, let us extract the test dataset also\rthrees_test_ds = [i for i in mnist_test_ds if i[1]==3]\rfives_test_ds = [i for i in mnist_test_ds if i[1]==5]\rlen(threes_test_ds), len(fives_test_ds)\rAnd finally, we convert these to PyTorch tensors, and we\u0026rsquo;ll build our model therefrom. The images in our dataset are objects of an Image Processing Library in Python, called PIL. So, to convert them to tensors, we use the transforms method of the torchvision library.\ndef convert_PIL_to_tensors(images):\rimages=list(images) #to make sure we can index the collection of images properly. Because of this, the input to this function need\r#not necessarily be a list, but can a set, tuple, generator or even a dictionary\rreturn torch.stack(list(map(torchvision.transforms.ToTensor(),images))).float()\rx_threes=convert_PIL_to_tensors([i[0] for i in threes_ds]).view(-1,28*28)\rx_fives =convert_PIL_to_tensors([i[0] for i in fives_ds]).view(-1,28*28)\rx_dataset = torch.cat((x_threes,x_fives))\ry_dataset = torch.stack([torch.tensor(1.)]*len(x_threes) + [torch.tensor(0.)]*len(x_fives))\rx_dataset.shape, y_dataset.shape\rSimilarly for the test dataset\nx_threes_test=convert_PIL_to_tensors([i[0] for i in threes_test_ds]).view(-1,28*28)\rx_fives_test =convert_PIL_to_tensors([i[0] for i in fives_test_ds]).view(-1,28*28)\rx_test_dataset = torch.cat((x_threes_test,x_fives_test))\ry_test_dataset = torch.stack([torch.tensor(1.)]*len(x_threes_test) + [torch.tensor(0.)]*len(x_fives_test))\rx_test_dataset.shape, y_test_dataset.shape\rA few details.\n  convert_PIL_to_tensors takes in a list of images, and returns a tensor containing numerical values (pixel values) of tensors. By the way, the pixel values of the MNIST dataset lie between 0 and 9. 0 is a completely black pixel, and 9 represents a white pixel. This scale is called the grayscale. Black and White Images are basically grayscale images. But torch.transform converts all values between 0 and 1.\n  .view() is a pytorch tensor method used to reshape tensors. So we\u0026rsquo;re essentially converting a tensor of shape (28,28) (The height and width of the image in pixels) to a single flattened tensor of length 28*28 = 784. What about the -1 as the first argument? It means, \u0026ldquo;whatever value is needed to account for all the values\u0026rdquo;. So if there are 100 images of shape (28,28), meaning there are 100x28x28 values in total, the -1 would internally be replaced by 100, in order to be account for all values. You can even add as many \u0026ldquo;dimensions\u0026rdquo; to the tensor as possible, as long as the total number of values in the tensors remains the same.\n  What are dimensions of a tensor? A tensor is a multidimensional array. So think of a tensor with 2 dimensions as an \u0026ldquo;array of arrays\u0026rdquo;, and with 3 dimensions as \u0026ldquo;an array of arrays of arrays\u0026rdquo;, etc. shape is nothing but the values of all dimensions. So if a tensor is of shape (100,784), it means that it is an array containing 100 arrays, each of which contains 784 values.\n  By convention, in Machine Learning, the first dimension represents the number of items, and the following values represent the shape of the data items. For example, a tensor of shape (100, 784) is a collection of 100 items, each having 784 features.\n  A tensor of shape (100, 3, 50, 50) is a collection of 100 items, each of which has the dimensions (3,50,50) (this is usually the case with images, which have 3 channels (RGB), and 50x50 is the height x width of the image). For grayscale images in the MNIST dataset, the tensor would be of the shape ($x$, 1, 28, 28), meaning a collection of $x$ items, each having an image shape of (1,28,28), meaning 1 single channel of shape 28x28 pixels. Don\u0026rsquo;t worry if it doesn\u0026rsquo;t make a lot of sense right now. You would get a hold of it through practice.\n For the sake of convenience, we convert all data tensors of shape (1,28,28), to a single array of 784 items. So now, these 784 items act as the features of the data. So correspondingly, our parameters, would be w1, w2, w3, ....., w784, b.\n  Note: We have assigned a label of 0 to our 3\u0026rsquo;s and 1 to our 5\u0026rsquo;s.\n  Let us see how the dataset is formatted. If you pick any image tensor (a tensor of length 784), and reshape it into the original size 28x28, you would get the original image back. To plot the pixels, we use the matplotlib library.\nimport matplotlib.pyplot as plt\rplt.imshow(x_dataset[0].view(28,28), cmap='gray')\rWe also combine both the x and the y into one single variable, and simply call it the dataset. This dataset can be indexed to get both the x and y together, without having to handle two variable (x and y).\ntrain_dset=list(zip(x_dataset,y_dataset))\rvalid_dset=list(zip(x_test_dataset,y_test_dataset))\rx,y=train_dset[0]\rx.shape,y\rSo now, we need 784 weights and 1 bias to create our model. Let us create a function to initiate parameters.\ndef init_params(size, requires_grad=True): return (torch.randn(size)).requires_grad_()\rweights=init_params((28*28,1))\rbias=init_params(1)\rLet us now create our model\ndef logistic_regression_model(x): return torch.sigmoid(x@weights + bias)\rLessons in Python!\nThe @ operator\nAs we know, Python is THE most famous language for data sciences. Matrix multiplications are such an integral part of data sciences, that in 2014, python introduced an operator that is dedicated only for matrix multiplication. Remember, if you want to matrix multiply two matrices (A and B) of shapes (m x n) and (p x q) respectively, p should be equal to n, and the resultant matrix would be of the shape (m x q). If you are unclear about the concept of matrices and matrix multiplication, here is a great source to help you visualize.\nLets test this model out!\npreds = logistic_regression_model(x_dataset)\rpreds.shape\rThe Loss Function So far we\u0026rsquo;ve looked at the Mean Square Error Loss Function. It represents the idea, that the larger the distance between two values, the larger the loss. This worked well for the regression case, where we wanted the predictions of the model to be as close as the target values. But that is not applicable for the classification case. We cannot define the difference between two classes, like the dog and class. What we need is a right/wrong approach. Is the prediction right? or is it wrong?\nLet us analyse what we would want our model to do.\nWe would want our model to predict the value for 3\u0026rsquo;s to be as close to 1., and predict 7\u0026rsquo;s to be as close to 0. Because as we mentioned, if the prediction is closer to a 1 than to a 0, we would classify the result to be a 1. And if the prediction is closer to a 0 than to a 1, we would classify the result to be a 0.If you cannot wrap your hand around this concept, please go back a step and read the concepts again. Because if you understand the concepts well, you can implement the concepts easily too.\ndef binary_classification_loss(preds,targets):\rassert len(preds)==len(targets)\rreturn torch.where(targets==1,1-preds,preds).mean()\rtorch.where(targets==1,1-preds,preds) is the same as [1-preds if i==1 else preds for i in targets ]. However, it is a much more efficient method to handle tensors, both in terms of speed and memory.\nIt basically means, for each data point whose target is 1, the loss is 1-prediction. So if the model prediction is more closer to 0 than to 1 in this case, a higher penalty is assigned. And wherever the target is not 1 (ie , it is 0), the loss is preds. So if the prediction is closer to 1 than 0 in this case, a higher penalty is assigned.\nFinally, we take the mean of all these losses to give the average loss. Let\u0026rsquo;s test if this loss function works!\nbinary_classification_loss(preds,y_dataset)\rNote: In a lot of course, you would have seen the binary loss function written something as:\n$$Loss(pred, target) = mean([-y_i log(pred_i) – (1 – y_i) log(1-pred_i)])$$\nIf you are familiar with this loss function, and are confused with why we have not used this form of expression - the answer is - we have implemented a similar idea. This loss function too penalizes the model based on similar principles. But we haven\u0026rsquo;t used the log in our loss function. Actually, the log arises during the mathematics behind theory of prediction. When you study the concept of Maximum Likelihood, you will learn the exact details. But theory aside, in practice, the loss function we have implemented is correct too! In Practice, Loss function should be thought of as a function that penalizes wrong predictions, and rewards correct predicitions. And it should have some nice mathematical qualites wherever required, such as smoothness and continuity (so that it can be properly differentiated during Gradient Descent, for example).\nOur task is to minimize this loss function. We do this through the Gradient Descent Algorithm. But before that, we will learn of an important variant of Gradient Descent - The Batch Gradient Descent.\nBatch Gradient Descent If you go back to the last lab session (Linear Regression), you would notice, that we passed the entire set of data into the model at once. This worked alright for our case, becaues the number of data points was not very huge. We were dealing with less than 2000 points. But in many cases, the number of datapoints is of the order of tens of thousands to anywhere close to millions of datapoints, and computers cannot handle so many datapoints at once. It leads to a lot of problems - sums tend to approach very large values which computers cannot process. It is also slow and thus, the model rarely works. This problem increases multifold when the number of features increases.\nThe solution? We divide the data into smaller chunks, called Batches. And we only train the model on one batch at a time. The number of items in a batch, is called the batch size. You would usually see batch_sizes as powers of 2 - 2,4,8,16,32,64,128 and so on. The larger each datapoint is in size, the smaller batch size is recommended. Eg - pictures have a lot of features (pixels), and thus, a smaller batch size, like 8 or 16 is recommended.\nIn PyTorch, this is done using the DataLoader, which is nothing but a class, that can divide the data into batches, and has some other useful properties, which you will learn as you use it more and more. PyTorch Dataloaders are found in the module torch.utils.data\nfrom torch.utils.data import DataLoader\rdl=DataLoader(train_dset,batch_size=16)\rvalid_dl=DataLoader(valid_dset,batch_size=32)\rA dataloader acts as an iterator (meaning something over which you can iterate. List too is an iterator. Remember how we do for i in list. We are basically iterating through it).\nSo, when we do for x_batch,y_batch in dl, we will get batches of x and y together. Each batch is a tensor of shape (batch_size, size_of_datapoint). Eg.\nx_batch,y_batch=next(iter(dl))\rx_batch.shape, y_batch.shape\rNext, we define the structure of our Gradient Descent Algorithm. This is the same code we developed for the last session, with only minor changes.\ndef calc_grad(x_batch,y_batch,model): preds=model(x_batch)\rloss=binary_classification_loss(preds,y_batch)\rloss.backward()\rdef train_epoch(model,lr,params):\rfor x_batch,y_batch in dl: calc_grad(x_batch,y_batch,model)\rfor p in params:\rp.data -= lr*p.grad p.grad.zero_()\rdef batch_accuracy(preds,y_batch):\rreturn ((preds\u0026gt;=0.5)==y_batch).float().mean()\rdef validate_epoch(model):\raccs=[batch_accuracy(model(x_batch),y_batch) for x_batch,y_batch in valid_dl]\rreturn torch.stack(accs).mean().item()\r#reinitializing the parameters. Do this everytime you want to rerun the model\rweights=init_params((28*28,1))\rbias=init_params(1)\rparams=weights,bias\rfor _ in range(50): #run this model for 50 iterations\rtrain_epoch(logistic_regression_model,0.2,params)\rprint(validate_epoch(logistic_regression_model),end=' ')\rThe model is about 88% accurate. That means, about 88% of the times, the model is able to tell which image is a three and which is a five. That is incredible! Note: the number of iterations in Gradient Descent are also called as Epochs. So for the above case, we train this model for 50 epochs.\nTask: (Non-Evaluative) - Try adjusting the learning rate and epochs by hit and trial and try to get as much accuracy as possible. Atleast try getting an accuracy of more than 90%.\nSo there you have it! You have trained your first classification model! Congratulations!\nReview Below we\u0026rsquo;ve given some review questions for you to try out. But before that, there are a few tips and review points, that you should go through, think over, and make sure you understand each of them. These points are the most important points of this entire session - and not surprisingly, these are the concepts and ideas that will actually help you be a good Machine Learning practitioner.\nSome Tips  Stay up-to-date with the tools of Machine Learning: As we\u0026rsquo;ve mentioned earlier, Machine Learning is an extremely fast moving domain. Every few weeks, new ideas and tools emerge, and within the next few weeks, these will become obsolete too. There are very few concepts and tools that last very long. That shouldn\u0026rsquo;t discourage you. But encourage you to learn the skill of unlearning and learning fast. For example, a lot of you may have heard of the library NumPy, which is a numerical processing library. A * lot * of practitioners use this library to carry out the calculations (that we have done in this session, such as calculation of the sigmoid function, or matrix multiplications, etc). But notice how we carried out all numerical processing using PyTorch itself. Its not true that we could not have done the processing using NumPy, but it just wasnt the right choice. PyTorch offers the majority of processing tools offered by numpy, with the only difference - it is optimized for Tensors. It clearly was better suitable in this case. However, in another problem, numpy would be a better choice. Its also not true that PyTorch\u0026rsquo;s numerical processing is superior than NumPy, its simply a case of suitability to the environment. We will use NumPy at some point too in these labs.  This intuition comes through practice. The important takeaway is, be open to new ideas and learn to adapt fast.\n  Proper Structuring of Code: If you notice the code that we wrote above, you would notice, that we define a lot of function on the way. We didn\u0026rsquo;t have to, we could have simply written the functionality directly, and calculated results. But it is not a good practice to do so. Code Refactoring(Structuring) is important to allow flexibility. Many times, your code simply wont work with some minor changes, if you dont structure your code properly. You also have to write the same piece of code again and again.\nWith proper structuring, you can make sure, you can tweak the functionality of the code with very minimal effort. This is especially important for highly complex codes, which have layers upon layers of code. If the code isn\u0026rsquo;t properly structured, finding even simple bugs will turn into a nightmare.\n  As you develop codes, it is important to test whether the peice of code is working correctly. You would have noticed - at each step, we find out the shape of output tensors, and what all does a function return.\n  Review Questions These are non-evaluative, but highly recommended to go through. Make sure you clearly know the answer to each of these concepts. The answers to all these questions are somewhere in this notebook, so if you find yourself unclear with a concept, go back up and find the answer!\n  We imported many important libraries in this notebook. Whenever a library was imported, the reason and the goal was mentioned. Can you enlist all the libraries used in this notebook, and why they were used?\n  What is a dataloader? What purpose does it serve? What is a batch of data?\n  We have structured the data at many levels - from lists of our x's and y's, all the way to a dataloader. Can you draw a tree explaining the heirarchy of the data? At the top would be the dataloader, and the bottom would be the lists of inputs and targets.\n  What is the sigmoid function? Can you relate the sigmoid function to the concept of probability?\n  Write the formulas in Pseudo Math and in pseudo Code, of both the Mean Square Error Loss, and the logistic Regression Loss.\n  What is the difference between hyperparameters and parameters?\n  Explain the difference between the training set, the validation set and the test set.\n  Exercise (Evaluative): Who survived the Titanic? This problem is one of the most famous problems on Kaggle. It has (real) information of all passengers on the Titanic, such as the age, sex, ticket class, fare price (which may be indicators of their social status), etc. Can you build a logistic regression model to identify who survived the Titanic, and who did not?\nThe dataset is available from Kaggle itself, so we need to download the data using the API. Run the code below to upload the kaggle.json file, and download the data.\nConsider the following features: (See the details on the Kaggle page)\n Sex (in terms of 0\u0026rsquo;s and 1\u0026rsquo;s) Age SibSp Parch Fare  Target Class: Survived\nYou need to Normalize the continuous variables (Fare and Age). See the bottom notes to see what Normalization is!\nYou also need to use the pandas library to read csv files. Then you need to convert the data into tensors of suitable dimensions. Many values are not available in the dataframe, so for the sake of simplicity, we simply replace all inavailable values with 0\u0026rsquo;s.\nFinally, we need to manually split the training set into a training dataset and the validation subset, because the test set provided does not have labels. So there is no way to evaluate the performance of the model through this dataset. We will do a 80-20 split. We\u0026rsquo;ve written a basic structure for you. Continue from that point, building your model step by step.\n%cd from google.colab import files\ruploaded = files.upload()\rfor fn in uploaded.keys():\rprint('User uploaded file \u0026quot;{name}\u0026quot; with length {length} bytes'.format(\rname=fn, length=len(uploaded[fn])))\r!mkdir -p ~/.kaggle/ \u0026amp;\u0026amp; mv kaggle.json ~/.kaggle/ \u0026amp;\u0026amp; chmod 600 ~/.kaggle/kaggle.json !kaggle competitions download -c titanic\r!mkdir titanic\r!mv train.csv titanic\r%cd titanic\r!ls\rimport pandas as pd\rtrain_df =pd.read_csv('train.csv')\rtest_df = train_df.iloc[int(len(train_df)*0.8):]\rtrain_df = train_df.iloc[:int(len(train_df)*0.8)]\rprint(len(train_df),len(test_df))\rtrain_df.head()\rFirst we use the define the training data.\nSteps you need to follow:\n convert the \u0026lsquo;Sex\u0026rsquo; attribute as a binary (0/1) feature. Clean the data. Replace any non-defined values with a 0, using df=df.fillna(0) normalize the required features. This can be done by x=(x-x.mean)/x.std() Extract the required features from the dataframe to the tensor. The tensors would be of the shape (x,5) (because of 5 features). You are expected to write this functionality on your own. If however you feel totally stuck, and cannot come up with anything, we provide you with a function structure. It is not totally intuitive, so you still would need to figure out how to work with this function structure.  def get_xtensors_from_dataframe(df,features:list,target: str,normalize:list=[]): x=None\rfor feature in features: feature_list = list(df[feature])\rif feature in normalize: feature_list = normalize_feature(feature_list)\rif x is None: x= [torch.tensor(feature_list)]\rcontinue\rx.append(torch.tensor(feature_list))\rreturn torch.stack(x).permute(1,0)\rOnce you have your tensors, build your dataset, then train your model using gradient descent.  #continue your code from here\r\r\r\rOptional: Normalization Many times, we come accross, data that are of different scales. For example, if you have two features - age and annual salary, age is of the order of $O(10^2)$, while annual salary may be of the order $O(10^5)$. This creates a problem when we build the model, especially during training. The gradients of each parameter is affected by the values of all features, and so it is desirable that all these features have equal and reasonable contribution.\nWe\u0026rsquo;ve already seen an example of normalization in the last session. We divided some values by a number, to make all features comparable. Normalization especially needs to be done for values of very large magnitude, because they often lead to gradient explosion (the gradient tends to become infinite (Nan)).\nA standard method to normalize values of a feature is to normalize them to a mean of 0 and standard deviation of 1. The way this is carried out is:\n$$x \\sim N(0,1) = \\frac{x-mean(x)}{std(x)}$$\nIn general you can adjust the mean and standard deviation of the resultant feature.\n$$x \\sim N(a,b) = b (\\frac{x-mean(x)}{std(x)} + a)$$\nThis not only helps training better, but leads to a stable model as well.\n"});index.add({'id':3,'href':'/docs/03-singular-value-decomposition-and-principal-component-analysis/','title':"03 Singular Value Decomposition and Principal Component Analysis",'section':"Docs",'content':"SVD and PCA Welcome to Session 3 of the practical Machine Learning sessions. In this session, we\u0026rsquo;ll understand and implement SVD and PCA, which stand for Singlular Value Decomposition and Principal Component Analysis. These two ideas are very similar, and have the same end goal.\nIn this session, we\u0026rsquo;ll only introduce ourselves to these concepts. The reason we won\u0026rsquo;t go deep into these topics in this session itself, is because these topics have no practical application independently. They are used as a part of bigger models. So in this session we\u0026rsquo;ll understand the concepts and intuition behind SVD and PCA, and later use it as a part of other Machine Learning projects. But yet, these topics deserve an entire session dedicated to themselves, because even though we cannot build a model using these topics, the idea behind them is quite important in Machine Learning, and its not totally self-intuitive. So the ideas do need a little bit of explanation. Also, we will still build interesting models using these concepts.\nAs you know, these sessions are more practically-oriented than theoretically-oriented. So, we will not dive into most of the mathematics and theory behind the concepts, and only touch the information that is needed to build the code for our model. Our focus will be on understanding the functionality. You can find the exact innards of these concepts through a lot of online resources. However, this is a concept that does require a little bit of mathematics to intuitively understand the concept, and so if you don\u0026rsquo;t get it at once, don\u0026rsquo;t be discouraged! Go through it slowly and you will surely understand the concepts.\nSo, Let us first understand what is the motivation behind these concepts, and then understand what SVD and PCA are.\nThe Curse of Dimensionality Data, especially in recent times, often are of very high dimensions, or features. For example, satellite images sometimes contains thousands of bands of frequency spectrums, called channels, each of which is a separate feature. In Natural Language Processing (that is, the branch of AI that deals with Language understanding, for example, voice recognition (Siri, Google Assistant, Alexa), language to language translation, Sentiment Analysis (identifying hate speech, violence, etc)), words are represented as individual features. So if you wanted to model the entire English Language, you would be dealing with 1 million+ features.\nInfact, think of the amount of data that your phone generates. Modern phones have a lot of sensors, like Accelerometer, Gyroscope, Magnetometer, light sensor, biometric sensor (fingerprint scanner), GPS sensor, etc., all of which generate data continuously and independently, which is eventually transmitted to the company, which it uses to provide various services. Similarly, Aeroplanes generate hundreds of biliions of gigabytes of data every year, in the form of multiple sensors and manually fed data. The list goes on and on.\nNow, ideally, the goal of any model is to understand the data completely, or in other words, extract all the information that the data can provide to us, and the utopian solution to this is to build a model over all the data available, including all features.\nBut, practically, its not an efficient method to use all the features of the data.\n It increases the time and space complexity of the problem. Hence, more resources are required. And this becomes a problem when we deal with huge amounts of data, (billions of data points, for example). (By the way, we call this Big Data in modern lingo). And even the most sophisticated computing system either are not capable of, or are pushed to the limits trying to handle such data. Most of the times, we do not even need all the features of the data. Many features are either useless, redundant, or provide very little information about the objective - so little, that even if we ignore the feature completely, there would not be a significant difference in the results.  For example, the health tracking app on your phone determines how many steps you walked in a day. The data from the biometric sensor is of no use for this application. However, an application that tracks your daily screen time would need data from the biometric scanner, the light sensor, etc, but on the other hand, would not require data from the gyroscope. Maybe the gyroscope is somewhat related to your usage of the phone, and may give some additional information about your screen time, but the light sensor and biometric sensor may give us enough information to accurately determine your screen time, and hence, we may not need the gyroscope information at all!\nSo, if you pick any domain, you will, more or less, have access to a lot of types of data. But not all of the data is useful or feasible to use, and so, as Data Science practitioners, one of the most important tasks in any project is to understand what data is useful for your purpose, and which is not. This problem of having way too much information, is called the Curse of Dimensionality.\nThis leads us to the idea of ranking features by their importance. We would like to retain more important features and discard the less important features. Cognitively speaking, we can identify which features are important to our particular problem. For example, if we were to build the fitness tracking app of your phone, which can track how many steps you take, you would know that the data from the light sensor is more or less useless to build this feature. This is possible because we have an understanding that features which are related to the number of steps taken are more relative to this application, and others are not.\nBut Machines do not understand these concepts, atleast not in today\u0026rsquo;s world. They treat all features with equal importance. So how do we build a system that can rank features based on their importance?\nVariance in Data: An intuition The idea is to rank features based on the amount of information each feature carries. In mathematics, the idea of the amount of information is represented by the variance of the value of a feature for all data points, or in other words, how spread out the values of a feature are.\n If you\u0026rsquo;re not familiar with the concept of variance, here is a great source to give you an intuition. This video is based on the idea of standard deviation, which is very much similar to variance. In mathematics, variance is defined as the square of the standard deviation. So the idea is similar. A larger standard deviation also means a larger variance. However, note, that we are not concerned with the mathematical formula of variance. Here, by reference, we only are refering to the idea of \u0026ldquo;spread\u0026rdquo;.\n  Note: Variance is a concept that is applied to a distribution of data, or in other words, a collection of data. The data that we deal with is also a distribution (collection of items). Hence for each feature, a separate distribution can be generated. The values of a particular feature of all data points will constitute the distribution. So each feature will also have its own variance.\n The more the Variance, more is the information constituted in the feature. The features with less variance do not provide us with a lot of information, and are thus, less relevant to us.\nTo understand this concept more intuitively, have a look at this meme below.\nThis is obviously meant as a joke, but it serves us as a great example. If you really wanted to differentiate between two species of animals, horses and crocodiles for example, you would collect data about these animals. The features could be - number of eyes, number of ears, weight, etc. As ridiculous as it may seem, even \u0026ldquo;whether it is culpable for the death of Princess Diana\u0026rdquo; is a feature.\nThese features essentially have no variance among them. All horses have 2 eyes, and so do all crocodiles (atleast physically). No model in this world can differentiate between horses and crocodiles on the basis on number of eyes. This occurs due to the lack of variance in this feature. If you were to plot the number or eyes of 100 horses and 100 crocodiles, the distribution would not be spread out at all!\nNow consider the length of the ears of these two animals. The ears of a horse are of about 5 cm. Crocodiles on the other hand, do not have an external ear. So now you can easily differentiate between a horse and a crocodile now. This is because there exists variation in the data. And because of this, you can create two very distinct groups of animals.\nSo now you see, among all the features, only one feature is sufficient to differentiate in this case. This is why extracting the more important features is important.\nThe job of SVD and PCA algorithms is to find which features have the most variance, and hence carry the most information. Not only is it useful to identify the more important features, but also help us visualize data better. As you know, we can\u0026rsquo;t imagine anything beyond 3 dimensions. So using SVD/PCA, we can represent almost all the information in just 3 dimensions, that would otherwise require, say, 100 dimensions.\nLets learn about some details. We\u0026rsquo;ll talk about SVD first, and then PCA. These 2 are anyways very similar concepts.\nHow SVD ranks features SVD refactorizes the input tensor (matrix) into a product of three matrices. Let us consider a matrix A of shape $m \\times n$. SVD breaks it down into three matrices $U$,$S$ and $V$, such that\n$$A_{m \\times n} = U_{m \\times q} S_{q \\times q} V^T_{q\\times n}$$\nwhere S is a eye Matrix (a matrix whose only diagonal values are non zero, and remaining are zeros), and $V^T$ refers to the inverse of matrix $V$. $q$ takes the value of $min(m,n)$.\nLet us test this out. PyTorch provides us methods to calculate SVD and PCA. We\u0026rsquo;ll use it for now, and later introduce you to other libraries that can carry out these operations. However, we prefer PyTorch because, as we mentioned, we\u0026rsquo;ll be using SVD and PCA as part of a bigger project, like a Linear Regression Model, or a Logistic Regression Model, or a Neural Network, and in that case, PyTorch will help us avoid gradient calculations, and also provide us with tensors, that can handle data very well.\nWe also import matplotlib at this point, which will help us visualize our tensors.\nimport torch import matplotlib.pyplot as plt Let us generate a random tensor, of a random shape, lets say 10 x 6 (mxn)\nA= torch.rand((10,6)) Let us perform SVD on it. If you google \u0026ldquo;pytorch svd\u0026rdquo;, the first or second link will lead you to the documentation of SVD in PyTorch. Here is the documentation. In the example section, the syntax is mentioned. Let us follow the exact syntax.\nu,s,v = torch.svd(A) In PyTorch, all data containers are expected to be tensors. U, S and V too must be tensors. Let us verify that.\ntype(u), type(s), type(v) Let us also determine the shapes of these tensors.\nu.shape ,s.shape, v.shape $q$ is $min(m,n) = min(10,6) = 6$.\nSo u is of shape $m \\times q = 10\\times6$\n$s$ is supposed to a diagonal matrix. Instead, PyTorch directly provides us with the values of the diagonal elements. You can simply convert it to the diagonal matrix by torch.diag(s).\nAnd finally $V^T$ is of shape $q\\times n =6\\times 6$ meaning, $V$ is of shape $6 \\times 6$ too.\nLet us verify if the relationship written above holds.\nu@torch.diag(s)@v.T #this is a tensor print(A) You can compare by a quick visual look. The two tensors are exactly identical. Using a randomly generated tensor may not be as intuitive. Let us try this with a real world example. Let us upload an image to this notebook. Let us work with a specific image.\nThis is a 1920x1080 pixel image. Pretty large! Let\u0026rsquo;s download this image to our notebook. To save you time, we\u0026rsquo;re using a publically shared google drive image, which can be downloaded into the local environment in Google Colab by using the !gdown statement!\nWe also use the PIL library, which is a standard Python Image processing Library (ie, to read a jpg/png file). Also, in order to convert jpg images to PyTorch tensors, we\u0026rsquo;ll need the PyTorch child library torchvision.\nfrom PIL import Image import torchvision #let us also define functions that convert image to tensor and viceversa image2tensor = torchvision.transforms.ToTensor() tensor2image = torchvision.transforms.ToPILImage() !gdown --id 1cEmeBLncAbrbDparKHZ7hU_AXaRvJ_ql \u0026gt;/.tmp !ls img=Image.open('03 landscape.jpg') img tensor_landscape = image2tensor(img).float() tensor_landscape.shape Notice that this image has 3 channels (Red Green and Blue), each of which is a tensor of shape (1080 x 1920). In total, there are 3x1080x1920 pixels = 6,220,800 pixels (6.2 million pixels). Thats a lot of pixels. We often undervalue modern technology. Just a few decades ago, it would not have been possible to fit this image in a single computer, that occupied an entire room (source).\n Note: In Many other libraries, image tensors are usually of the shape (m,n,3) rather than (3,m,n). This is one speciality about PyTorch. This is because PyTorch does all operations from the right. For example, PCA/SVD is done on a 2D matrix. So PyTorch would only operate SVD/PCA on the right most 2 dimensions, and leave the rest dimensions untouched. As you will see, this is not only efficient (doesn\u0026rsquo;t require extra lines of code to exclude certain dimensions), but also is desirable for visualization as well as ease of operation of large amounts of data.\n So Now let us apply SVD to this image. same method as above!\nu,s,v = torch.svd(tensor_landscape) u.shape, s.shape, v.shape Notice the extra dimension of 3 in these tensors. This is because of the 3 channels in the image. PyTorch has applied SVD on all 3 2-D matrices independently, and kept all dimensions separate.\nLet us verify that we can reobtain the image from these vectors. We have defined a function that can take these decomposed tensors (u,s,v) and return a tensor which represents the original tensors. You can ignore all the new PyTorch functions used in this function. We do this because the @ operator does not know how to handle batches. So we have to use inbuilt pytorch functions.\ndef decomposed2tensor(u,s,v): return torch.matmul(torch.matmul(u,torch.diag_embed(s)),torch.transpose(v,-1,-2)) tensor = decomposed2tensor(u,s,v) tensor2image(tensor).resize((800,400)) We have recovered the same image!\nSo now lets choose a subset of features. As you would remember from the MNIST example from the Logistic Regression session, each pixel in this image is a separate feature. Here too, each pixel is a separate feature. But there are way too many features here - 6,220,800 features.\nHow do we choose the most important features?\nThe $S$ tensor is the key to our answer. $S$ gives the variance of each new feature in descending order. Hence, it also represents features in the descending order of the amount of information the features carry.\nLet us see this for ourselves!\nplt.plot(s[0]) plt.xlabel('feature number') plt.ylabel('amount of information') As you can see, $S$ is a constantly decreasing tensor. The first value corresponds to the feature with the highest variance. You can also notice - the first 50 or so features carry most of the information. Then why would we need the remaning 1030 features?\n Note: This method of determination of number of useful features is called the elbow selection method. Wherever there is a steep bend in the amount of information is a good selection of number of crucial features. However, this is not a hard and fast rule, and trial and error is the only absolute method. It depends on how much information you\u0026rsquo;re willing to throw away.\n By the way, internally, $USV^T$ is not exactly same as $A$, even though the final values are the same. Actually, U, S and V jumble up all the features of A in such a manner, that the feature with the most variance (can be a linear combination on many original features) emerges at the first position, and the one with the least variance comes at the end.\nLet us now choose the first 50 most important features from this photo.\nRemember, A is decomposed as\n$$A_{m \\times n} = U_{m \\times q} S_{q \\times q} V^T_{q\\times n}$$\nOut of q features, if you were to choose 50 features you would choose the first 50 elements of $S$. Correspondingly you would choose the first 50 elements of the columns of $U$ and first 50 rows of $V^T$, in order to make the matrix operations compatible. Here is a great visualization of the concepts.\nOr mathematically,\n$$A_{m \\times n} = U_{m \\times 50} S_{50 \\times 50} V^T_{50\\times n}$$\nThe most beautiful thing about this equation is that, the output of the product of these vector is still the original size (ie, 1080x1920 pixels)\nu1,s1,v1 = u[:,:,:50] , s[:,:50] , v[:,:,:50] u1.shape, s1.shape, v1.shape Let us Reconstruct the original image from these vectors.\ntensor = decomposed2tensor(u1,s1,v1) img=tensor2image(tensor) img.resize((800,400)) The image quality has slightly gone down, But, the image is still understandable. All the while, having reduced its size by a lot.\nThe original Image was 3x1920x1080 pixel values = 6,220,800 pixels This image, made from decomposed matrices, have a size of only 450150 values (3x1080x50 + 3x50 + 3x1920x50), which is over 13 times smaller than the size of the file. If the original file was 5MB, this file would be less than 400 KiloBytes.\nThis is how WhatsApp transfers images. It does not transfer the image in its original quality, but compresses the image, using a similar algorithm. The quality is not compromised much, but the size reduces dramatically.\nHow much variance (information) has been used to generate this image?\nprint((torch.sum(s[:,:50])/torch.sum(s)), '% of the information has been used') Meaning, almost the first 50 features contain 50% more information than all remaining 1030 features combined.\nObviously, SVD throws away certain information. So we can\u0026rsquo;t expect the quality of the image to be the same as the original, and at the same time, the memory efficiency of the data to reduce. You always need to do a tradeoff. But SVD is a great tool when memory is an issue, and at the same time, some tradeoff in the quality of data is acceptable. For example, we can still accept a Machine Learning model to identify that this is a picture of a lake and mountains, even though we\u0026rsquo;ve reduced the size by over 13 times.\n For the sake of simplicity, let us redefine a function, so that we dont have to manually slice tensors according to the number of features to be selected. To this you can pass the decomposed tensors, as well as (optionally) how many features to choose. If nothing is passed to k,all the features are chosen.\nYou can also pass in any types of data, not necessarily a single image, but a batch of mutiple images (both multichanneled and single channeled (such as Black and White images).\ndef decomposed2tensor(u,s,v,k=None): if k is None: k=s.shape[-1] + 1 u1,s1,v1 = u[...,:k] , s[...,:k] , v[...,:k] return torch.matmul(torch.matmul(u1,torch.diag_embed(s1)),torch.transpose(v1,-1,-2)) #does it work? decomposed2tensor(u,s,v,k=10).shape , decomposed2tensor(u,s,v).shape PCA PCA is an optimized version of SVD.\n It normalizes the data before decomposing it It only calculates certain number of features, and not all possible features, thus speeding up the process of calculations. In PyTorch, this value is $min(6,m,n)$, though it can be adjusted by us.  Here is the documentation of the function.\nIf you remove the normalization, and set the number of features to be calculated the same as SVD, then this algorithm will basically work like SVD. Hence, you see, PCA and SVD are not very different algorithms.\nFor this example, we will not normalize the data, because normalization usually does not work well with images. Just to demonstrate, we first show you the result of PCA with normalization.\nHowever, for other types of data, normalization gives better, more stable results.\nu,s,v = torch.pca_lowrank(tensor_landscape) # with normalization u.shape, s.shape, v.shape tensor=decomposed2tensor(u,s,v) tensor2image(tensor).resize((500,500)) Clearly, normalization creates a lot of noise in Images. To avoid normalization, set the center parameter as False. We will also choose 50 features, just as before, using the q parameter. (Read documentation to understand syntax)\nu,s,v = torch.pca_lowrank(tensor_landscape,center=False,q=50) tensor=decomposed2tensor(u,s,v) tensor2image(tensor).resize((800,400)) This is absolutely the same as the result we received from SVD.\nCan we Apply SVD/PCA through other libraries too? (Optional) Yes we can! Why just stick to PyTorch? In Many cases, other libraries may be more appropriate to use!\n1. Using NumPy NumPy provides us with a method to calculate the decomposed matrices (U,S,V).\nimport numpy as np Let us begin by creating a random matrix, just like we did in the beginning of this notebook.\narray = np.random.randn(10, 6) Now let us calucalte the decomposed matrices. Note: that np.linalg.svd directly returns $V^T$, and not $V$.\nu,s,vT = np.linalg.svd(array,full_matrices=False) u.shape, s.shape, vT.shape Can we reconstruct array from these matrices?\npred = u @ np.diag(s) @ vT pred array You can compare that these two matrices are the same. Again you can simply slice the decomposed matrices to obtain the more important features.\n2. PCA Using sklearn from sklearn.decomposition import PCA pca = PCA(n_components=3) sklearn too expects numpy arrays as input.\npca.fit(array); And then you can access the values of each of these matrices separately.\npca.singular_values_ # S values  Review There you have it! You have implemented the PCA and the SVD algorithm. Congratulations! Below we\u0026rsquo;ve given some review questions for you to try out. But before that, there are a few tips and review points, that you should go through, think over, and make sure you understand each of them. These points are the most important points of this entire session - and not surprisingly, these are the concepts and ideas that will actually help you be a good Machine Learning practitioner.\n Its natural you would be feeling slightly overwhelmed by all the mathematics and concepts we learned above. But in reality, there are entire books written just on the mathematics of these algorithms. We\u0026rsquo;ve barely scratched the surface, and to be honest, thats all that we need to know to get what we need.  Don\u0026rsquo;t try to memorize the formulae, the concepts are enough to know! Regarding the code, you can always come back, and refer to the structure of the code, and even copy paste it to your own application. Programming is all about how to get the most out of the resources available. You should know the art of how to write, rather than what to write!\n As Data Science Practitioners, we cannot completely avoid the math and the stats. However, in practical Machine Learning, a lot can be avoided (unless you\u0026rsquo;re pursuing research, in which case, math is essential!). That being said, don\u0026rsquo;t be afraid of the math. Patience is key!  Review Questions: These are non-evaluative, but highly recommended to go through. Make sure you clearly know the answer to each of these concepts. The answers to all these questions are somewhere in this notebook, so if you find yourself unclear with a concept, go back up and find the answer!\n Mathematically, how is the concept of information represented in data? What does the tensor $S$ represent in SVD (or PCA)? Why do we need to rank the importance of data. Can you give an example from outide the examples above? When is dimensionality reduction suitable? How do we determine how many features are the optimal number of features to use?  Exercise In Session 2: Logistic Regression, we built a MNIST Digit classifier. We acheived an accuracy of about 88% between 3\u0026rsquo;s and 5\u0026rsquo;s. Let us now further apply SVD to the dataset and see how it affects the performance of the mdel. To simulate this, we will apply reduce the size of all images in our dataset by selecting a few important features from the images.\nWe provide you with a structure of the code we used before. You need to modify this code to apply PCA to the dataset.\n#necessary imports from torch.utils.data import DataLoader #necessary definitions def convert_PIL_to_tensors(images): images=list(images) #to make sure we can index the collection of images properly. Because of this, the input to this function need #not necessarily be a list, but can a set, tuple, generator or even a dictionary return torch.stack(list(map(torchvision.transforms.ToTensor(),images))).float() def init_params(size, requires_grad=True): return (torch.randn(size)).requires_grad_() def logistic_regression_model(x): return torch.sigmoid(x@weights + bias) def binary_classification_loss(preds,targets): assert len(preds)==len(targets) return torch.where(targets==1,1-preds,preds).mean() def calc_grad(x_batch,y_batch,model): preds=model(x_batch) loss=binary_classification_loss(preds,y_batch) loss.backward() def train_epoch(model,lr,params): for x_batch,y_batch in dl: calc_grad(x_batch,y_batch,model) for p in params: p.data -= lr*p.grad p.grad.zero_() def batch_accuracy(preds,y_batch): return ((preds\u0026gt;=0.5)==y_batch).float().mean() def validate_epoch(model): accs=[batch_accuracy(model(x_batch),y_batch) for x_batch,y_batch in valid_dl] return torch.stack(accs).mean().item() mnist_train_ds = torchvision.datasets.MNIST(root='',download=True) mnist_test_ds = torchvision.datasets.MNIST(root='',train = False, download=True) len(mnist_train_ds), len(mnist_test_ds) threes_ds = [i for i in mnist_train_ds if i[1]==3] fives_ds = [i for i in mnist_train_ds if i[1]==5] threes_test_ds = [i for i in mnist_test_ds if i[1]==3] fives_test_ds = [i for i in mnist_test_ds if i[1]==5] x_threes=convert_PIL_to_tensors([i[0] for i in threes_ds]).view(-1,28*28) x_fives =convert_PIL_to_tensors([i[0] for i in fives_ds]).view(-1,28*28) x_dataset = torch.cat((x_threes,x_fives)) y_dataset = torch.stack([torch.tensor(1.)]*len(x_threes) + [torch.tensor(0.)]*len(x_fives)) x_threes_test=convert_PIL_to_tensors([i[0] for i in threes_test_ds]).view(-1,28*28) x_fives_test =convert_PIL_to_tensors([i[0] for i in fives_test_ds]).view(-1,28*28) x_test_dataset = torch.cat((x_threes_test,x_fives_test)) y_test_dataset = torch.stack([torch.tensor(1.)]*len(x_threes_test) + [torch.tensor(0.)]*len(x_fives_test)) At this point, when we trained our model.\nweights=init_params((28*28,1)) bias=init_params(1) params=weights,bias train_dset=list(zip(x_dataset,y_dataset)) valid_dset=list(zip(x_test_dataset,y_test_dataset)) dl=DataLoader(train_dset,batch_size=16) valid_dl=DataLoader(valid_dset,batch_size=32) for _ in range(100): train_epoch(logistic_regression_model,0.25,params) print(validate_epoch(logistic_regression_model),end=' ') Now, apply svd to the train and test datasets. We apply SVD only to the x datasets , ie, only the x_dataset and the x_test_dataset. Determine the appropriate number of features through the training dataset. Train this dataset.\nVary the number of features and record the change in final testing accuracy. Plot a number-of-features(k) vs accuracy plot for a few k values.\nYou may use some of the functions defined earlier in this notebook.\n "});index.add({'id':4,'href':'/docs/04-bayesian-learning/','title':"04 Bayesian Learning",'section':"Docs",'content':"Bayesian Learning Welcome to the 4th session on Machine Learning practicals. In this session, we\u0026rsquo;ll learn about a classic Machine Learning method - the Bayesian Learning method. The Bayesian approach towards problem solving is completely based on probabilities. This aligns with the idea behind Machine Learning, since in the latter too, every prediction is visualised as a probability.\nFor example, in logistic regression, the final output (the output from the sigmoid function) lies between 0 and 1. We visualize the output as the probability that the prediction is 1, and 1-output as the probability that the prediction is 0. And we create a threshold, meaning any prediction less than 0.5 is not acceptable as a prediction of 1. (Or in other words, you should be atleast 50% sure that the prediction is a 1). Let us take an example - suppose a logistic regression model returns 0.8 as the output for a particular input, the model is 80% sure that the prediction is equal to a 1, and 20% sure that the prediction is equal to a 1. We do qualify this prediction as a 1. However, the aim of training a model is to adjust the parameters in such a way that this probability is maximized for all inputs.\nSo Bayesian Learning is an important stepping stone towards Machine Learning. However, its a very crude form of Learning - or in other words, a very superficial form of learning - it learns the pattern of distribution of data directly, and nothing else. No intrinsic features are learned, all learned features are independent and remain uncorrelated, etc. But still, Many researchers still find Bayesian Learning a useful tool to use in ensemble with modern Machine Learning techniques. This is because despite its straight forward approach, it is the purest form of data representation - it gathers * all * the information that the distribution of the data provides - no more and no less. We will look into these details in the following sections.\nIn this session, we will learn about 2 important Bayesian Learning approaaches - the Naive Bayes Classifier and Markov Model. So let us begin!\nThe Naive Bayes Classifier The Naive Bayes Classifier simply provides us with the information - given a dataset, what is the probability that a particular model gives correct classification on the data.\nYou must have come across the Bayes Equation:\n$$ P(E|D) = \\frac{P(E) P(D|E)}{P(D)}$$\nThe D refers to our data. We would always have some data that we wish to build a model upon. The $E$ refers to the evidence, or simply the prediction of the model. So in simpler terms -\n$$ P(pred|data) = \\frac{P(pred) P(data|pred)}{P(data)}$$\nFor each of different choices of predictions, the Bayes Theorem holds:\n$$ P(pred=1|data) = \\frac{P(pred=1) P(data|pred=1)}{P(data)}$$\n$$ P(pred=0|data) = \\frac{P(pred=0) P(data|pred=0)}{P(data)}$$\n\u0026hellip; and so on.\nBut these math equations don\u0026rsquo;t make a lot of sense. Let us try to get an intuitive understanding of their meanings.\n P(data) simply means - what is the probability that a particular data results in the correct prediction. Obviously, one datapoint may have many features. This can be represented as P(data) = P(feature1 AND feature2 AND .... AND featureN), meaning, what is the probability of getting a correct prediction when value of feature1 is such, the value of feature2 is such, and so on. In Probability, when you want to find the probability of a combination of different occurences, you can simply multiply the individual probabilities.  $$P(data = (f1,f2,f3,\u0026hellip;.fN)) = P(f1)P(f2)\u0026hellip;P(fN)$$\nP(f1) simply means - what is the probability of f1 attaining a particular value. This can be found out from the dataset itself, using a frequentist approach, meaning, out of all the data, how many of feature f1\u0026rsquo;s have one particular value.\n So, $ P(data|pred)$ means that if whenever the prediction has a value, what would be the probability that if that particular data is passed in the model, this output is achieved. We will look at how to calculate this in the following subsection. T\n  $P(pred)$ is simply the probability of occurence of a particular prediction. Basically, the fraction of data which has a particular pred value as the output.\n  $P(pred|data)$ means, given a data point, having a particular set of features, what is the probability of obtaining a prediction, which is what we want to find out. Infact, this is what all Machine Learning wishes to find out. Even in logistic regression, we build a model that can predict an output with high probability, given some inputs.\n  The proof of the Bayes theorem is actually quite simple, and can be found on the internet!\nBut let us see how to build a Naive Bayes Classifier. So first of all we would need some data. Let us try to pick an interesting classification problem.\nLet us pick the Airplane Passenger Satisfaction, which contains data about how satisfied passengers have been by the service of their respective flights. They have provided information about the flights and the passengers themselves, such as the class in which they traveled, their experience of food, in-flight entertainment, etc. This is a binary classification problem, meaning the experience reported is either satisfactory, or dissatisfactory. This dataset can be found on kaggle.com, and so, like always, we would need to upload the kaggle.json file. You must have it downloaded on your system from the last sessions. If not, you can download a fresh Run the cell below to upload the kaggle.json file.\n%cd from google.colab import files uploaded = files.upload() for fn in uploaded.keys(): print('User uploaded file \u0026quot;{name}\u0026quot; with length {length} bytes'.format( name=fn, length=len(uploaded[fn]))) !mkdir -p ~/.kaggle/ \u0026amp;\u0026amp; mv kaggle.json ~/.kaggle/ \u0026amp;\u0026amp; chmod 600 ~/.kaggle/kaggle.json !kaggle datasets download -d teejmahal20/airline-passenger-satisfaction !unzip airline-passenger-satisfaction.zip !mkdir airlines \u0026amp;\u0026amp; mv test.csv airlines \u0026amp;\u0026amp; mv train.csv airlines %cd airlines !ls So if you see, this dataset contains two csv files, namely the training dataset and the testing dataset. Let us import the pandas library to read these files, and visualize the dataset.\nFor the sake of simplicity, we\u0026rsquo;ve removed all continuous variables, and only retained categorical variables (i.e. the variables that only take values among finite number of categories), otherwise the concept of probability of occurence of an event doesn not make sense. Ideally, a continuous variable can take infinite values, so how do you find the chances of occurence of a particular value?!\nThese variables are - \u0026lsquo;Departure Delay in Minutes\u0026rsquo;,\u0026lsquo;Arrival Delay in Minutes\u0026rsquo;,and \u0026lsquo;Flight Distance\u0026rsquo;, \u0026lsquo;Age\u0026rsquo;\nimport pandas as pd train=pd.read_csv('train.csv',index_col=0).drop(['id','Departure Delay in Minutes','Arrival Delay in Minutes','Flight Distance','Age'],axis=1) test=pd.read_csv('test.csv',index_col=0).drop(['id','Departure Delay in Minutes','Arrival Delay in Minutes','Flight Distance','Age'],axis=1) len(train),len(test) train.head() As you can see, each of the features (all columns except the target variable - \u0026lsquo;satisfaction\u0026rsquo;) have either one of finite number of values. For example, \u0026lsquo;Gender\u0026rsquo; only takes one of two values - Male or Female.\nBut, notice, many of these features have strings as their categories. We know, that computers cannot understand strings. So we need to convert them into numbers. We define a function that converts all values into numerical categories, ranging from 0 to num_categories-1 (num_categories categories in total). Lets call this funtion categorify!\ndef categorify(df): df.dictionary={} for col in df: numerical_categories = {k:i for i,k in enumerate(sorted(df[col].unique()))} df[col]=[numerical_categories[i] for i in df[col]] df.dictionary[col]=numerical_categories categorify(train) categorify(test) /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access  So now, if you take a look at your dataframes, you\u0026rsquo;ll see that all categories are now 0,1,2,\u0026hellip;and so on. Even the target (satisfaction) column!\ntrain.head() For convenience, we\u0026rsquo;ve also added a method in the dataframe, called dictionary, which will help us identify which category refers to which original category!\ntrain.dictionary This is a dictionary, a python object which indexes all values in the form of {key: value}. If you type in dictionary[key], you will get the value. So its a great method to store and retreive information. Its just like a real word dictionary. You look up information through a key, which is the word itself. So you want to know the meaning of a word, you don\u0026rsquo;t search for the meaning itself, but the word. And besides the word, we find the definition too!\nSo now that we are set up with our data, let us build the probabilities. We need to find 3 different probabilities - $P(pred)$,$ P(data)$ and $P(data|pred).$ Using these, we will find $P(pred|data)$. If you\u0026rsquo;re unclear with the meanings of these terms, Please go back up and read it through again.\nLet us start with building $P(pred)$. Meaning, what is the probability of getting a particular prediction (0 or 1, which correspond to the satisfaction of the passenger) (Go take a look at the \u0026lsquo;satisfaction\u0026rsquo; key in the dataframe\u0026rsquo;s dictionary, which we printed just above this cell).\nThis probability can be simply found by counting the fraction of the number of times in the whole dataset, that the prediction was 1 or 0.\ndef get_ppred(df,tgt='satisfaction'): f\u0026quot;get P(pred) as a dictionary, where tgt is the target variable and does not count as a feature.\u0026quot; #this is a comment for the user p_pred=df[tgt].value_counts().to_dict() for p in p_pred: p_pred[p]/=len(df) # convert the frequencies into a ratio of occurence return p_pred p_pred = get_ppred(train,'satisfaction') p_pred We see, that p_pred (which stands for $P(pred)$), is a dictionary, which tells us the probabilities of occurences of all possible predictions (that is, 0 or 1).\nNext, let us calculate $P(data)$. This is a tricky one. Our data is made up of multiple features, like Gender, Class, Seat Comfort, etc. As we mentioned above, we simply need to calculate the probability of occurence of all these features separately. In the end, we multiply the probabilities of all features. So for now, let us calculate the individual probabilities of all features (except ofcourse, the target class Satisfaction)\ndef get_pdata(df,tgt='satisfaction'): return {k:get_ppred(df,tgt=k) for k in df.keys().drop(tgt)} p_data=get_pdata(train) p_data This is again a dictionary, with individual features as the keys in the dictionary. The value of each key is again a probability, which contains the probability of occurence of each value that the feature can take. For example, the \u0026lsquo;Customer Type\u0026rsquo; feature takes 2 values (0 and 1) and so we have calculated the probability of occurence of each of these values. How? by simply calculating the fraction of times these two features occur in the whole training dataset\np_data['Customer Type'] In fact, you can ensure that the sum of all these probabilities for any feature is always 1.\nsum(p_data['Customer Type'].values()) Let us run a small snippet of code to ensure the sum of probabilities of all features is equal to 1. In the code snippet below, we ensure that the difference between 1 and the sum of all probabilities is almost negligible (it might not be 0, because the individual probabilities may have lower precision. For example, a 0.33333333333333333 + 0.66666666666666666 is in all practical sense, equal to 1. But according to the computer, not equal to 1, but a 0.99999999999999999 .\nfor key in p_data: assert abs(sum(p_data[key].values())-1)\u0026lt;1e-6, f\u0026quot;probabilities of the key: {key} does not sum up to 1\u0026quot; Now let us define P(data|pred). This is final component of our model. This can be thought of sub-cases of P(data) itself. Basically, it means - what is the probability of the occurence of a data, given pred assumes a certain value?! So, if you look at P(data), it is the collection of the probabilities of occurence of different features for all the datapoints.\nNow if we calculate the occurences of a feature only for a particular target(pred) value, we get P(data|pred)\nObviously, if you add up the P(data|pred) for all possible prediction values, it will result in the corresponding P(data) itself. We define get_pdata_given_pred, which calculates this for us.\ndef get_pdata_given_pred(df,tgt='satisfaction'): return {k:calc_pdata_given_pred(df,tgt,key=k) for k in df[tgt].unique()} def calc_pdata_given_pred(df,tgt,key): ret={f:None for f in df if f!=tgt} l=len(df) df=df[df[tgt]==key] for f in ret: conditional_probs=df[f].value_counts().to_dict() for o in conditional_probs: conditional_probs[o]/=l ret[f]=conditional_probs return ret p_data_given_pred = get_pdata_given_pred(train) p_data_given_pred p_data_given_pred is simply a dictionary with keys as the different possible predictions (0 and 1), each of whose values are a dictionary containing the corresponding P(data) dictionaries (dictionary containing information about the probability of occurence of each individual feature value)\nLet us also verify that the for each feature, the sum of $P(data|pred)$ for each individual prediction is equal to the overall $P(data)$\nfor feature in train.keys().drop('satisfaction'): assert p_data_given_pred[1][feature].get(0,0) + p_data_given_pred[0][feature].get(0,0) == p_data[feature].get(0) Lessons in Python: The get method. We\u0026rsquo;ve seen how we can access a particular item of a list of a dictionary, using square brackets. Internally, this is possible by defining a def __get__ (pronounced dunder get) method in the respective class definition (of the list, dictionary, etc.). In addition to __get__, dictionaries also have a def get method defined, which can take a key, and returns the associated value. Additionally, if a particular key is not found, then you can decide what default value is returned. By default, it is None. So the syntax is as follows:\ndict.get(key,default=None)\nHere, we\u0026rsquo;ve write the code to get the value of the 0 key, and if the 0 key does not exist (which is the case when the associated probability is 0), we choose to return a default value of 0 itself.\nFinally, let us now define a function that can caluculate the final predictions.\ndef get_prediction_accuracy(df,tgt,p_data,p_pred,p_data_given_pred): probability_of_being_1=[] probability_of_being_0=[] features=df.keys().drop(tgt) for _,row in df.iterrows(): probability_of_being_1.append(calculate_prob(row,features,p_data,p_pred,p_data_given_pred,tgt=1)) probability_of_being_0.append(calculate_prob(row,features,p_data,p_pred,p_data_given_pred,tgt=0)) pred=[p_one\u0026gt;=p_zero for p_one,p_zero in zip(probability_of_being_1,probability_of_being_0)] return (pred==df[tgt]).mean() def calculate_prob(row: pd.Series,features,p_data,p_pred,p_data_given_pred,tgt=1): p_pred_tgt = p_pred[tgt] #p_pred for target=1 p_data_1=1. #initializing value p_data_given_pred_1 =1. #initialize for f in features: p_data_1*=p_data[f][row[f]] p_data_given_pred_1*=p_data_given_pred[tgt][f].get(row[f],0) return p_pred_tgt*p_data_given_pred_1/p_data_1 Let us check the prediction accuracy on the training and testing dataset\nget_prediction_accuracy(train,'satisfaction',p_data,p_pred,p_data_given_pred) get_prediction_accuracy(test,'satisfaction',p_data,p_pred,p_data_given_pred) #note that p_data,p_pred and p_data_given_pred are probabilities corresponding to the training dataset, not the testing dataset. #THis is because we build our model using the training dataset only. Testing dataset is not used for finding parameters. So there you have it. You have built a Naive Bayes Classifier from scratch. Obviously, we don\u0026rsquo;t use this classifier independently in real word applications anymore, because there exist much more sophisticated models, but it is an important concept, that somehow made way for more advanced techniques. And even today, you can find a lot of research combining modern AI techniques with traditional probabilistic techniques.\nThis is a good time to understand why this classifier does not perform exceptionally well. Or, to put it in better words, why modern techniques can easily outperform such a probabilistic model.\nModern models can understand relations between different features, which can be helpful to understand which features are more meaningful and important than others. Understanding relations can also help derive much complex information from the data, the lack of which causes the Bayesian model to be very superficial, since every feature is considered independently, and no analysis is done over the feature. The frequency of occurence is the sole factor that drives the predictions.\nMarkov Chains Now let us look at another interesting Bayesian Model, called Markov Model. Its a fairly simple probabilistic approach. Markov Models, also known as Markov Chains, are used to predict continuously from a set of possible predictions, and this completely depends on probabilities. To put the whole idea in one sentence, A Markov Model tries to give the next prediction given a current prediction. So given a prediction, we try to find out what is the probability of the next prediction. So based on these probabilities, the next prediction is made. Predictions with more probability are more likely to be made.\nLet us understand this with an example. Suppose you know that today is a Cloudy Day, what is the probability of a Sunny Day tomorrow, or a windy day, or a Raining day. How is this calculated? By analysing data. Suppose you gather data on the weather conditions of each day. The probability of the occurence of a particular weather condition is simply calculated by a probabilistic model - how many times, of all days in the data that the weather was like today, was the weather of the next day Sunny? or windy?. More occurences of Sunny after a day in the past, on which there was weather condition was similar to today, higher the probability of the occurence of a sunny weather tomorrow too. Hope the idea is clear. This is pretty straighforward, and makes practical sense too.\nThis modeling technique can be used in many scenarios. Obviously it can be used to predict a sequence of occurences, like the weather conditions. But there are some more advanced and interesting applications. For example, You can create an Auto text generator using this. Let us see how this works:\nCase Study: Auto Text Generator The idea is the same, suppose I start with a single word. We will build a Markiv Chain Model, that predicts the next word by analysing data. The data can be a huge corpus of text. Suppose the first word is \u0026ldquo;The\u0026rdquo;. We find out the occurence of different words after \u0026ldquo;The\u0026rdquo; in the data. The word that occurs the highest number of times has a higher chance of being the next word. Lets say, that 60% of the times, the word that follows \u0026ldquo;The\u0026rdquo; is \u0026ldquo;Apple\u0026rdquo;, 20% of the times is \u0026ldquo;Banana\u0026rdquo;, 10% of times \u0026ldquo;Orange\u0026rdquo; and 10% of the times \u0026ldquo;Horse\u0026rdquo;. So out of 100 times the word \u0026ldquo;The\u0026rdquo; appears in our predicted text, the Markov model would predict the next word to be \u0026ldquo;Apple\u0026rdquo; approximately 60 times, and so on.\nLet us pick a random text as our data. We use the most boring text there could be (for real, apparantly!).\ninput=f\u0026quot;\u0026quot;\u0026quot;I am writing something. Yes, I plan to make it the most boring thing ever written. I go to the store. A car is parked. Many cars are parked or moving. Some are blue. Some are tan. They have windows. In the store, there are items for sale. These include such things as soap, detergent, magazines, and lettuce. You can enhance your life with these products. Soap can be used for bathing, be it in a bathtub or in a shower. Apply the soap to your body and rinse. Detergent is used to wash clothes. Place your dirty clothes into a washing machine and add some detergent as directed on the box. Select the appropriate settings on your washing machine and you should be ready to begin. Magazines are stapled reading material made with glossy paper, and they cover a wide variety of topics, ranging from news and politics to business and stock market information. Some magazines are concerned with more recreational topics, like sports card collecting or different kinds of hairstyles. Lettuce is a vegetable. It is usually green and leafy, and is the main ingredient of salads. You may have an appliance at home that can quickly shred lettuce for use in salads. Lettuce is also used as an optional item for hamburgers and deli sandwiches. Some people even eat lettuce by itself. I have not done this. So you can purchase many types of things at stores. If I drive around, I sometimes notice the houses and buildings all around. There are also pieces of farm land that are very large. Houses can be built from different kinds of materials. The most common types are brick, wood, and vinyl or synthetic siding. Houses have lawns that need to be tended. Lawns need to be mowed regularly. Most people use riding lawnmowers to do this. You can also use a push mower. These come in two varieties: gas-powered and manual. You don’t see manual push-mowers very much anymore, but they are a good option if you do not want to pollute the air with smoke from a gas-powered lawnmower. I notice that many families designate the lawnmowing responsibility to a teenager in the household. Many of these teenagers are provided with an allowance for mowing the yard, as well as performing other chores, like taking out the trash, washing the dishes, making their bed, and keeping the house organized. Allowances are small amounts of money given by parents to their children, usually on a weekly basis. These usually range from 5 dollars to 15 dollars, sometimes even 20 dollars. Many parents feel that teenagers can learn financial responsibility with this system. Now I will talk about farm land. Farm land can be identified by some common features. They almost always consist of a very large patch of dirt with small green plants lined up in very long rows. You may sometimes see farm equipment riding over these rows, like tractors or combines. These machines help farmers grow more crops in less time. They are a very helpful invention. Some different types of crops are soybeans, cotton, corn, tomatoes, tobacco, and lettuce (which I mentioned earlier). Most crops are used as food, and can be defined as either fruits or vegetables. Some are commonly eaten raw, after being rinsed in water to remove any dirt. Some are often cooked, which helps give them a more pleasant taste and makes them easier to chew. A very versatile vegetable is the potato. It can be eaten raw, or it can be cooked in a variety of ways. They can be baked, and many people like to add butter to them. They can be mashed, and a lot of times brown gravy or milk gravy is poured on top of them. They can be cut into thin strips and fried. Typically a large amount of grease is required to prepare potatoes in this style, but they are easy to make and easy to eat. You can order them at several fast-food restaurants. Potatoes can also be boiled, stewed, and scalloped. There is a wide variety of options available to you when cooking potatoes. Some other types of crops grown on farm land are used for other purposes. Cotton is used to make clothing (which I also mentioned earlier). It is a very versatile and inexpensive material for clothes. Such items as shirts, pants, socks, and underwear can be made from cotton. The process of converting cotton from a cotton plant to clothing is fairly complicated. Today, cotton is harvested more efficiently through the use of the cotton gin, invented by Eli Whitney many years ago. Tobacco is another type of crop. It is used in making cigarettes. A lot of people smoke cigarettes, even though many medical sources have identified them as harmful to people’s health. Warnings are printed on cigarette packages reminding people of possible dangers resulting from smoking. Cigarettes are available in several brands, including Marlboro, Salem, and Virginia Slims. There is a brand called Kool, but I don’t know whether they are still available at most outlets. Tobacco farming is a large industry, and currently there is debate about it. Recently the government decided on some regulations that cost tobacco companies a large amount of money. If you notice, some farm lands have animals living on them. Most of these are cows, and there are also pigs, sheep, and goats living on farms. Some are raised for the milk they provide. This milk goes through several processes to ensure that it is not contaminated before it is made available to consumers at stores (which I mentioned earlier). Another use for farm animals is meat. Three popular types of meat are beef, pork, and chicken. Beef comes from cows. Pork comes from pigs. Chicken comes from chickens, but you probably knew that. These animals are raised to become plump and healthy, then they are killed, sometimes at slaughter houses. The meat is then removed from their bodies, cleaned, and made available at a variety of stores and restaurants. Sometimes this process can seem gross, but it is part of an advanced ecological food chain on earth. Just like birds eat worms and tigers eat deer, human beings eat cows and pigs. The main difference is that we don’t eat animals raw. We cook the meat to remove blood, fat, and germs from it. We also season our meat with salt or different kinds of sauces. The end result is food that is very tasty and is healthy for us. Farmers do not like trespassers. If a farmer sees one, he will sometimes shoot at them with a shotgun that he owns. Trespassing is against the law. Laws are created by government to prevent people from living in fear. They are meant to provide safety for citizens. Our government in America consists of a legislative branch, an executive branch, and a judicial branch. The legislative branch makes laws based on the concerns of citizens they represent. The executive branch consists of the President. This person enforces the law, and he has certain other duties like declaring war and approving bills prepared by members of the legislative branch. The President is also considered the leader of our country. The judicial branch interprets the laws. This branch consists of the courts and the trials held in them. Here a judge and jury determine from evidence presented by lawyers whether someone is guilty of breaking a law. Initial law enforcement takes place among police officers. They are the first people to encounter situations where a law is being broken. If a criminal (law-breaker) becomes too violent or hostile, they will use guns or mace or nightsticks to administer immediate punishment. Their goal is to bring the criminal under control, so that he can receive a punishment determined by members of the judicial branch of government. Punishments mostly include time in jail, but they can also include fines and, in extreme cases, the death penalty. There is controversy surrounding the death penalty. Children play with toys. This is common to almost all kids. Toys come in a very wide variety. Boys tend to like cars, action figures, and toy weapons. Girls tend to like dolls, toy kitchens, and make-up. Both of them like building or assembling things, be it with Legos, blocks, Play-Doh, or something similar. Toys can be found at most stores, and these days entire stores are dedicated to selling only toys. The most popular of these is Toys ‘R’ Us (with a backwards “R”). Their mascot is Geoffrey the Giraffe. Children love to go to Toys ‘R’ Us and look at the wide variety of toys available. Most children receive the greatest quanitity of toys on their birthdays, or during the holiday season in December. For the majority of children, this holiday is Christmas. For Jewish children, the holiday is Channakuh. Either way, the kid gets presents during this time, and most of these presents are toys. Christmas is a holiday which has gradually become centered around the character “Santa Claus” and his elves and reindeer. Children are told that Santa’s elves build their toys, and Santa delivers them personally to each house in the world by riding in an airborne sleigh hauled by nine reindeer, including Rudolph the red-nosed reindeer, who leads the way. Another popular Christmas character is Frosty the Snowman. Frosty is basically any snowman that comes to life. So during Christmas, many children build snowmen, and some of them hope that theirs might come to life. But all of these characters are myths. The true origin of Christmas is a celebration of the birth of Jesus, who founded the religion of Christianity a couple of thousand years ago. Many popular Christmas carols deal with his story, such as “Joy to the World” and “Silent Night.” Other holidays include Thanksgiving, Halloween, and Independence Day. Thanksgiving has become a tradition of preparing large quantities of food for a large gathering of people, mainly family and friends. This meal usually features turkey or ham as the main course. Turkey and ham are both kinds of meat (which I mentioned earlier). The meal usually also consists of dressing and a wide assortment of vegetables (which I also mentioned earlier). The origin of Thanksgiving is usually traced to the days of the pilgrims, who were the first settlers in America. They made peace with the native people, the Indians, and together enjoyed a large feast, thanking God for providing them with such an abundance. (Their concepts of God were probably very different.) Halloween is the holiday when people dress in costumes to look like other characters. Most of these are children, who go from door to door in different neighborhoods to request candy from the people living there. They usually say “trick or treat” then receive a treat. Very rarely does the person in the house respond with a trick. Halloween has some sort of demonic origin that I am not quite sure about, but the name derives from “All Hallow’s Eve.” I will not say much about Independence Day, but it is the day Americans celebrate the anniversary of our independence from Britain. Most families purchase fireworks during this holiday and set them off in their lawns (which I mentioned earlier). America gained independence from Britain in the late 1700’s after the Revolutionary War. Britain was hoping to extend its empire across the Atlantic Ocean, but the colonists who settled the territory did not want to be under Britain’s control, with their various taxes and regulations. Both sides were very passionate about their position on the issue, so a war occurred. This war featured a few heroes, including George Washington and Paul Revere. George Washington became America’s first president when we gained independence. I am not sure what happened to Paul Revere. The Declaration of Independence was written before the war by Thomas Jefferson in 1776 and made clear the position of the colonists. It was signed by many important people, including Ben Franklin and John Hancock. Ben Franklin is well-known for many things. One of these is inventing electrical conductors in the form of lightning rods. A famous tale is that he flew a kite with a small piece of metal somewhere on the string during a lightning storm. This was an effective way to test his theory. Another thing Ben Franklin is known for is publishing Poor Richards Almanac. This was like a magazine and contained some of his famous writings and quotations. One famous quote was “Tell me, I forget. Teach me, I remember. Involve me, I learn.” Maybe this had something to do with why he flew that kite. Trees are one of our most important natural resources. They are made of wood, and wood can be made into a variety of products. Some of the more obvious kinds are furniture, houses, and toothpicks. However, wood can also be made into paper. When I first heard this, I was skeptical, but it is true. Paper is a very important product in our society. Writers and artists have greatly benefited from the invention of paper. With only some paper and a pen or pencil, a writer can produce stories and poems that can captivate readers. They can also write down historical facts about their society. Actually, these writings don’t become historical until years later. At the time, the writings could probably be considered news. Artists use paper for their drawings and paintings. They can also use canvas. Drawings and paintings can be very beautiful. They can depict a wide variety of subjects, including flowers, animals, landscapes, and people. They can be realistic or impressionistic. Some paintings also attempt to convey emotions merely by the way the colors are combined and the brushstrokes are applied. This is a modern or contemporary approach to art. Many people think this approach does not require as much talent as the realistic styles. I will end my writing here. I have tried to make it very boring, and I hope I have succeeded. There are plenty of boring documents available for you to read. Check your public library for more information. You can also find boring materials at a bookstore or on websites. Sometimes this information can be found in magazines (which I mentioned earlier).\u0026quot;\u0026quot;\u0026quot; For the sake of simplicity, we remove all special characters (except a blank space) and even convert all text into lowercase text. To remove all special characters, we use a special type of language in python, called Regex, which stands for Regular Expressions. Its like telling python to do tasks which are otherwise hard to execute by code. For example, \u0026ldquo;take all alphabetical letters, except the letter \u0026lsquo;a\u0026rsquo;,\u0026lsquo;b\u0026rsquo; and \u0026lsquo;z\u0026rsquo;, but only uptil you encounter an underscore, after which, you only omit the letter \u0026lsquo;z\u0026rsquo; \u0026ldquo;, and so on. You get the idea! Its extremely useful to know regex in Python, and there are great tutorials online. But many times, you can find prewritten regex for your particular task on forums, and so not knowing regex is not necessarily a barrier!\nSo regex can be used after importing the re library.\nimport re We define a function that can remove all special characters from a text, and convert everything to lowercase. It then returns a list containing the words in the text in the exact same sequence as they occur in the text.\ndef simplify(text: str): return re.sub(r\u0026quot;\\W+|_\u0026quot;, \u0026quot; \u0026quot;, text).lower().split() #returns a list of words text =simplify(input) text[:10],len(text) So now, in order to create a model, we will create a data structure, which contains the next occurences of all words, because that\u0026rsquo;s what we\u0026rsquo;re concerned with. So we use a python dictionary, with keys as words, and their corresponding values as a list of all the successive occurences in all of the data (the text above).\nThe idea behind this is as follows. We will start with a word. Then we look up the successive occurences to the word from this dictionary, and choose a random word. Now this word is our present prediction. Now we will predict the next word through the successive occurences of this present prediction, and this cycle goes on. You can visualize this using the representation below. Each prediction acts as an input for the next prediction!\n(Credits: Joshua Payne for this article)\nEach word\u0026rsquo;s corresponding occurence list can have one word appear more than once. So we will simply make a list of all the successive occurences in the text - and a word can occur in this list as many times as the combination of the present word and a particular successor occur together. This will infact help us. Since we will choose a random word as our prediction, more the occurences of a particular word, more likely it is to be chosen, right?! So that aligns with our concepts very well.\ndef get_occurences(text: list): \u0026quot;\u0026quot;\u0026quot; This function takes in a sequence of text as a list, and creates a dictionary of all words in the text, with their values being a list of all words that have been seen to occur after the word, in the text. Sometimes, a word may not have a successor at all (like the ending word of the text). We don't want our predictions to stop. So we simply add all the words from the text to the value of that word key, so that the model will randomly choose any word from the entire corpus, and start predicting all over again. \u0026quot;\u0026quot;\u0026quot; dictionary={k:[] for k in set(text)} for k,i in enumerate(text[:-1]): dictionary.get(i).append(text[k+1]) for key in dictionary: if not dictionary.get(key): dictionary.get(key).extend(list(set(text))) return dictionary text_dict=get_occurences(text) print(text_dict) We can now finally build our Markov Model. We will randomly choose a word, and continuously predict the following words. To choose random items, we will use a library called random.\nimport random word=random.choice(text) #initialize with a random word word for _ in range(1000): print(word,end=' ') word=random.choice(text_dict[word]) And, with that, you have successfully built your text generation system.\nYou would notice that this generated text does not make a lot of sense (Though it does make sense somewhere, atleast in short phrases!). Thats because the model does not understand contexts. It only predicts words based on the current word, and does not consider the past words at all! This is called a memoryless model, meaning it does not have memory of past words, and only works based on the current input.\nIt also makes random predictions based on probabilities. This randomness often creates meaningless predictions. Nowadays, we use much more sophisticated models, such as Recurrent Neural Networks, that can understand very complex contexts and even hidden meanings in the text.\nReview In this session, we learnt about two very interesting Bayesian models, or probabilistic models - the Naive Bayes Classifier and the Markov Chain Model. These both work in different ways and have great applications in their own domains. We built a classifier using no iterative learning, and only through analyzing the frequencies of occurence of a particular value.\nSimple probabilistic models are very crude, because of the fact that there exists unguided randomness in these models. But yet, there does exist logical sense in these models.\nReview Questions These are highly recommended to go through. Make sure you clearly know the answer to each of these concepts. The answers to all these questions are somewhere in this notebook, so if you find yourself unclear with a concept, go back up and find the answer!\n What is the significance of probability in Machine Learning? Explain what $P(pred)$,$ P(data)$ and $P(data|pred)$ mean in essence?, and how are they related to our classification model? How have we represented each of $P(pred)$,$ P(data)$ and $P(data|pred)$ in code? Meaning, what data structures have been used, and how are they organised? Go to the section where we define get_prediction_accuracy function. Understand the working of this code, and in 4-5 lines of plain English, explain the working in pseudo terms. Can you give 5 different everyday examples, where the Markov Chain can be used? What is Regex? When would a Markov Chain result in an infinite looping situation?  Exercise Problem 1: Extended Naive Bayes Classifier In the above problem, we simply dropped all continuous variables (Why?). But there are ways to convert continuous variables into categorical variables. One such method is binning, which means, we divide the data into separate categories, or bins, based on their value. For example, values from 0 to 5 lie in one bin, 5 to 10 in another and so on. So let us try to include continuous variables as well into our data, and see if we can get a better Bayesian accuracy.\nSo first do it for the variable Age. According to the dataset Age lies between 0 and 85. So you can create bins of 10 years (ie, 0 to 10, 10 to 20 and so on).\nHint: You can do it easily using list comprehensions:\ntrain['Age'] = [some_function(i) for i in train['Age'] #Do it for the testing dataset too train=pd.read_csv('train.csv',index_col=0).drop(['id','Departure Delay in Minutes','Arrival Delay in Minutes','Flight Distance'],axis=1) test=pd.read_csv('test.csv',index_col=0).drop(['id','Departure Delay in Minutes','Arrival Delay in Minutes','Flight Distance'],axis=1) #continue your code from here Build a Naive Bayes classifier with the new dataframes now. Is there a significant improvement/deterioration in the accuracy? (There need not necessarily be, thats okay!)\nNext, build a classifier with various combinations of continuous variables, in addition to all categorical variables. Bin the continuous variables using an appropriate bin size. Keep the following things in mind - the number of bins should not be too large, or there is no difference between this and a continuous variable (both take large number of values, hence the corresponding probabilities of occurence would be very low), or too small either (otherwise there is loss of information). A moderate number of bins is sufficient. Try to push the accuracy as far as possible, and report which combination of features gives the best informatoin about customer satisfaction.\nAs a final step, try to think of which categorical variables do not affect customer satisfaction (through your own personal understanding), and remove upto 3 such variables, and see how far the accuracy can be achieved. This answer is subjective, so accuracies may vary for everyone.\nFor your reference, the following are all the features of the Airplane Passenger Satisfaction Dataset.\n['id', 'Gender', 'Customer Type', 'Age', 'Type of Travel', 'Class', 'Flight Distance', 'Inflight wifi service', 'Departure/Arrival time convenient', 'Ease of Online booking', 'Gate location', 'Food and drink', 'Online boarding', 'Seat comfort', 'Inflight entertainment', 'On-board service', 'Leg room service', 'Baggage handling', 'Checkin service', 'Inflight service', 'Cleanliness', 'Departure Delay in Minutes', 'Arrival Delay in Minutes', 'satisfaction'] id will be always dropped (removed) from the dataframe\n Problem 2: Markov? Shakespeare? What is in the name! We learnt that the next prediction of a Markov Model only depends on the present prediction/value, i.e. the model is a memoryless model.\nLet us build a memory model, ie a model, which also takes into account the predecessor of the current prediction, along with the current prediction itself. Meaning, what would be the next word, given the last 2 predictions?!\nBut this most probably won\u0026rsquo;t work on the text we wrote above (the boring text), because it contains only 2400 words. So there is a very low probability that the successor of two consecutive words will have more than one possible options. We will end up getting the exact same text as the text we wrote, or almost on the same lines.\nLet us download a bigger text file - how about a whole Shakespeare play. And we will try to build a model that can predict text in the style of Shakespeare!\nLet us download this dataset and put it in a string.\n!kaggle datasets download -d adarshpathak/shakespeare-text !unzip shakespeare-text from pathlib import Path txt = Path('text.txt').read_text() txt[:1000] #print the first 1000 words Now we need to define our functions. We\u0026rsquo;ve written the code to incorporate two consecutive words at once.\ndef simplify(text: str): words = re.sub(r\u0026quot;\\W+|_\u0026quot;, \u0026quot; \u0026quot;, text).lower().split() return [' '.join((words[i],words[i+1])) for i,_ in enumerate(words[:-1])], words def get_occurences(text: list,original_wordlist): dictionary={k:[] for k in set(text)} for k,i in enumerate(text[:-1]): dictionary.get(i).append(text[k+1]) for key in dictionary: if not dictionary.get(key): dictionary.get(key).extend(text) return dictionary Continue the code from here, and write the loop function to generate the Markov Chain Model\u0026rsquo;s predictions.\nRandomly choose a word, and predict the next 100 words.\n "});index.add({'id':5,'href':'/docs/05-clustering-algorithms-in-machine-learning/','title':"05 Clustering Algorithms in Machine Learning",'section':"Docs",'content':"Clustering Algorithms in Machine Learning K Means and K Nearest Neighbors Algorithms Welcome to the fifth session of the Practical Machine Learning sessions. In this session, we will move slightly towards a relatively overlooked domain of Machine Learning - Unsupervised Learning. Till Now, and for the most of the part in the future, we will be looking at Supervised Learning problems. So, first of all, what is the difference between supervised learning and unsupervised learning?\nIf I were to tell you the textbook definition of the difference between the two,I would simply tell you, that unsupervised learning requires labels, and supervised learning does require labels. Yes, that is a difference, but in introspection, it is really not a good way to tell the difference between the two. We simply answered the question, \u0026ldquo;What do these learning methods require in order to learn\u0026rdquo;, but not the most important question - What on earth IS unsupervised learning, or supervised learning, and what is the difference in terms of the approach?\nSo, lets spend some time to understand that first. All of the supervised training algorithms we saw till now had a very common theme. We had some data. We build a model which takes in inputs, and gives and output (the input and output may not necessarily be different things. For example, in the text generator example, the input itself was the target).\nAnd we try to optimize the model such that the prediction outputs and target variables are as close to each other as possible. By this, we intend to build a model that knows how to map those inputs to the real world targets. For example, if you were to build a model that can generate music for you, you provide your model with lots of examples of music peices, and expect the model to learn how to make music, without explicitly teaching it music theory, or the idea of melody, etc, and only through example. The idea is - we try to optimize the model with a specific goal in mind that we wish the model performs.\nThis is not the case for unsupervised learning. We dont build models with a specific goal in mind. The objective is to simply identify whatever patterns are observable in the data. Like, What is the mathematical distribution? How dense is the data? What is the range of the values in the data? How many clusters are observable? Is there a certain visible relation between different types of data?(like, if one increases, does the other increase too?) and so on.\nUnsupervised learning is used to identify patterns, whereas supervised learning is used to get specific results from the data. This is the correct method to differentiate between these two!\n Now, coming to our topic today - Clustering Algorithms! We\u0026rsquo;ll be looking at two algorithms - K Means and K Nearest Neighbors Algorithms, and we\u0026rsquo;ll explore both separately! So let\u0026rsquo;s get started\u0026hellip;\nK Means Algorithm Take a look at this data below.\nThis is a social network graph. It is basically a visualization of who is a connection of whom. For example, if this network belonged to Facebook, the image would depict which people are friends on facebook, and who aren\u0026rsquo;t. People who are friends on facebook have a faint grey line joining their profile images. You might notice that there are some distinct groups in this photo, which appear as blobs. You can see two major such blobs, one on the left and one on the right. These are called clusters of data.\nLets say that people in the left blob are college going kids, and on the right are business professionals. Obviously, college going kids are more likely to know each other than they would know business professionals, and similarly business professionals are more likely to know each other, rather than college going kids, only because of the shared experiences and backgrounds. Hence, you would expect college kids to be friends with each other, far more than you would expect college kids being Facebook friends with business professionals. Hence these two clusters.\nWithin each cluster, people are connected to each other via Facebook. Obviously it is not true, that no college kids are connected to any business professionals, and vice versa. But still, there is such a stark lack of such connections, that the graph almost seems like its divided into two major clusters.\nJust by looking at the data, you can tell that there are 2 clusters in the data. You don\u0026rsquo;t even have to know which person goes to college, and which is a business professional. We would like our machines to do that for us - without necessarily having to tell which person belongs to which category. Hence the concept of Unsupervised Learning!\nWhy would we want to identify clusters? Let\u0026rsquo;s take the example of Facebook itself. Facebook can\u0026rsquo;t recommend the same Ads to everyone. It obviously needs to optimize this. But Facebook knows that if some college goers are interested in a product, say Fidget Spinners (even though that\u0026rsquo;s not a thing anymore!) , others will be too! And they will be much more likely to buy these, than business professionals. So Facebook would recommend Fidget Spinners to college goers rather than business professionals. On the other hand, if Facebook realizes that there is a trend of buying international holiday trip packages, it would rather recommend those to all business professionals, rather than college kids, which are much less likely to afford one! Ofcourse to do all this, Facebook might not necessarily know the fact that which group is which in profession.\nHope this idea is clear!\nIn Machine Learning, a great algorithm to carry out unsupervised clustering is the K Means Algorithm.\nThe objective of the K Means Algorithm is to create K different clusters, that are visible separated from each other. The procedure is as follows:\n create K random data points, called as centroids. Each of these centroids will form a separate cluster later on. For each datapoint, find out which centroid is the datapoint closest to. Whichever centroid it is closest too, assign the datapoint the same group as the centroid. So at the end, each datapoint will be assigned a group, containing exacly one centroid. Update the value of centroids as the Mean of all the datapoints that exist in the group. Repeat Steps 2 and 3 until you get desired results. During step 2, the group assigned to a datapoint may change, depending on which centroid it is closest too at this point (after updating the position of centroids).  Imagine what happens during step3. By updating the value of the centroid as the mean of all the datapoints, we are essentially pulling the centroid to where there is more density of data. If this process is repeated multiple times, we will eventually pull the centroid to a position where this high density of data, or in other words, a blob/cluster exists. Here is a quick visualization of the K Means algorithm.\nIn today\u0026rsquo;s session we\u0026rsquo;ll be looking at a very interesting problem - We will be identifying the crime hotspots of a city - meaning, identifying which areas are more prone to crimes and which are safer.\nCase Study: Crime Hotpots in Vancouver We have some data about crimes in the city of Vancouver, Canada. The data includes their nature, the time of occurence, the location, etc. Our job is to identify the areas of the city where the crime is more likely to occur. This can help the local Police Department deploy enough personnel, so that they are ready, equipped, and well numbered to tackle crimes.\nThis can also help people like us, who would want to find new homes. We would obviously like to buy a house in a safer locality, rather than a crime-prone locality.\nSo let us first download the dataset from kaggle. Before that, you need to upload the kaggle.json file\n#run only once per session %cd from google.colab import files uploaded = files.upload() !mkdir -p ~/.kaggle/ \u0026amp;\u0026amp; mv kaggle.json ~/.kaggle/ \u0026amp;\u0026amp; chmod 600 ~/.kaggle/kaggle.json !kaggle datasets download -d wosaku/crime-in-vancouver !mkdir -p vancouver_crime \u0026amp;\u0026amp; unzip crime-in-vancouver.zip -d vancouver_crime %cd vancouver_crime !ls As you can see, there is a file called the \u0026ldquo;crime.csv\u0026rdquo; in this dataset. So to read csv files, we\u0026rsquo;ll be using the pandas library. We\u0026rsquo;ll also be using the matplotlib library to visualize the locations of crimes in a graph plot.\nimport pandas as pd import matplotlib.pyplot as plt df=pd.read_csv(\u0026quot;crime.csv\u0026quot;) print(len(df)) df.head() So there are 530,652 cases of crimes in this dataset. You can see, the dataset provides us with the coordinates of the crimes, as well as the Lattitudes and Longitudes and the times. They also provide us with the type of crimes. However, a crime is a crime, and we don\u0026rsquo;t necessarily need to know the type of crime while looking for a house, or while the police department deploys personnel in different regions. But still, let us try to see what all crimes happen in Vancouver!\ndf.TYPE.unique() Before we start working with this data, let us remove any empty records (for example, a policeman, while recording the crime, might not have filled the coordinates, or might not have thought of it as relevant to the crime).\ndf.dropna(inplace=True) So let us now visualize this dataset. Let us plot on a graph, all the X cordinates and the Y cordinates of the crimes, so we get an idea of where do crimes take place, and how many crimes take place in any locality (a rough idea!). But before that, lets see the actual map of Vancouver!\nThis map also shows the Neighborhoods in Vancouver, which is provided to us in the dataset as well.\nLet us now plot all crimes on a graph, and see where in Vancouver are crimes found!\nplt.scatter(df.X,df.Y,s=0.001) #s is used to control the size of datapoints on the graphs. There are so many datapoints, that if you use the normal size, all the datapoints will get merged. Hence, use a small 's' You can see, that there are crimes all over the city. However, you may observe that in some parts, the density of crimes is higher than the rest of the city (For example, in the northern parts of the city)!\nSo let us see, which neighborhoods have more crime than others. Below, we plot a colour coded map showing different neighborhoods, as well as a bar graph, showing the number of crimes per neighborhood.\ncategory='NEIGHBOURHOOD' for name,group in df.groupby(category): plt.scatter(group.X,group.Y,label=name) plt.legend(bbox_to_anchor=(1, 1), loc='upper left'); count={i:len(df[df[category]==i]) for i in df[category].unique()} # a dictionary that can help us keep track of which neighborhood has how many crimes for i in count: plt.bar(i,count[i]) plt.xticks(rotation='vertical'); Now that we know the number of cases per neighborhood, Let us jump to our ultimate objective - to differentiate neighborhoods by the number of crimes. We would like to create 3 clusters - High crime rate neighborhood, Moderate crime rate neighborhood, and Low crime neighborhood. Let us first also plot the number of crimes in a straight line.\nfor i in count: plt.scatter(count[i],0,label=f'{i}: {count[i]} cases') plt.xlabel('Number of Crimes') plt.legend(bbox_to_anchor=(1, 1), loc='upper left') You\u0026rsquo;ll notice some things - The Central Business District has more crimes than any Neighborhood, and by a HUGE margin. Obviously that will be clustered as a high crime region. Then come low crime regions and moderate crime regions. We would like our model to automatically detect which district is lower/moderate in crime (based on blobs).\nLet us begin by initializing centroids. To do this, we\u0026rsquo;ll be using the NumPy library, which is a numerical processing library in python. It provides us with mathematical tools, which are otherwise computationally and memory wise - intensive. NumPy also provides us with arrays which is a concept similar to tensors in PyTorch. However, NumPy arrays are 1-Dimensional only. We\u0026rsquo;ll be using these arrays to do calculations on our data.\nWhy numpy arrays and not lists? This is because NumPy arrays and functions are optimized to run at C/C++ speed, which is many orders of magnitude faster than Python itself. This is suitable for large amounts of data, and here we\u0026rsquo;re dealing with hundreds of thousands of datapoints.\nimport numpy as np values=list(count.values()) clusters=['Low','Moderate','High'] colour_scheme={'Low':'green', 'Moderate': 'orange', 'High': 'red'} num_clusters=len(clusters) #INITIALIZE CENTROIDS centroids=np.sort(np.random.uniform(low=min(values),high=max(values),size=num_clusters)) centroids #initial centroids. Represent the number of crimes As we mentioned before, K Means intrinsically doesn\u0026rsquo;t know which cluster belongs to what kind of crime rate. But for better understanding, we will assign the categories of crime rates (Low, moderate and high). So to do that, what we\u0026rsquo;ve done is we\u0026rsquo;ve made sure that the randomly generated centroids are always in ascending order. These represent ascending order of crime rates. Correspondingly, the labels will automatically become Low Crime Rate, Moderate Crime Rate and High Crime Rate.\nWe\u0026rsquo;ve also assigned colour codes to these - green being low crime regions, orange for moderate and red for high crime regions.\nBelow, we\u0026rsquo;ve plotted the crime rates, along with the centroids, marked with crosses.\nplt.scatter(list(count.values()),[0]*len(count)) plt.scatter(centroids,[0]*num_clusters,marker='X',s=100) plt.xlabel('Number of Crimes'); We\u0026rsquo;ll make lists corresponding to each of these categories. For example, if a neighborhood\u0026rsquo;s crime frequency is closest to the centroid corresponding to Low Crime rates, we will add the neighborhood to the list corresponding to low crime rates\ncategories=[[] for _ in range(num_clusters)] for i in count: categories[np.argmin([abs(count[i]-o) for o in centroids])].append(i) categories You can see, there are three lists in the variable categories, containing neighborhoods that lie closest to the corresponding centroids (randomly initialized, so the may not make proper sense right now).\nSo now, We will assign the crime rate category to each datapoint that belongs to a certain category. This way, we can colour code the city, based on the location of the crime.\ndf['groups']=None for k,o in enumerate(categories): for i in o: df.loc[df['NEIGHBOURHOOD']==i,'groups']=clusters[k] df.head() We\u0026rsquo;ve added a column in the dataframe, called \u0026lsquo;groups\u0026rsquo;, which contains the crime rate category, as done by K means algorithm.\ncategory='groups' for name,group in df.groupby(category): plt.scatter(group.X,group.Y,label=name,c=colour_scheme[name],s=0.1) plt.legend(bbox_to_anchor=(1, 1), loc='upper left'); You can see, that each region is colour coded as per crime region. Note: you MIGHT be encountering a small problem - you might not be seeing all the categories. You might be seeing less than 3 categories. This is because K Means is very sensitive to initialization. Because the initialization is random, it may be possible that some centroids do not lie closest to any datapoints. Each datapoint might be closest to any centroid But a particular centroid. If you\u0026rsquo;re encountering such a problem, go back up, and reinitialize the centroids, and run all the cells of code upto the one just above this peice of text, and repeat this until all three categories are not visible. (Even if you are not encountering this issue, its a fun exercise to see this happen!).\nNext Step? Update the centroids as the mean of all datapoints that have been assigned to that cluster.\nfor k,o in enumerate(categories): if o: centroids[k]=np.mean([count[i] for i in o]) centroids Now all we have to do is repeat this procedure (excluding the random initialization - that needs to be done only once). So Let us write all the peices of code together, and see it work in one go!\nTry running the next two cells of code multiple times to see if you get slightly different results each time. This is because of the randomness during initialization. But nevertheless, you will get satisfactory results.\n#Step 1: initialize centroids centroids=np.sort(np.random.uniform(low=min(values),high=max(values),size=num_clusters)) centroids #initial centroids for iteration in range(5): #We run this model for 5 iterations #Step 2: categorize all points into one or the other groups categories=[[] for _ in range(num_clusters)] for i in count: categories[np.argmin([abs(count[i]-o) for o in centroids])].append(i) for k,o in enumerate(categories): for i in o: df.loc[df['NEIGHBOURHOOD']==i,'groups']=clusters[k] #Step 3: update centroids for k,o in enumerate(categories): if o: centroids[k]=np.mean([count[i] for i in o]) # Visualize for name,group in df.groupby(category): plt.scatter(group.X,group.Y,label=name,c=colour_scheme[name],s=0.1); plt.title(f'iteration: {iteration+1}') plt.show() And there you have it! You have successfully built your own Crime Hotspot Detection system! And using 4 simple steps only - Initialize, Assign, Update, Repeat! Congrats on that!\nBut there is something we can, and should do. These 4 steps should not so many lines of code. We should refactor the code into functions, so that the model becomes easier to run and understand. So we\u0026rsquo;ve build a basic structure to help us with all the major steps.\ndef initialize_centroids(values,num_clusters): centroids=np.sort(np.random.uniform(low=min(values),high=max(values),size=num_clusters)) return centroids def count_frequencies_in_df(df,group_by='NEIGHBOURHOOD'): df.dropna(inplace=True) df.count_freqs={i:len(df[df[group_by]==i]) for i in df[group_by].unique()} def categorize(df,centroids,clusters,group_by='NEIGHBOURHOOD'): num_clusters=len(clusters) categories=[[] for _ in range(num_clusters)] # for i in df.count_freqs: categories[np.argmin([abs(df.count_freqs[i]-o) for o in centroids])].append(i) for i in df.count_freqs: argmin=np.argmin([abs(df.count_freqs[i]-o) for o in centroids]) categories[argmin].append(i) df['groups']=None for k,o in enumerate(categories): for i in o: df.loc[df[group_by]==i,'groups']=clusters[k] return categories def update_centroids(centroids, categories): for k,o in enumerate(categories): if o: centroids[k]=np.mean([df.count_freqs[i] for i in o]) return centroids def visualize(df, category_column, iteration ): for name,group in df.groupby(category_column): plt.scatter(group.X,group.Y,label=name,c=colour_scheme[name],s=0.001); plt.title(f'iteration: {iteration+1}') plt.show() And let us see an entire Crime Hotspot detection model running in only 7 lines of code!\nPS: Try running the cell below multiple times, to get slightly different results.\n#Entire K means in just 7 lines of code , excluding comments, ofcourse #step0 group_by='NEIGHBOURHOOD' count_frequencies_in_df(df,group_by=group_by) #method which creates a data structure based on which distinction is to be done # step1 centroids=initialize_centroids(values, num_clusters) for i in range(5): #step2 categories = categorize(df,centroids,clusters,group_by=group_by) # step3 centroids = update_centroids(centroids,categories) visualize(df,'groups',iteration=i) #repeat step 2 through 3 Just for comparison, here is the map of all crimes. Try noticing if high density regions have been correctly marked as red, and if low density regions correcly as green? What about orange?\nplt.scatter(df.X,df.Y,s=0.001)  Now, let us move onto the second algorithm that we\u0026rsquo;ll look at today - the K Nearest Neighbors algorithm.\nK Nearest Neighbors K nearest Neighbors is a very simple algorithm. You are given some data. Now, suppose when you get some new data point, and you wish to either classify or predict a value, you simply look at its neighbors.\n  If you want to classify a datapoint into a category, look at K nearest datapoints. Whichever category occurs the most, is most likely to be the class of the datapoint as well.\n  If you want to conduct regression, look at the values of K nearest datapoints. You can take the average of the taget values of these datapoints, and it will be a likely estimate of the target value of your datapoint as well.\n  The idea is simply as follows - \u0026ldquo;Birds of a feather flock together\u0026hellip;\u0026quot;.\nThis is technically a supervised learning problem, but still is closer to unsupervised learning than other supervised learning algorithms. There is no learning of parameters, only observing the position of other datapoints relative to the input data point.\nSo previously we tried to cluster neighborhoods based on the number of crimes per neighborhood. Now we will try to classify each locality with respect to the most common crime in that area. We will analyse K nearest Neighbors and figure out which crime is the most common. Ofcourse, that is the most likely crime to happen at that particular location.\nLet us begin with some random coordinates and visualize it on the map!\ncoordinates=(494000,5453000) plt.scatter(df.X,df.Y,s=0.001) plt.scatter(*coordinates,c='RED',marker='X',s=100) x,y=coordinates The next step is to calculate the Distance of this coordinate from other datapoints. This will be used to find the the nearest neighbors! But we won\u0026rsquo;t write a for loop - that is awfully slow. Infact, we will use function that NumPy provides, which run at C/C++ speed. For 2D cordinates, the distance is defined as the euclidean distance between the two, given by -\n$$distance((x_1,y_1), (x_2,y_2)) = \\sqrt{(x_1 - x_2)^2 + (y_1 -y_2)^2}$$\nWe write all these distances in a new column of the dataframe - the \u0026lsquo;Distance column\u0026rsquo;.\ndf['Distance']=np.sqrt(np.square(x-df.X) + np.square(y-df.Y)) Now, we need to find nearest neighbors. To do that, we will simply sort the dataframe with respect to the distances, and simply choose the top K rows.\ndf.sort_values('Distance',inplace=True) You can see that the rows are arranged in the ascending order of the distances.\ndf.head(10) Choose a K. There is no definite method to choose K. But for this case, we\u0026rsquo;ll choose K as 1000. Meaning, we\u0026rsquo;ll be analysing the most probable crime on a given pair of coordinates, based on 1000 surrounding points. Whichever crime occurs in majority in these 1000 points, is the most likely crime in that location as well.\nk=1000 Finally, let us carry out the prediction, which is simply the crime which occurs most frequently in the K points.\nprediction = df[:k].TYPE.value_counts().index[0] prediction Hence, we have found the most probable crime on those coordinates. Let us also see, that, out of 1000 datapoints, how many of each crimes occur, and verify that the most frequent crimes is the same as our prediction.\ndf[:k].TYPE.value_counts() Let us visualize this! We plot only these K points, and see, the number of crimes in the area, and visually verify that most common crime in the area is the same as our prediction. For better visual aid, we\u0026rsquo;ve enlarged the most probable crime with respect to other crimes.\nfor name,group in df[:k].groupby('TYPE'): plt.scatter(group.X,group.Y,label=name,s=50 if name is prediction else 1 ) plt.scatter(x,y,c='RED',marker='X',s=200,label='Target') plt.legend(bbox_to_anchor=(1, 1), loc='upper left') And thats it! You\u0026rsquo;ve built your own crime predictor as well! With such less effort, that too!\nBut again, we\u0026rsquo;ve written way too much of code for such simple tasks. We should refactorize the code. Let us write some appropriate functions, carrying out the same tasks.\ndef visualize_target(coordinates:tuple, df): x,y=coordinates assert df.X.min()\u0026lt;=x\u0026lt;=df.X.max(),f'x cordinate should lie in Vancouver City - between cordinates {df.X.min()} and {df.X.max()}.' assert df.Y.min()\u0026lt;=y\u0026lt;=df.Y.max(),f'y cordinate should lie in Vancouver City - between cordinates {df.Y.min()} and {df.Y.max()}.' plt.scatter(df.X,df.Y,s=0.001) plt.scatter(x,y,c='RED',marker='X',s=100) def euclidean_2d_distance(x,y,df): return np.sqrt(np.square(x-df.X) + np.square(y-df.Y)) def get_k_nearest_neighbors(df,target_coordinates,k=1000,distance_func=euclidean_2d_distance): df['Distance']=distance_func(*coordinates,df) df.sort_values('Distance',inplace=True) return df[:k] def get_prediction(df): prediction = df.TYPE.value_counts().index[0] return prediction def visualize_prediction(df,pred=None,k=None): if pred is None: pred=get_prediction(df) if k is None: k= len(df) for name,group in df[:k].groupby('TYPE'): plt.scatter(group.X,group.Y,label=name,s=50 if name is prediction else 1 ) plt.scatter(x,y,c='RED',marker='X',s=200,label='Target') plt.legend(bbox_to_anchor=(1, 1), loc='upper left') # KNN in 4 lines visualize_target(coordinates,df) #line1 pred=get_k_nearest_neighbors(df,coordinates) #line2 get_prediction(pred) #line3 visualize_prediction(df,pred=pred,k=1000) #line4 Try running this algorithm with a different pair of coordinates!\nReview In this session, we studied about clustering algorithms in Machine Learning, which is a step towards Unsupervised learning in ML. In both the algorithms above, we did not direct our model towards a specific goal, but did general inference (patter identification) on the data. And still got some really meaningful results. That being said, one should not think that unsupervised leanring is not useful at all. It is very useful in its own applications.\nA few practical Tips The only useful tip that you need to learn from this session is - Refactoring code. Most Machine Learning practitioners do not pay attention to this, and this creates a problem for them later on. Why? because if you simply make a good structure of code, you need not rewrite the entire structure if you want your model to perform just slightly differently. As you will see in the exercises below (in the K Means related question), all you need to do is modify one function to solve the biggest problem that K Means algorithms face.\nQuestionaire: These are non-evaluative, but highly recommended to go through. Make sure you clearly know the answer to each of these concepts. The answers to all these questions are somewhere in this notebook, so if you find yourself unclear with a concept, go back up and find the answer!\n What characteristics of Unsupervised Learning does the KNN algorithm have? What is the advantage of the NumPy library? Similar to the bar graph depicting the number of crimes per district, can you write a piece of code to build a bar graph showing number of crimes per category? (like, Theft from Vehicle and so on).  Exercises (Evaluative) 1. A Robust K Means Algorithmic system We learnt about the initialization problem in K Means. Many a times, a cluster is not assigned any datapoints. Let us try to solve it.\nAll we need to do is, keep re-initializing the centroids, until some or the other datapoint lies closest to it, and the datapoint is assigned that cluster. You need to redefine the initialize_centroids function to implement this functionality.\nThen run the model again to see if your function works. (Even After reinitializing multiple times, there should be no empty cluster)!\n 2. Regression using K Nearest Neighbors Just like classification, we\u0026rsquo;ll be carrying out Regression. Instead of finding the majority class, our Job is to find the average of the values of K Nearest Neighbors.\nIn this exercise, we will be finding out the most probable Hour of crime in a particular locality. You can use any cordinates as the reference. Use k as 100.\n "});index.add({'id':6,'href':'/docs/06-support-vector-machines/','title':"06 Support Vector Machines",'section':"Docs",'content':"06 Support Vector Machines Welcome to the 6th Practical Session on Machine Learning. In this lab, we\u0026rsquo;ll be learning about Support Vector Machines. This lab session is a fairly short session, so it may be more enjoyable to study. Support Vector Machines, or SVMs for short, were considered a very strong system for classication, and even regression in the 1990s. At that time, People did not believe in the systems that we use today, like the Neural Networks or Random Forests, so SVMs were pretty much State of the Art. In the early 1990s, there was almost no practical evidence that Neural Networks could perform better than SVMs. But that was obviously when Neural Networks were really shallow (there was no Deep Learning), and now we know for sure that Neural Networks are far more sophisticated, complex are well performing, than SVMs. But, SVMs are still an integral part of Machine Learning - SVMs can literally be approximated as other techniques such as Linear Regression or Logistic Regression.\nIdeally, Support Vector Machines has a lot of complicated Math and Theory. According to many, the math behind just the working of the Support Vector Machines is even more complex than Neural Networks. So, in the spirit of the practicality that we always try to pursue in these Lab Sessions, instead of the math and theory, we will not go deep into theory, but learn how to run SVM models in practice. You only need a basic understanding cum overview of the working of SVMs to be able to run SMVs. To be honest, you will mostly never need the theory (unless you decide to research upon it). And we\u0026rsquo;ll leave the theory of it to your Lecture Classes.\nSo what are SVMs in the first place?\nWhat are Support Vector Machines Lets start with a simple binary classification example. Our job is to create a model that separates the two. In Logistic regression, we modelled a line, that gave a value either greater than 0 or less than 0 (and hence an output of less than 0.5 or more than 0.5 when passed through a sigmoid function), based on which category the datapoint lies in. Something like the figure below:\nThis is all that we did in Logistic Regression - model a line\n(or a plane to be more precise - A line is basically a 1-dimensional version of a 2D plane, and separates 2-D data. A 2D plane separates 3-D data, and similarly there can be N-dimensional planes as well, to separate N+1 dimensional data)\nthat best separates the two categories of data. Basic Geometry tells us that if you model a line(or plane) $f(x) = 0$, and then give it an input x that does not lie on the line, you will get a non-zero value ($f(x) \\neq 0$). If you input a value that lies on one side of the line, you will get a positive value, and if you input a value that is on the other side, you will get a negative value. So it makes sense to build a line that lies in the middle of the two clusters of data. Ofcourse its not always possible to always get 100% accuracy, because of the distribution of data. But our focus is to get the best possible results.\nSVMs simply do the same, except that SVMs take it to the next level. SVMs not only try to get the best results, but also the best fitting line, even if the accuracy might not increase per se. What do we mean by the best fitting line?\nIn the figure given above, all the three lines ($L1$,$L2$ and $L3$) would ideally give 100% accuracy in this binary classification problem, since they clearly separate the two classes. But Imagine what would happen if you get new datapoints around the existing clusters. Lines $L1$ and $L3$ are so closely cutting one of the categories, that there\u0026rsquo;s a much higher chance of misclassification when new data would be introduced. Line $L2$ seems much more robust in that view.\nMathematically, whats going on, is that Line $L2$ has the best Margin of Separation. Meaning, its the most far away from any datapoint . This creates room for variance in data. Even if you introduce new data that is slightly out of the already-existing clusters, the line with the most margin (Line L2) is still more likely to correctly separate the categories. This is what the Margin of Separation looks like:\nThe space in between the two dotted lines is the margin of separation. There ideally should be no data in between these two lines. The actual separating (classifying) line is in the between the two dotted lines.\nA logistic regression model would not know the difference between the lines $L1$,$L2$ and $L3$. SVM does! SVM not only tries to minimize the error in prediction, but also tries to maximize the margin between the two categories.\nSo our data in N Dimensional, there is SOME N-dimensional plane that Best separates the data, with the widest margin of separation. The remaining idea is the same - build a model, build a cost function that you wish to minimize in order to get the best results, and build an optimization method that changes the position of the line/plane such that the cost function is minimized.\nBasically,\n  Step1:\nModel a line that can separate the data. Say $y=w_1 x_1 + w_2x_2 + \u0026hellip;. $\n  Step2:\nCreate a loss function. This loss function not only accounts for error in prediction (which we\u0026rsquo;ve already done in the Logistic Regression Lab , which isn\u0026rsquo;t important to remember letter-by-letter as of now!) as well as the margin. And we need to minimize the loss function. But if you remember, we wanted to maximize the separation. That\u0026rsquo;s no problem, simply take the inverse of the term. Maximizing a term is the same as minimizing its mathematical inverse!\n$Minimize: Loss = BinaryLossOfPrediction + \\frac{1}{Margin of Separation}$\n  Step3:\nOptimize the line/plane by minimizing the Loss function. How do we minimize the loss function? You can use the technique that we already know - Gradient Descent. Or you can even use a direct formula based approach as we saw earlier.\n  That is all of SVM in theory - atleast the theory that we need to understand the basic intuition behind SVM. The above approach applies to classification, but regression is possible too! The approach is the same. Build a model, build a loss function, and build an optimizer! Ofcourse there\u0026rsquo;s no Margin of Separation concept in regression, so we simply have to build a Linear Model.\nWe won\u0026rsquo;t implement all these details by ourselves. Fortunately, there are libraries that do this for us. We\u0026rsquo;ll be using SciKitLearn\u0026rsquo;s SVM implementation to carry out the task. This is a wonderful point to talk about SKLearn (SciKitLearn)(also called SKL, for short).\nSKLearn is a wonderful Machine Learning Library, containing tools to build almost all fundamental Models - including Linear Regression, Logistic Regression, PCA, even Neural Networks. The only issue is, that there is only so much flexibility that it provides. So, either ways, its a great library to build basic implementations of models. If you however, wish to build more complex functionalities, this is where more sophisticated libraries come into picture.\nThe limitations in the functionalities of SKLearn are mostly because of obsolete code structures. If you go into the source code of SKLearn, at some point, it assumes the inputs to functions to be of a very specific type, representing only specific types of information.\nBuilding an SVM Model Let us build an Classification model using SVMs. We\u0026rsquo;ve looked at many Image Classification models, but haven\u0026rsquo;t built NLP (Language) Models. So Let us explore a language model, just for fun!\nIn this lab, we\u0026rsquo;ll be learning to build a Movie Review Classification System. We have some reviews from actual people over actual movies from IMDB\u0026rsquo;s website, and our model will classify which review are positive reviews, and which are negative reviews!\nTo help you appreciate the problem - a model that can identify whether the sentiment behind a piece of text is positive or negative, needs to understand meanings of words. Consider a positive statement, such as \u0026ldquo;I Loved this movie!\u0026quot;. As humans, we look for keywords, like Love or Hate or liked or (did) Not (like), to identify the sentiment. Our model would need to identify how to separate such words. First of all, it would need to find a method to represent words in space. Then it would need to find a model (like a linear model), that separates the two concepts (positive and negative).\n Side Note: One of the objectives of these Lab Sessions is not just to learn to learn how models work, but also to explore different problems. Many a times, you will have to deal with data that are of different types - the data can be images, videos, texts, audios, graphs, etc. As Data practitioners, its our job to learn how to handle all these different types of data, and convert them into data that the computer can understand (into numbers, that is ). For example, Computers cannot understand Images, but it can understand tensors, or matrices. We will now look at methods to represent text in terms of numbers that our model may understand.\n The IMDB dataset is available here. Since it is a kaggle dataset, we would need to upload a kaggle.json file. Run the Cell below to do that!\n#run only once per session\r%cd from google.colab import files\ruploaded = files.upload()\r!mkdir -p ~/.kaggle/ \u0026amp;\u0026amp; mv kaggle.json ~/.kaggle/ \u0026amp;\u0026amp; chmod 600 ~/.kaggle/kaggle.json\r!kaggle datasets download -d lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\r!mkdir imdb !unzip imdb-dataset-of-50k-movie-reviews.zip -d imdb\r% cd imdb\r!ls\rAs you can see, we have a csv file, so we will import the pandas library to read it as a dataframe. According to the dataset\u0026rsquo;s webpage, there are 50000 reviews along with the sentiments of whether they are positive or negative reviews.\nimport pandas as pd\rimdb_dataset=pd.read_csv('IMDB Dataset.csv')\rimdb_dataset.head()\rThe imdb_dataset dataframe contains two columns - review, which contains the review themselves, and the sentiment, which contain labels. Lets look at one of the reviews.\nsentence=imdb_dataset.iloc[0]\rprint(sentence.sentiment) #tells us which type of review it is (positive of negative)\rsentence.review\rLet us also find out how many positive and negative reviews are there in the dataset. Ideally, we would want our dataset to be well balanced - meaning, the number of positives should be more or less equal to the negatives. Otherwise, there is a chance of the model being biased towards the more dominating category. Also the accuracy metric does not perform as well on unbalanced categories.\nimdb_dataset['sentiment'].value_counts()\rThankfully, this dataset contains equal numbers of positives and negatives.\nThe next job is to split our dataset into the training and validation dataset. If we have no validation dataset, there is no way to know how well the model performs at the end. We\u0026rsquo;ve defined a function to do so. It randomly picks frac fraction of data into the training dataset, and the remaining go to the validation dataset.\nNote: Here, we will only use a total of 1000 data points in total. This is because, if you go to the documentation of SVM Classifier by SKLearn, it says that the time required to optimize the model is dependent on the number of datapoints, by a quadratic factor. So as the number of datapoints increase, the model fitting time increases. Beyond a point, it becomes practically infeasible.\ndef split_dataset(df,frac=0.8):\rtrain=df.sample(frac=frac) #random state is a seed value\rtest=df.drop(train.index)\rreturn train,test\rtrain,test=split_dataset(imdb_dataset.iloc[:1000])\rlen(train),len(test)\rHopefully in our small subset, the number of positives and negatives are nearly balanced. (A little off than perfectly balanced is okay!)\ntrain['sentiment'].value_counts()\rNow that we are done setting up our data, we need to find a way to represent words from our data in a way that our model can understand. The simplest way, is to create vectors, one dimensional arrays(or tensors), each element represents a word. If in a sentence, the word occurs, put a 1 in that place, and if that word doesn\u0026rsquo;t occur in the sentence, put a 0 in that place. This is called a hot encoding vector. In SkLearn, this is called a CountVector.\nfrom sklearn.feature_extraction.text import CountVectorizer\rvectorizer=CountVectorizer()\rtrain_vectors = vectorizer.fit_transform(train['review']) # transform the text to countVectors using the information in the training dataset\rtest_vectors = vectorizer.transform(test['review']) #just like the training dataset, we also need to convert the text in the testing set into vectors. Note, vectorizer is simply a python class, with methods such as fit_transform, which takes in the training dataset, and based on the words present in it, creates a vocabulary of words, and a sample vector, where each element represents a word.\nThen once the vocabulary has been built, we create vectors for the testing dataset as well.\nYou can see below, that there are a total of about 16000 unique words in the training dataset.\nNext, build the model! You can go to the documentation to see an example of how the model works. We will follow the same method.\nlen(vectorizer.vocabulary_.keys()),vectorizer.vocabulary_.keys()\rHere\u0026rsquo;s what a single vector looks like\ntrain_vectors.toarray()[0], sum(train_vectors.toarray()[0])\r#sum shows us how many words are 1's - meaning the corresponding document had a total of those many words from sklearn import svm\rfrom sklearn.metrics import classification_report\r# Perform classification with SVM, kernel=linear\rclassifier_linear = svm.SVC(kernel='linear') # a python class that optimizes the model using a Linear line(plane)\r%time classifier_linear.fit(train_vectors, train['sentiment'])\rThe above peice of code may be confusing. Let us break this down.\nSklearn provides a module called svm, which contains a classifier called as SVC (Support Vector Classification) (Remember, there is Support Vector Regression as well).\nsvm.SVC takes a parameter called kernel. It basically means, what is the nature of our model. We use a linear model as of now (as we did in Logistic Regression as well). The possible values, according to the documentaion are: kernel{‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’}, default=’rbf’\nPoly - fits a polynomial function instead of a linear function to the data. (Like we did in the Linear Regression Lab - We tried to fit a cubic function to the data)\nSigmoid tries to fit a sigmoid function to the data.\nRadial Basis function fits circles rather than straight lines. Basically, find some n-dimensional radius, which creates an n-dimensional sphere, which separates the two classes.\nHowever, the circle/sphere is N-Dimensional and not 2/3 dimensional.\nThe last one is Precomputed, meaning, we use a custom function, and give it precomputed outputs, so it doesnt have to do manual computing. We don\u0026rsquo;t use this in our lab!\nApparantly, research shows that this radial function outperforms all types of kernels.\nFor now, we will use the Linear Model.\nNext, we will obtain predictions on the testing dataset, so we can see how many our model gets correctly. So the predict method of the classifier gives us predictions.\nSKlearn also provides a function called classification_report which takes in the predictions along with the ground truth (test['sentiment'] in this case) and gives us a report of the predictions telling us about the performance.\nprediction_linear = classifier_linear.predict(test_vectors)\rreport = classification_report(test['sentiment'], prediction_linear, output_dict=True)\rreport\rSo, report contains many metrics, such as accuracy, precision, recall etc. Precision and Recall are useful metrics when the dataset is imbalanced in terms of categories. But since we verified that our data is well balanced, accuracy will suffice\nreport['accuracy']\rSo we have successfully built a Movie Review Classifier. Congratulations.\nNext, just for the sake of completion, we will try to improve our model. This has nothing to do with SVMs, but rather with the NLP part of the model, meaning, we will try to better represent the data. You can keep this in mind for whenever you build language models - its a wonderful trick to improve information representation of the data.\nInstead of the CountVectors, we use a representation technique called as Term Frequency - Inverse Frequency Document Frequency. If you wish to read the theory behind it, you can find some here.\nThe essence is that, instead of putting 1\u0026rsquo;s in place of words that occur in a document, TF-IDF puts a fraction, which represents how heavily/frequently a word is present in a document. But at the same time, if a word is found in both categories frequently (like, extremely common words like \u0026ldquo;is\u0026rdquo;, \u0026ldquo;the\u0026rdquo;, \u0026ldquo;a\u0026rdquo;), those words do not help us differentiate between the two documents. So TF-IDF makes sure that less weightage is given to words that are very common. But more weightage to words that occur only in one category. So obviously this will help us in easily differentiating between the two categories.\nfrom sklearn.feature_extraction.text import TfidfVectorizer\rvectorizer = TfidfVectorizer(min_df = 5,max_df = 0.8, sublinear_tf = True, use_idf = True)\rThe syntax of this TFIDF vectorizer is simply picked up from the documentations. At this point, we only want to see it work, so its not important to understand the parameters of the function above.\nSo this vectorizer class has the same structure as the vectorizer above, and so the next steps remain the same.\n#convert training and testing data into TF-IDF vectors\rtrain_vectors = vectorizer.fit_transform(train['review'])\rtest_vectors = vectorizer.transform(test['review'])\rclassifier_linear = svm.SVC(kernel='linear')\rclassifier_linear.fit(train_vectors, train['sentiment'])\rprediction_linear = classifier_linear.predict(test_vectors)\rreport = classification_report(test['sentiment'], prediction_linear, output_dict=True)\rreport['accuracy']\rYou can see that the model has increased in accuracy!\nSo thats it! You have your own Movie Review Classifier. Let us actually see it working for ourselves. Below, we\u0026rsquo;ve written two sentences, which we wish to see whether the model thinks of it as a positive or a negative review!\nreview = f'Absolutely loved the movie!'\rreview_vector = vectorizer.transform([review]) # vectorizing\rprint(classifier_linear.predict(review_vector))\rreview = f'What a waste of money! I wish I could get a refund'\rreview_vector = vectorizer.transform([review]) # vectorizing\rprint(classifier_linear.predict(review_vector))\rTry it out for yourself! Enter a sentence and find out what the model thinks of it!\nReview In this lab, we learnt of classification/regression system, that takes the traditional linear/logistic regression models to the next level, by not only finding the best results, but also the best fit model. We build an NLP model using this system, and also learnt about some techniques in NLP to represent textual information in the needed format.\nIn the end, any classification or regression problem can be solved by any suitable model, like the SVM, if the information is correctly represented. This comes from experience, so whats more important right now is to simply keep these concepts in mind. As and when you start coming across these topics more, you will learn the depths too.\nQuestionnaire These are non-evaluative, but highly recommended to go through. Make sure you clearly know the answer to each of these concepts.\n What are Kernels in SVM? What are the different types of kernels in SVM? Describe the two representation techniques we studied about text. SVMs uses a layered class approach - meaning there are multiple steps involved in a model, each of which has a state, contained in the form of a python class. In this example, what was the structuring of our code? What was the heirarchy of classes used?  Whats Next in these Sessions? Upto Now, we\u0026rsquo;ve covered all the basics and fundamentals of Machine Learning. Now its time for us to move to the modern era of Machine Learning. So for the remaining sessions, we\u0026rsquo;ll look at progressively modern and progressively important concepts in Machine Learning. We\u0026rsquo;ll be transitioning towards Deep Learning slowly, which is THE field that dominates the present state of Machine Learning. We\u0026rsquo;ll also be learning Decision Trees and Random Forests, and even Reinforcement Learnings, which have their own interesting applications. So you\u0026rsquo;ll definitely enjoy it.\nA gentle suggestion from our end. Please do attempt these sessions, even if you feel intimidated to attempt the exercises. As you might have realized, most of these sessions are fun exercises. Many exercises are simply meant for you to enjoy and explore. There\u0026rsquo;s no wrong answers, and you are almost never evaluated based on how well your model performs.\nYou also don\u0026rsquo;t need to know a lot of theory to attempt these sessions. So if you feel you\u0026rsquo;re not good at Machine Learning theory, you can still attempt the sessions, since we never focus on theory, but only practical techniques in these sessions. So, if you wish to be an effective Data practitioner, these sessions are guaranteed to help you.\nIf you feel you don\u0026rsquo;t know enough coding to attempt these sessions, we always encourage Google Searches. This is not an exam, and these sessions are meant to reflect what you will come across in the real world. You will almost always have internet, so why not learn coding that way? So, feel free to search your queries up. As we mentioned before in one of the sessions, the ability to know what and how to search for how to code is far more important than knowing coding itself.\nYou don\u0026rsquo;t even need to build advanced code structures. We mostly provide you with code structures and directions. So through this, you not only feel less burdened, but also learn how to approach problems through code.\nIf not anything, there is no harm in running a notebook once. Who knows, you may come across a very interesting problem, that you had always wished to know about, and now you would finally be able to build it yourself. We\u0026rsquo;re gonna do a lot of wonderful applications in the future. (Far more interesting than the ones we\u0026rsquo;ve already done). \u0026ndash;\u0026gt;\nExercise In this exercise, we\u0026rsquo;ll try to apply SVM Classification to a different data type - something that we are already aware with - Visual Data. To be more precise, we\u0026rsquo;ll be working with Images, and we\u0026rsquo;ll be trying to perform binary classification.\nWe\u0026rsquo;ll be trying to identify the emotions of people by looking at their pictures. Humans are pretty efficient in doing so. We now build a machine model that can do so!\nLets us download the dataset\n%cd\r!kaggle datasets download -d msambare/fer2013\r!mkdir facial_expressions\r!unzip fer2013.zip -d facial_expressions\r% cd facial_expressions\r!ls\rIn our current folder, there are two folders - train and test, each containing jpg images of 48x48 pixels. We\u0026rsquo;ll be accessing each image through their paths, for which we\u0026rsquo;ll use the Path module of the pathlib library, which is an extremely efficient library to handle paths. And since we\u0026rsquo;ll be using the Pillow Library, which we have used in previous labs as well. It contains the Image module, which can handle standard Image formats, including jpg images.\nfrom pathlib import Path\rfrom PIL import Image\rLet us visualize one of the images for you to understand what the data looks like. This Image is from a surprised person!\nImage.open(Path('train/surprise/Training_99984132.jpg'))\rYour job is to build a classifier that differentiates between happy and sad people. You need to choose 1000 pictures of happy people and 1000 pictures of sad people for the training set, and 400 pictures each for the testing dataset. We\u0026rsquo;ve provided a function that provides the Paths of these images. This is an important steps, because you will need to access each image separately, and convert it into a format that is understandable by computers - in this cases, 2D arrays (aka matrices).\nTo walk through the entire folder, we will use the os folder, which lets us go through each directory and find what we\u0026rsquo;re looking for.\nimport os\rdef return_paths(path=Path('.'),categories=['happy','sad'],items=1000):\rf\u0026quot;\u0026quot;\u0026quot;\rThis function returns the paths of images in `categories` that lie under the `path` directory, along with their labels. Return `items` number of items per category (or as many are available). Paths returned are pathlib Path objects.\r\u0026quot;\u0026quot;\u0026quot;\rimg_paths=[]\rimg_labels=[]\rfor r,_,_ in os.walk(path):\rr=Path(r)\rif (r.stem) in categories:\ritems_ = items if len(os.listdir(r))\u0026gt;items else len(os.listdir(r))\rfor img in os.listdir(r)[:items_]:\rimg_paths.append(r/img)\rimg_labels.append(r.stem)\rreturn img_paths,img_labels\rtrain_paths,train_labels=return_paths(Path('train'),items=1000)\rvalid_paths,valid_labels=return_paths(Path('test'),items=400)\rtrain_paths\rYou need to go through all these paths, and first access them using the Image library, then convert them into Numpy arrays.\nFor each image, you will get a Numpy 2-D Array of shape 48x48. You can simply convert it to a 1-D array of 48 times 48 = 2304 elements, by simply.\nSo each image will be represented as a 2304 element array. There are the a total of 2000 Images in the training set, so the Input to the SVM model will be a numpy 2-D array of shape 2000x2304. The target variables will be a Numpy 1-D array of shape 2000.\nYou need to iterate through all paths, and open them using Image.open and convert them into numpy arrays. Then the Numpy Array (48x48 pixels in shape) needs to be converted to a 1-D array by using the array.flatten() method.\nThen you\u0026rsquo;ll have a list of 2000 1-D numpy arrays. You need to Google Search to find how to convert a list of 1-D arrays to a single 2-D array.\nYou also need to convert the list of labels to a single 1-D array, by googling how to convert a list of elements into a numpy arrays. Do this for the testing dataset as well.\nAt this point you will have the data ready - the training set 2-D Numpy array of shape 2000x2304 will be similar to train_vectors in the text model example above, and similarly you will have test_vectors as well.\nInstead of train['sentiment'] and test['sentiment'] in the language model above, you can directly use the Numpy arrays containing labels.\nBuild the SVM Model and find the accuracy over the testing dataset. Use the Radial Basis function kernel (\u0026lsquo;rbf\u0026rsquo;).\nimport numpy as np\r\r"});index.add({'id':7,'href':'/docs/07-decision-trees-and-random-forests/','title':"07 Decision Trees and Random Forests",'section':"Docs",'content':"Decision Trees and Random Forests Welcome to the 2nd half of this course, and the 7th practical session in Machine Learning. This session is on Decision Trees and Random Forests.\nTill now we\u0026rsquo;ve learnt about classical Machine Learning techniques. Now we\u0026rsquo;re going to shift towards the modern era of Machine Learning, that lead to the present state of Artificial Intelligence. The state of Artificial Intelligence has become quite sophisticated.\nAt present, methods like Deep Learning and Reinforcement Learning are state of the art, and will remain so in the near future. These methods have only emerged recently.\nIn early 2000s, Random Forests was introduced, and it started becoming a popular method for Regression and Classification in Machine Learning. Part of the reason for its popularity was because of its ability to handle complexity well, and the other part of its popularity was its simplicity and intuitiveness.\nInfact, even after the emergence of sophisticated methods like Deep Learning, Random Forests are still very significant. Deep Learning can solve a lot of complex problems, but is not very effective in many domains. For example, Deep Learning does not perform really well wherever traditional multi-layer perceptron type networks are used (it has reached to good levels of performance, buts its not sophisticated yet). For example, tabular data prediction is solved using traditional types of Neural Networks, and they do not perform very well as of now. But, Random Forests perform very well in the case of tabular data. Random Forests are infact more popular and effective than Neural Networks presently. (Though in 2019, some research showed that good feature engineering can help Neural Networks perform better, but still, Random Forests still outperform Neural Networks). Having said that, it is an important and really interesting topic to learn. We\u0026rsquo;ve tried to solve many problems using tabular data, so today we\u0026rsquo;ll be learning of the State of the Art (for real) modeling technique in Machine Learning to predict on tabular data.\nRandom Forests are an extension of Decision Trees. So in this session, we\u0026rsquo;ll first learn about Decision Trees, and then about Random Forests.\nDecision Trees Decision Trees are a very intuitive way to process information. They are not specific to Machine Learning, and actually come from psychology. Decision Trees were initially used to create logic models. Lets understand the beauty of Decision Trees with an example.\nFigure Credits\nWhen one applies for a loan, the bank needs takes into consideration many factors. And there can be many \u0026ldquo;chains\u0026rdquo; of queries(or questions, or that can determine the final outcome (whether or not a loan should be given out to the applicant). For example, the above figure shows the following chain of decisions to be made while considering a loan application.\n  Do they have a credit history? Meaning, have they taken a loan before?\n  If yes, do they have a pledge? (Meaning an asset that is on another loan or mortgage). If no, then it doesn\u0026rsquo;t matter whether they have a pledge or not. The important question now is, how much debt they have. If its less than some amount, then its a yes for the loan. If the amount is greater than a value, then its a no.\n  and so on\u0026hellip;\nYou can think of many examples in real life where you think through such a structured chain of thought.\nDecision Trees come with an idea of features ranked by importance. For example, in the above example, whether or not an applicant has a pledged asset is not necessarily a deciding factor, but whether or not a person has a credit history is!\nSo, how do Decision Trees work?\nDecision Trees ask a series of binary questions (yes/no questions) to arrive to a conclusion. That is the entire model! Now once you have this model, you can simply traverse through the tree to find the final answer.\nA decision Tree starts with one question, that splits into a yes, and a no. Each one of those too split into further yes and no\u0026rsquo;s. Each question is called a node.\nThe first question that is asked (or the first node. In this case, the question about whether or not an applicant has a credit history) is the most important question. Its called the root of the tree. The root branches out into children nodes. Each child further branches out, until we reach the final node in any branch. This node is called the leaf node.\nHow do we decide which feature is more important and which node is less important? And how do we decide the correct sequence of decisions that need to be taken? We\u0026rsquo;ll briefly describe the method.\nBefore that, let us pick an interesting problem.\nCase Study: Tic Tac Toe You must have played a game of tic-tac-toe. Two players mark either a X or an O on a grid of 3x3 cells. If anyone manages to mark a complete row, column or diagonal, they win the game. We can visually look at the board and understand the meaning of a row, cell or column. But how would you hardcode the logic to a tic tac toe game app that you can find on the internet?\nOne of the ways is to build a tree, that goes as follows.\n Mark all the cells as 1 through 9. If a person marks cell, does the adjacent cell have the same mark? If yes, then is does the same mark exist in the next cell in the same direction? If yes, the player wins the game.  This dataset contains all the possible configurations of the tic-tac-toe grid that result in one of the players winning the game. Each cell has three possible values - X, O or blank (b). Configurations that lead player \u0026lsquo;X\u0026rsquo; to win are labeled as positive, and the ones that lead player \u0026lsquo;O\u0026rsquo; to win, are labeled as negative.\nOur job is to build a classifier that can take in a board configuration and come to a conclusion about which player won the game.\nLet us download this dataset. Since we are downloading this dataset from Kaggle, we need to first upload the kaggle.json file, that you may have downloaded earlier.\n%cd from google.colab import files uploaded = files.upload() !mkdir -p ~/.kaggle/ \u0026amp;\u0026amp; mv kaggle.json ~/.kaggle/ \u0026amp;\u0026amp; chmod 600 ~/.kaggle/kaggle.json !kaggle datasets download -d aungpyaeap/tictactoe-endgame-dataset-uci !mkdir -p tictactoe \u0026amp;\u0026amp; unzip tictactoe-endgame-dataset-uci.zip -d tictactoe %cd tictactoe !ls We have a csv file containing data. So we\u0026rsquo;ll use the pandas library to read and process this library.\nimport pandas as pd df=pd.read_csv('tic-tac-toe-endgame.csv') df.head() If the data is not intuitive to you, let us visualize the first row in a way that we are more used to seeing.\nimport numpy as np config_=df.iloc[0] config=config_[:-1] config=[o if o!='b' else '' for o in config] print(f\u0026quot;winner: player {'X' if config_[-1]=='positive' else 'O'}\u0026quot;) pd.DataFrame(data=np.reshape(config,(3,3))) How many rows does this dataset contain in total?\nlen(df) And are there any rows which contain incomplete information in this dataset?\ndf.isna().any().any() No! There are no empty features in this dataset. We\u0026rsquo;ve now setup our data. The next step is to feed the data into the model. But before that, we need to preprocess the data. Meaning, we have to first convert each feature into numbers, because computers cannot understand anything other than numbers. So we need to categorize all variables into numbers.\nWe defined a function in Session 4 (On Bayesian Learning) called categorify, which categorizes each categorical variable, and also adds a method to the dataframe, called the dictionary, which keeps track of which categorical numbers refer to which original category. We\u0026rsquo;ll categorify our dataframe below.\ndef categorify(df): df.dictionary={} for col in df: numerical_categories = {k:i for i,k in enumerate(sorted(df[col].unique()))} df[col]=[numerical_categories[i] for i in df[col]] df.dictionary[col]=numerical_categories categorify(df) df.head() You can see that all feaures in the dataframe are now numbers. Lets see which features refer to which\ndf.dictionary Finally, lets split this data into the input and output , or x and y.\nx=df.drop(['V10'],axis=1) y=df['V10'] len(x),len(y) Finally lets build the model that we will use to build the classifier.\nWe won\u0026rsquo;t build a Decision Tree from scratch, but use an already existing implementation that is provided by the scikitlearn library.\nfrom sklearn.tree import DecisionTreeClassifier According to the documentation here\u0026rsquo;s the syntax to build a classifier.\nmodel = DecisionTreeClassifier(criterion = 'entropy', max_leaf_nodes=5) To understand how this model works, Let\u0026rsquo;s explain how Decision Trees are built!\nAs we mentioned before, Decision Trees are built upon the principle of ranking different features. For example, in the bank loan example, whether a person had a credit history is the most important feature.\nHere are the steps to build a Decision Tree.\n Go through each feature of the dataset one by one. For each possible value in the feature column, classify all relevant datapoints using that value as the threshold. Whichever threshold (among all values of all features) gives the best result becomes the criteria of classification at that node, and the corresponding feature becomes the node of the tree. Continue till all datapoints till you reach a satisfactory result, or reach a maximum number of iterations.  For example, while going through all features, when the model comes across the feature \u0026ldquo;credit history\u0026rdquo;, and looks for all possible values. The possible values are 0 or 1. So the decision is, whether the value of credit history is 0 or 1. The ones for which credit history is 1, are classified as \u0026ldquo;positive\u0026rdquo; (for whether a loan should be given or not) \u0026ldquo;negative for the other\u0026rdquo;. So there are, say, a number of datapoints in the branch where credit history is 1, and b datapoints in 0. All of the \u0026lsquo;a\u0026rsquo; datapoints are predicted to be positive by this model at this point. And all of \u0026lsquo;b\u0026rsquo; datapoints are predicted as negative. Note, a+b is the total number of datapoints. And the reason that \u0026ldquo;credit history\u0026rdquo; was chosen as the node among all features, because it could classify the points the best among all features. Not perfect, but still the closest.\nNow, For the a datapoints, we go through all features again (excluding credit history, because we\u0026rsquo;ve already covered it for this particular branch.) and choose the feature that further gives the best accuracy while doing a split. And we do this until we reach a point where either all the datapoints have been correctly classified, or we\u0026rsquo;ve reached a limit on the depth or levels of the tree.\nIn the syntax of the line model = DecisionTreeClassifier(criterion = 'entropy', max_leaf_nodes=5)\ncriterion is the metric which helps decide which feature is better than the other. Its similar to a loss function we\u0026rsquo;ve seen before. We\u0026rsquo;ve studied about Entropy Loss in Logistic Regression. Here we will use the \u0026lsquo;entropy\u0026rsquo; criterion.\nWe will also put a limit to the maximum number of leaf nodes there can be, so that the model does not try to fit every single datapoint correctly. That would be similar to overfitting. We don\u0026rsquo;t want that. So we\u0026rsquo;ll stop the model to go all the way to the end.\nLet us now train (aka fit) the model. According to the documentation, here\u0026rsquo;s the syntax to do so.\nmodel.fit(x, y); And we have built a Decision Tree. We would like to visualize this tree. We will do so by using the tree module in sklearn. Matplotlib is used to display the figure in Colab.\nimport matplotlib.pyplot as plt from sklearn import tree plt.figure(figsize=(6,8)) tree.plot_tree(model,filled=True) plt.show() For each node, the branch on the left corresponds to the \u0026lsquo;Yes\u0026rsquo; case, and the right corresponds to \u0026lsquo;No\u0026rsquo;. At each node, entropy of the decision tree is mentioned (basically, how well the tree is able to predict on the data , the lower - the better). You can also see that as we go down a tree, the entropy reduces. That is exactly what we want.\nSamples tells us about the number of datapoints that are classified as positive or negative by the node above it. You can verify that at any node, the number of datapoints on its left and right child nodes (ie the datapoints classified as positive or negative) add up to the number of datapoints in that node.\nAnd finally, we would like to see how good the model performs on the dataset. We will do this by finding the accuracy of predictions. We haven\u0026rsquo;t used a validation/testing dataset for this problem. We\u0026rsquo;ll simply examine the trend of model performance on the training dataset itself.\nfrom sklearn.metrics import accuracy_score y_pred=model.predict(x) accuracy_score(y_pred,y) Ideally if you put no condition on the depth of the tree, the model will keep splitting the data until every datapoint is correctly classified. But that probably won\u0026rsquo;t work well on real world data, because of overfitting. But never the less, lets try it out.\nmodel = DecisionTreeClassifier(criterion = 'entropy') model.fit(x, y); y_pred=model.predict(x) accuracy_score(y_pred,y) plt.figure(figsize=(6,8)) tree.plot_tree(model,filled=True) plt.show() This is quite a deep and complex decision tree! So, if you slowly increase the number of max_leaf_nodes in the model, your accuracy will increase. Initially we set it as 5, now lets set it as 20\nmodel = DecisionTreeClassifier(criterion = 'entropy', max_leaf_nodes=20) model.fit(x, y); y_pred=model.predict(x) accuracy_score(y_pred,y) plt.figure(figsize=(8,8)) tree.plot_tree(model,filled=True) plt.show() You can see that the accuracy has increased!\nSo that is how Decision Trees are built in python!\nNow let us understand what Random Forests are\u0026hellip;\nRandom Forests In 1994, Leo Breiman, a recently retired UC Berkley professor of statistics introduced Bagging, which is a statistics technique used to improve the performance of models (It is actually used by almost all corporates in any Machine Learning application). The idea is to not rely on a single Machine Learning model to get predictions, because there might be errors in the model predictions.\nHowever, if you aggregate the results of multiple Machine Learning model (using a democracy rule), you are likely to get stronger predictions. For example, a particular model might predict an input wrongly, but it is quite less likely for 10 models to predict wrong on the same input.\nBreiman proposed to do this by building multiple models, each with a random subset of the data. Because of the randomness, each model is independent of the other. The final result of the model is the aggregate of the predictions of all models. For example, for calssification, this aggregation may be done by a majority-voting method.\nIn 2001, Breiman demonstrated this method for Decision Trees, and that came to be called Random Forests.\n(Forests because forests have trees, and Random because of the randomness in the bagging technique.)\nSklearn provides us with a class that build a Random Forest Classifier for us. We will use this..\nfrom sklearn.ensemble import RandomForestClassifier We again set a limit on the maximum number of leaf nodes in any decision tree.\nn_estimators is the number of decision trees in a Random Forest. Its default value is 100, meaning a 100 DTs will be built, and the final result will be an aggregate of all these 100 decision trees.\nThere\u0026rsquo;s also an attribute called as max_samples in the RandomForest function, which tells the model, what fraction of the entier dataset must be chosen to build any of the 100 decision trees.\nmodel=RandomForestClassifier(criterion='entropy',max_leaf_nodes=5,n_estimators=100,max_samples=0.5) model.fit(x,y); y_pred=model.predict(x) accuracy_score(y_pred,y) You can see that the performance of a RandomForest for the same limit of max_leaf_nodes is better than its Decision Tree counterpart.\nTry playing around with the parameters of the model function and see the effect on the performance.\nExcercise (Evaluative) Your task is to build both Decision Trees and Random Forests for another task. The task we will use in this question is the Mushroom Classification Dataset, which gives information about the mushrooms appearance, habitat, etc. We need to identify which mushroom is poisenous, and which is edible (given in the class column of the dataframe)\n%cd !kaggle datasets download -d uciml/mushroom-classification !mkdir mushroom \u0026amp;\u0026amp; unzip mushroom-classification.zip -d mushroom %cd mushroom !ls df=pd.read_csv('mushrooms.csv') df Some columns have ?, which is equivalent to NaNs. We need to remove these blank values.\n(df=='?').any() \u0026lsquo;stalk-root\u0026rsquo; contains such ?\u0026lsquo;s. So we\u0026rsquo;ll drop the entire column for the sake of this problem\ndf=df.drop('stalk-root',axis=1) Exercise  Build a Decision Tree for this dataset. Run this decision tree for max_leaf_nodes varying from 2 to 20. Store the accuracy of the model for each of these decision trees in a list. Similarly build a Random Forest Model for max_leaf_nodes varying from 2 to 20. Store the accuracies in another list Plot the accuracies of the two models in a single graph, using matplotlib. Also plot another graph showing the effect of changing the n_estimator parameter of the Random FOrest constructor from 1 to 100, keeping max_leaf_nodes as 5.  "});index.add({'id':8,'href':'/docs/08-neural-networks-part-1/','title':"08 Neural Networks Part 1",'section':"Docs",'content':"Session 08 - Neural Networks: Part 1  Before Starting, we will change the accelerator of this notebook from a CPU to a GPU, since we\u0026rsquo;ll be running our Neural Networks on the GPU.\n  Go to Runtime \u0026ndash;\u0026gt; Change Runtime Type \u0026ndash;\u0026gt; GPU\n Welcome to the 8th practical session on Machine Learning. Today we\u0026rsquo;re going to start learning about the most important and widely applied avenue of Machine Learning - Deep Learning. Deep Learning is nothing but a fancy word for Machine Learning applications with Neural Networks.\nBut why does Deep Learning get to have a whole phrase dedicated to it? That\u0026rsquo;s because Deep Learning is so flexible and efficient, that it can solve almost any problem in the modern age. Neural Networks have reached a point, where they can process almost any kind of data - videos, images, audio, text, tabular data, etc. And in a lot of applications, Neural Networks have been claimed to perform better than human performance. (This notebook provides a comparison of a lot of models on various benchmark problems along with human performance. You would get a general idea about the current state of Deep Learning).\nThat\u0026rsquo;s crazy if you think about it, since Artificial Intelligence literally aspires to be as good as human intelligence. We consider human intelligence to be the most superior, and every form of intelligence, including animal intelligence, and Artificial Intelligence is compared to human intelligence. So, you would now understand why there is so much hype about Deep Learning. Almost all research (including corporate research too) in Machine Learning is about Deep Learning now.\n What are Neural Networks? You may have seen a visual representation of Neural Networks before, something like the picture below.\nThis is nothing but an augmented version of the Linear Model we learned about in the Linear/Logistic Regression sessions.\nIn the Linear Regression Model, we fed an input tensor (with different features $x_1$,$x_2$,\u0026hellip;$x_n$ into the model $f(x) = w_1.x_1 + w_2.x_2 + w_3.x_3 + \u0026hellip;.. + b$. (If you don\u0026rsquo;t remember this, head back to the Linear Regression and Logisitic Regression sessions, and come back once you\u0026rsquo;re clear with that concept!)\nNeural Networks are nothing but multiple Linear functions stacked on top of each other.\nSo the output of linear model is fed to the next, and this is done multiple times. Finally we get an output just like we did in Linear/Logistic Regression models.\nBut wait, why do we even need to stack multiple Linear models on top of each other?\nSo, if you remember the Linear Regression session, you would know that the \u0026ldquo;model\u0026rdquo; is nothing but some mathematical function in the space of the feature vectors ($x_1$, $x_2$ and so on). Our job is to find that mathematical function that best solves a task (like a Cat vs Dog classification problem).\nBut most of the times, the model is not necessarily as simple as a single Linear Function. Neural Networks is a series of linear functions, that apparantly, can represent any complexity of mathematical functions in space. The more the number of layers, or the number of linear stacks (in other words, the deeper the network) (Hence the name Deep Learning), the more complexity the model can handle. Isn\u0026rsquo;t that an interesting approach?\nWhile Deep Learning may have gained popularity in the past decade only, Neural Networks are definitely not a new concept. They existed as back as in 1940s. There is a wonderful article by Stanford University on the history of Neural Nets. Check it out if you want to know more!\nThis is how a Neural Networks work. Remember? in the Linear/Logistic Regression Model we fed an input, and passed it through the linear function, and based on the output, made our predictions! Here too, we feed the input through multiple linear functions, and based on the final output, make our predictions. We will look at this in more detail below!\nIn this session, we will learn how to build our own Neural Networks with PyTorch. We\u0026rsquo;ll be learning how to use PyTorch\u0026rsquo;s inbuilt functionality to vreate Neural Networks with minimal effort. Then we\u0026rsquo;ll slowly breakdown each component and learn how to write them in python from scratch. That being said, this session has some really good lessons on python development, especially Object Oriented Programming concepts, which will help you develop better Machine Learning code for the rest of your life. Most Deep Learning practitioners are not good software developers, and so it is a valuable skill to master in order to develop efficient tools and techniques.\nBuilding a Neural Network with PyTorch Let us pick a problem and solve it using Neural Networks. For the purpose of this session, we\u0026rsquo;ll be using the Fruits Dataset, which contains images of 44 fruits and vegetables, which we wish to classify. For simplicity, we\u0026rsquo;ll only be classifying bwtween any 2 categories for now.\nLets us dowload this data from kaggle. First of all, upload your kaggle.json file.\n%cd from google.colab import files uploaded = files.upload() for fn in uploaded.keys(): print('User uploaded file \u0026quot;{name}\u0026quot; with length {length} bytes'.format( name=fn, length=len(uploaded[fn]))) !mkdir -p ~/.kaggle/ \u0026amp;\u0026amp; mv kaggle.json ~/.kaggle/ \u0026amp;\u0026amp; chmod 600 ~/.kaggle/kaggle.json !kaggle datasets download -d moltean/fruits !unzip fruits.zip \u0026gt;./tmp %cd fruits-360 !ls The following are the classes that we are dealing with.\n!cd Training \u0026amp;\u0026amp; ls This dataset contains 100x100 pixel images for any category. So, for example, let us look at two categories - Pears and Cauliflowers. Just fr visualization purposes, we\u0026rsquo;ll be using the PIL library\nfrom PIL import Image im=Image.open('Training/Cauliflower/236_100.jpg') print(im.size) im im=Image.open('Training/Pear/236_100.jpg') print(im.size) im Converting the data into PyTorch tensors As you may remember, PyTorch processes all data in the form of tensors, which are nothing but N-dimensional arrays. Our first job is to find a way to convert images to tensors and vice versa (just in case we wish to visualize tensors as images).\nA quick google search tells us that the torchvision module of the PyTorch library functions contains some classes that can do this for us. Let us create objects of these classes, namely img2tensor and tensor2img. The names are pretty self explanatory!\n Lessons in Python:\n  Class\n  A class is a collection of different things, all under one roof. These things can be anything - data or functions (these are the two broad categories that everything in programming can be put into, if you think about it). The data can be anything like - numbers, strings, objects of other classes *we\u0026rsquo;l look at objects next!), and functions are the peices of code that operate on this data, to provide us users with some functionality. (These functions within a class are also called as methods).\n  So, if you remember building models with SKLearn, we always used classes to build models. We first named our class something, and first gave it some data (usually our x\u0026rsquo;s and y\u0026rsquo;s) and then used the .fit() method (or function) to train the model!\n  Objects are nothing but the real world implementations of classes. Classes are only templates about how data and functions (attributes and methods, to be more precise) can exist together and function together to provide usefulness to the user! An object is like the physical version of a class, that Actually does tasks and stores data physically.\n  If you\u0026rsquo;re not aware of the concept of classes and objects at all, it would be useful to spend some time learning about them. Here is a video to help you understand the intuition behind them. The video teaches this concept in JAVA, which is a really old (classic) but popular language to learn object oriented programming (which is nothing but the style of programming, which contains lots and lots of classes and objects).\n  One we dive a little deeper into classes in python, we\u0026rsquo;ll provide with more resources to learn about the more technical details.\n Python is a fully object oreiented programming language, meaning everything in python is an object of some class. Turns out, that objects are essential in data sciences, because of the flexibility they provide in bundling different data and functionality together, which is also one of the reasons why python is so famous for data sciences.\nLet us start with creating img2tensor and tensor2img, which are objects of the classes ToTensor and ToPILImage.\nfrom torchvision.transforms import ToTensor,ToPILImage img2tensor = ToTensor() tensor2img=ToPILImage() Let us see these classes in action!\nimg2tensor(im) #gives us a tensor img2tensor(im).shape img2tensor takes in a PIL image as input, and gives a tensor as an output.\nNotice how img2tensor is an object of a class, but is used like function. This is a special feature of python. We have the liberty to define the functionality of an object, when it is used like a function. (like output = object_name(input) We will learn about this in more detail below.\nBut first let us move ahead. Next, we need to go to the location where all images are, and convert each image to a tensor so that we can get the final x and y for our model!. To handle paths, we use the pathlib library. And to navigate through directories, we use the os library in python.\nimport os from pathlib import Path Path.ls =lambda x: os.listdir(x) # this funcitonality gives us the contents of a directory, in a LINUX style command (\u0026quot;ls\u0026quot;) Next, we\u0026rsquo;ve written a function that takes in the paths of all the directories where are images are (for example, if our model contains data on cauliflowers and pears, we will pass in the paths where cauliflower and pear images are). And the model returns the x tensors and the corresponding ground truth labels y.\nNow, since we are dealing with tensors, we need the PyTorch library (torch), to handle tensors. So we import that!\nimport torch def get_tensors_from_folder(folder_paths:list=None,normalize_x=True,preprocess_y=True): f\u0026quot;\u0026quot;\u0026quot; Takes in the paths of all the categories that need to be considered, and returns a collective tensor for the input tensors and the target tensors. normalize_x: Default True. If True, the function will automatically Gaussian normalize the input tensor x. preprocess_y: Default True. If True, the function will convert the labels to a series of numbers ranging from 0 to N-1, where N is the number of classes(categories). This is important if your classes are strings, or unordered. Computers cannot understand anything other than numbers. More than that, Neural Networks, as you will see, only understand categories that go as 0,1,2,....N-1 (if there are N categories to be classified in total.) \u0026quot;\u0026quot;\u0026quot; x = None y = [] folder_paths=list(folder_paths) for folder_path in folder_paths: for img_path in folder_path.ls(): x=torch.cat((img2tensor(Image.open(folder_path/img_path))[None],x)) if x is not None else img2tensor(Image.open(folder_path/img_path))[None] # y=torch.cat((torch.Tensor(folder_path.stem),y)) if y is not None else torch.Tensor(folder_path.stem) y.append(folder_path.stem) if normalize_x: x.sub_(x.mean()).div_(x.std()) if preprocess_y: proc={v:k for k,v in enumerate(sorted(set(y)))} y=[proc[v] for v in y] y=torch.Tensor(y).long() return x.reshape(x.shape[0],-1),y So, now that we\u0026rsquo;ve defined the function to retrieve the relevant tensors, we will use it to derive the training x and y tensors and the testing x and y tensors.\npath_cauliflowers = Path('Training/Cauliflower') path_pears=Path('Training/Pear') x_train,y_train=get_tensors_from_folder([path_cauliflowers,path_pears]) x_train.shape, y_train.shape path_cauliflowers = Path('Test/Cauliflower') path_pears = Path('Test/Pear') x_test,y_test = get_tensors_from_folder([path_pears,path_cauliflowers]) x_test.shape,y_test.shape Datasets and Dataloaders Next, we need to learn about 2 important concepts in datasciences - Dataset and Dataloaders.\nDataset: A Dataset is nothing but a class which bundles the x and y. This is important, because its very inefficient to have to specify x and y separately everytime. They always go together. So the dataset class makes our life easier, by simply bundling x and y. You can simply retrieve the individual x and y by calling the attributes dataset.x and dataset.y.\nBelow is a simple implementation of the Dataset class.\nAny dataset class should have three properties\n It should have x and y as its attributes. We should be able to get the length of the dataset (with the __len__ (pronounced dunder len) method) We should be able to get a specific value by index (with the __getitem__ method)  class Dataset(): # to wrap x and y together def __init__(self, x,y):self.x,self.y=x,y # __init__ is pythons way to initiate classes into objects. self.x is basically the process of setting x as an attribute of the object def __len__(self): return self.x.shape[0] def __getitem__(self,i): return self.x[i], self.y[i] #Let us derive the training dataset train_ds=Dataset(x_train,y_train) len(train_ds) # we can use len on train_ds, because of the __len__ method train_ds[0] # __getitem__ #similarly let us derive the Validation Dataset valid_ds=Dataset(x_test,y_test) len(valid_ds) Lessons in Python. You may have used len and list_name[i] very frequently in the case of lists. Even list is a class in python, which ultimately implements the __len__ and __getitem__ methods\nmy_list=[1,2,3,4,5] len(my_list), my_list.__len__() my_list[0], my_list.__getitem__(0) The next concept is dataloaders. A dataloader is a convenient class in python that does 2 things.\n It loads the data into the model (that we are going to build). Sometimes, it is not a wise idea to feed all the data into the model at once, because of memory constraints, so the dataloader can also feed data to the model in batches. The dataloader can also make sure that the data is loaded onto the correct device (CPU or GPU). By default, everything is on the CPU, so you need to explicitly load the data on the GPU, if you wish so. So, dataloaders can save us from this unnecessary effort!   What is a GPU?\n  A GPU stands for Graphical Processing Unit (Just like CPU stands for Central Processing Unit). Its also known as a graphics card. If anyone of you is interested in gaming, you would know what a graphics card is! A GPU is like a CPU, but multitudes faster (sometimes millions of times faster). A GPU can do matrix multiplications and additions very fast, which is exactly what the graphics of any video game are - lots and lots of matrix operations. A CPU is much more capable of handling more complex operations than matrix operations, but a GPU, even though less capable of doing complex operations, is very fast at doing these basic operations. Turns out, that Deep Learning is nothing but a lot of matrix multiplications and additions, which is why we use GPUs for Deep Learning.\n  There\u0026rsquo;s a newer type of device, called the TPU, or the Tensor Processing Unit, which is a GPU, but meant specifically for Tensor Operations. So, a TPU can ideally handle much less complexity than a GPU, but can perform Deep Learning related operations at a much faster rate. They are still not commercially used, because\n  They are still very costly Because the community awareness is still low, there isnt much support for TPUs in deep learning libraries. It will take a few years, before TPUs become mainstream.  PyTorch implements DataLoaders for us. We will use that!\nfrom torch.utils.data import DataLoader train_dl=DataLoader(train_ds,batch_size=64,shuffle=True) valid_dl=DataLoader(valid_ds,batch_size=128,shuffle=True) #batch_size double the batch_size of train_dl since valid_dl doesnt require gradients, so less space will be used, and thus batch_size can be increased. As you can see that, the DataLoader takes in a dataset, along with batch_size and an option to specify whether the data should be returned in random order, or in the same order as given in the dataset.\nThis dataloader now works just like an iterator.\nlen(train_dl) for x,y in train_dl: print(x.shape, end=' ') We can simply feed in one batch of size 64 to the model at once.\nNow that we are set up with our data, let us move onto building our Neural Networks\nCreating our own Neural Network PyTorch provides us with a really easy way to define Neural Networks, by simply stacking the different components (or layers) in a function called Sequential.\nPyTorch provides a module called nn which contains all Neural Network architecture related stuff\nfrom torch import nn def model(in_features,hidden_features,out_features): return nn.Sequential(nn.Linear(in_features,hidden_features), nn.ReLU(), nn.Linear(hidden_features,out_features),nn.Sigmoid()) What does\nnn.Sequential(nn.Linear(in_features,hidden_features), nn.ReLU(), nn.Linear(hidden_features,out_features), nn.Sigmoid())\ndo ?\nFirst of all, what are nn.Linear, nn.ReLU, and nn.Sigmoid?\nnn.Linear is nothing but a linear model class. It is the implementation of the linear model we defined in the Linear/Logistic Regression session. That\u0026rsquo;s the basic building block of a Neural Network.\nnn.ReLU is the implementation of the ReLU Activation function, or the Rectified Linear Unit. We\u0026rsquo;ve seen the sigmoid activation function. The ReLU activation function looks like this.\nAnd is simply defined as\n$$ReLU(z) = max(0,z)$$\nWhat is an activation function anyways?\nActivation Functions As you know, a Neural Network is nothing but a stack different linear functions. But if you analyze a simple linear stack mathematically, as follows\u0026hellip;\n$$y_1=w_1.x + b_1$$ $$y_2 = w_2.y_1 + b_2 $$\n$y_2$ can be simplified as:\n$$y_2 = w_2(w_1.x_1 + b_1) + b_2 $$ $$ y_2 = w_2.w_1.x_1 + (w_2.b_1 + b_2)$$\nwhich is nothing but another linear function\n$$ y_2 = W.x_1 + B $$\nwhich is exactly what $y_1$ could have represented. Then what was the point of stacking multiple linear layers? The ultimate point of stacking linear layers was because a single linear layer could not represent the complexity of functions in space.\nYou can try stacking 3,4, or even more linear layers together, and write them mathematically, then simplify them, and see, that the final result will still be equivalent to one linear layer.\nSo, how do we overcome this problem? This is where Activation Functions come into use. Activation Functions are non-linearities put in between of two linear layers. It turns out that that because of this non-linearity, the resulting model can achieve any level of complexity, with as little as 2 layers. Infact, a research proposed that with just 2 layers of Neural Networks (excluding the output layer), with a non-linearity activation function, can achieve arbitrarily any level of model complexity. Obviously, this is only in theory, and in practice you need more than 2 layers to achieve a level of complexity, which is why many Neural Nets have 100s, maybe even 1000s of layers.\nFor now, we\u0026rsquo;ll be focussing on only 2 layers (the input layer, and the hidden layer). (Hidden Layer is basically any linear layer in between the output and the input layer, both excluded. Refer to the figure in the beginning of this notebook).\nThere are many activation functions. Even Sigmoid is an activation function. Infact, in the earlier days, sigmoid was used in place of ReLU. But now, we only use ReLU as the activation function.\n Finally nn.Sigmoid is the implementation of the sigmoid function. Just like in the logistic regression model, we use sigmoid to scale the output between 0 and 1. This helps us interpret the model output better, in terms of probability.\nnn.Sequential stacks all these components together. When we write\n nn.Sequential(nn.Linear(in_features,hidden_features), nn.ReLU(), nn.Linear(hidden_features,out_features),nn.Sigmoid())\nWhen you feed data into this, the data fill be fed into the first layer of the stack, ie nn.Linear. The output of this Linear layer is fed into ReLU, the output of the ReLU is fed into the second Linear Layer, and finally the output of the Linear layer is fed to the Sigmoid function.\nThe output of the Sigmoid function is our final output function, and we will make predictions based on that\nmy_model = model(x_train.shape[-1],50,2) my_model #prints out all the modules # uses the __repr__ method You can see the different layers in my_model.\nnn.Sequential has some wonderful properties. It provides methods like the .parameters to return the list of learnable parameters in the model. So, if you remember the gradient descent algorithm from the Linear/Logistic Regression session, you would know that we need to update parameters. So nn.Sequential makes it easy for the model to recognize which data are the parameters.\nfor p in my_model.parameters(): print(p.shape) And we can simply use my_model as a function (even though its a class object). As seen below. This comes by implementing the __call__ method, which tells the object to do something, if the object is directly used as a function. So the __call__ method of the nn.Sequential must be implementing the functionality to pass the input through all the layers in the model and returning the output\nxb,yb=next(iter(train_dl)) pred = my_model(xb) ??my_model.__call__ # You can see, that there is some implementation of __call__, in the form of _call_impl . Details dont matter as of now What do the predictions from the model look like? Let us analyze the pred tensor\npred.shape pred is basically a tensor of 2 values for every one of the 64 inputs. These 2 outputs per input is basically the prediction of the probability of each one of the 2 categories (pears and cauliflowers). As we saw earlier, whichever neuron has the highest value, is basically the prediction of the model for that input.\npred Gradient Descent on Neural Networks. Next, we will simply apply the Gradient Descent Algoithm to optimize our Neural Nets. If you don\u0026rsquo;t remember what Gradient Descent is, head back to the Linear Regression and Logistic Regression sessions to understand it, and then come back!\nThe steps are the same.\n Get predictions from the model Calculate loss function Derive Gradients of parameters with respect to the loss function Update the parameters by a specific step size (called as learning rate) Calculate the accuracy on the validation/test dataset to find out how good your model is performing Repeat this process for many iterations (or epochs)  We implemented this from scratch earlier. But now, we will use PyTorch\u0026rsquo;s built in Gradient Descent Algorithm.\nlearning_rate = 0.001 optimizer = torch.optim.SGD(my_model.parameters(), lr=learning_rate) And we will also use the Cross Entropy Loss function inbuilt in PyTorch. If you dont remember what the Cross Entropy loss function does, head back to the Logisitic Regression session!\nloss_fn=torch.nn.CrossEntropyLoss() Next, we will pass the input through the model, and calculate the loss. Passing the data through the model is called a forward pass through the model.\n# FORWARD PASS pred=my_model(xb) # we only pass one batch at a time. Because many a times, the entire dataset is too large for the model to handle at once loss=loss_fn(pred,yb) And we calculate the gradients after this. You don\u0026rsquo;t have to implement the gradient calculations on your own. PyTorch does that for you!\nYou would remember from the Linear/Logistic Regression session that we simply wrote loss.backward to calculate the *gradient of all parameters with respect to the loss`. (Now you understand why nn.Sequential\u0026rsquo;s .parameter method is useful. With this, the model keeps track of where parameters and present, so that gradients can be calculated and they can be updated).\nThe method is called .backward because traditionally, the process of calculating gradients is called a backward pass. Its nothing but an implementation of the chain rule for derivative calculations.\n# BACKWARD PASS loss.backward() So, just to understand the state of the model at this point - We haven\u0026rsquo;t trained the Neural Net at all right now. So let us see how the model is performing on the dataset right now, and we will compare it with the performance after we train the model.\nSo for that, we define an accuracy function, which basically checks how many predictions are the same as the target value (ground truth value).\nThe neural Network has 2 neurons at the output layer, corresponding to the two classes (pears and cauliflowers). Whichever neuron has the highest value, is the prediction for the given input. torch.argmax basically returns the index of the neuron with gives the highest value.\ndef accuracy(pred,targ): return (torch.argmax(pred,dim=1)==targ).float().mean() #understand this line of code. Search on google for the functionality of a term, if you dont understand accuracy(pred,yb) Now, if we update the parameters once, using the optimizer.step method of the optimizer class, let us see how the accuracy increases.\noptimizer.step() optimizer.zero_grad() accuracy(my_model(xb),yb) You can see that the accuracy has increased!\nWhat is optimizer.zero_grad()? We mentioned this in the earlier sessions too, when we were discussing gradient descent, but we didnt describe it back then. Let us understand it now.\nThe idea is simple. By default, PyTorch accumulates (or keeps adding up ) gradients if you keep calculating gradients. This is not a bad thing, infact, its quite helpful when you have very small batch sizes, and you wish to accumulate gradients of multiple batches, (more the data used for parameter update, the better generalisation occurs in the final output). At the end, you have to manually clear the gradients, so that fresh gradients can be calculated. .zero_grad does exactly that. It replaces the gradients with a zero value. So the next time .backward is called, gradients are added to 0, which basically means, that fresh gradients are calculated.\nThis is it. You\u0026rsquo;ve built your own Neural Network!\nRefactoring the entire code to a convenient class The above code is quite scattered and clunky. We will refactor the code to make the entire Neural Network more understandable and easy to work with.\nNow is a good time to understand what __init__ does. Here is a great video by Cory Schafer on YouTube that explains this. Actually, Cory Schafer has one of the best tutorials on object oriented programming in Python. So go check it out if you\u0026rsquo;re interested!\nclass NeuralNetwork(): def __init__(self,train_dl,valid_dl,model,loss_fn=nn.CrossEntropyLoss(),optimizer=None): f\u0026quot;valid_dl can be None. In that case, the model will be evaluated on the training set only\u0026quot; self.train_dl = train_dl self.valid_dl = valid_dl self.model = model self.loss_fn=loss_fn self.optimizer = torch.optim.SGD(self.model.parameters(),lr=0.1) if not optimizer else optimizer def train(self,epochs=1,lr=None,loss_fn=None): self.train_loss,self.valid_loss=[],[] if lr: self.opt.defaults['lr']=learning_rate if loss_fn: self.loss_fn=loss_fn for epoch in range(epochs): # self.model=self.model.train() for x,y in train_dl: loss=self.loss_fn(self.model(x),y) loss.backward() self.optimizer.step() self.optimizer.zero_grad() self.train_loss.append(loss) if self.valid_dl: print(f'epoch {epoch+1}:validation accuracy - {self.get_validation_accuracy()}') else: print(print(f'epoch {epoch+1}: training accuracy - {self.get_validation_accuracy(validation=False)}')) def get_validation_accuracy(self,validation=True): dl=self.valid_dl if validation else self.train_dl with torch.no_grad(): acc=[accuracy(self.model(x),y) for x,y in dl] return torch.Tensor(acc).mean().float() my_model = model(x_train.shape[-1],50,2) # reinitializing the model optimizer=torch.optim.Adam(my_model.parameters(),lr=0.0001) my_network = NeuralNetwork(train_dl,valid_dl,my_model,loss_fn=loss_fn,optimizer=optimizer) my_network.get_validation_accuracy() # checking the model performance before training, for comparison my_network.train(1) You can see that with just one epoch, the model has improved significantly.\nTry writing the above class from scratch, without looking at it. You\u0026rsquo;ll be surprised by how much you learn about python and Neural Nets with just this one class definition. It should take you about 3-4 hours to write it. Peeking at the original function once in a while is okay, but your aim should be to understand everything, and being able to write it from scratch.\nImplementing nn.Sequential from scatch We\u0026rsquo;ve implemented a model using nn.Sequential from scratch. But lets see what goes on under the hood. Let\u0026rsquo;s actually look at the source code of the nn.Sequential function.\nIn Jupyter notebooks, you can look up the source code of any function using the ?? sign.\nSo let us first go to the source code of the model function which we defined above.\n??model As expected, it would show us the definition of the model, as defined above. We used nn.Sequential within the model function.\nSo next, let us look up the source code of the nn.Sequential class.\n??nn.Sequential First of all, notice that nn.Sequential inherits from nn.Module.\nnn.Module is the base class for all functions in the nn (torch.nn). It contains basic information about how nn functions should behave. The __call__ for all functions is implemented in nn.Module .\nclass Sequential(Module): ??nn.Module.__call__ There\u0026rsquo;s a lot of technical code over there, but look for this main line of code.\nresult = self.forward(*input, **kwargs) So, behind the scenes, nn functions call a function called forward. This is nothing but the function corresponding to the forward pass. Now you know why you can do a forward pass by simply calling the object like a function, with input as the argument to the function.\nThis forward function is all you need to define for any nn function. Even nn.Linear, nn.ReLU, nn.Signmoid define only a forward function, and the rest is done by the __call__ method.\nNext, lets take a look at the forward function of the nn.Sequential function\n??nn.Sequential.forward You would see this.\ndef forward(self, input): for module in self: input = module(input) return input what is a module? A module is the basic unit in nn.Sequential function. In our case, nn.Linear, nn.ReLU, nn.Sigmoid, etc are the basic units. Let us run this ourself to understand this better\n#my_model is a nn.Sequential object for module in my_model: print(module) As you can see, module is nothing but the different building layers of our model. So what nn.Sequential.forward is doing, is that it iterates through each layer, and passes the output from the previous layer to the next.\nNow that we know how it works, let a model using this behaviour from scratch!\nclass model(nn.Module): def __init__(self,layers): super().__init__() self.layers=layers for k,v in enumerate(self.layers): self.add_module(f\u0026quot;layer {k}\u0026quot;,v) #nn.Sequential automatically added each layer as a module. We didnt have to do that. #But now we have to do it ourselves. nn.Module has a function called add_module that can add any layer as a module of the class. Now all we need to do is, define a forward function in this model class. But before that, lets see, if the model behaves properly\nin_features=x_train.shape[-1] hidden_features=50 out_features=2 layers=[nn.Linear(in_features,hidden_features), nn.ReLU(), nn.Linear(hidden_features,out_features),nn.Sigmoid()] my_model=model(layers) my_model Yes, it does. You can see, that all the building blocks have been added as modules. Now we can create a forward function.\nLet us actually verify that this model class will use the forward function when called\n# this will show an error at this point because forward is not implemented # my_model(x_train) #Uncomment to run You can see that the error occurs at the line result = self.forward(*input, **kwargs). Meaning, all we need to do is, define a forward function, and nn.Module will take care of the rest.\nclass model(nn.Module): def __init__(self,layers): super().__init__() self.layers=layers for k,v in enumerate(self.layers): self.add_module(f\u0026quot;layer {k}\u0026quot;,v) def forward(self,x): for layer in self.layers: x=layer(x) return x my_model=model(layers) pred=my_model(x_train) pred.shape Yes, the model prediction is working correctly now!\nReview That\u0026rsquo;s it for this session. In the next session, we\u0026rsquo;ll look at more advanced concepts in Neural Networks, and following that, we\u0026rsquo;ll dive into specific applications in Deep Learning, like Computer Vision, NLP and so on.\nIn this session, we learnt how to build our own Neural Networks from scratch. We dived deep into PyTorch\u0026rsquo;s elegant API and even learnt a lot new stuff in python.\nYou are not expected to learn all this stuff in one go. There were too many concepts involved in this session, so its okay to take some time to understand all this stuff. Go through the whole notebook again if you need to, and make sure you understand everything well.\nWe built Neural Networks the practical way - no math involved. Thats because libraries that exist today make sure we don\u0026rsquo;t have to involve ourselves in unnecessary bookish mathematics. Infact, we saw how PyTorch provides us with functionalities to build our own Neural Networks in just one line of code - nn.Sequential(...). Yes, we went much deeper than that. The sole reason behind that was to show you how things work at the foundational level of code.\nThis is important when you want to modify the functionality of a code. If you simply don\u0026rsquo;t know how nn.Sequential works, you cannot make it work in any other way than the default way. However, once you know how things work at the grassroot level, you can make your code work in whatever way you want!\nExercise Your task is to define the nn.Linear, nn.Sigmoid and nn.ReLU from scratch. You only have to implement the forward function in each of these classes. Once you do that, build a nn.Sequential object using not nn.Linear, etc, but the functions you define below. Run the Pear vs Cauliflower Classifier and make sure you get the same results.\nYou can use any python or pytorch inbuilt function if needed, but you cannot directly copy paste the code from the repective forward definitions of nn.Linear, etc.\nimport math class linear(nn.Module): def __init__(self, in_features, out_features): super().__init__() self.in_features = in_features self.out_features = out_features self.weight = nn.Parameter(torch.Tensor(out_features, in_features)) self.bias = nn.Parameter(torch.Tensor(out_features)) self.reset_parameters() #used to kaiming initialize parameters def reset_parameters(self): #implementation of the kaiming initialization. IGNORE THIS nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5)) fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight) bound = 1 / math.sqrt(fan_in) nn.init.uniform_(self.bias, -bound, bound) def forward(self,x): raise NotImplementedError class relu(nn.Module): def __init__(self): super().__init__() def forward(self,x): raise NotImplementedError class sigmoid(nn.Module): def __init__(self): super().__init__() def forward(self,x): raise NotImplementedError in_features=x_train.shape[-1] hidden_features=50 out_features=2 layers=[linear(in_features,hidden_features), relu(), linear(hidden_features,out_features),sigmoid()] my_model=nn.Sequential(*layers) my_model optimizer=torch.optim.Adam(my_model.parameters(),lr=0.0001) mynet=NeuralNetwork(train_dl,valid_dl,my_model,optimizer=optimizer) mynet.get_validation_accuracy() #checking accuracy before the training cycle mynet.train()  "});index.add({'id':9,'href':'/docs/','title':"Docs",'section':"Palaash Agrawal",'content':"redirect\n"});index.add({'id':10,'href':'/shortcodes/buttons/','title':"Buttons",'section':"Shortcodes",'content':"Buttons Buttons are styled links that can lead to local page or external link.\nExample {{\u0026lt; button relref=\u0026#34;/\u0026#34; [class=\u0026#34;...\u0026#34;] \u0026gt;}}Get Home{{\u0026lt; /button \u0026gt;}} {{\u0026lt; button href=\u0026#34;https://github.com/alex-shpak/hugo-book\u0026#34; \u0026gt;}}Contribute{{\u0026lt; /button \u0026gt;}}  Get Home  Contribute  "});index.add({'id':11,'href':'/shortcodes/columns/','title':"Columns",'section':"Shortcodes",'content':"Columns Columns help organize shorter pieces of content horizontally for readability.\n{{\u0026lt; columns \u0026gt;}} \u0026lt;!-- begin columns block --\u0026gt; # Left Content Lorem markdownum insigne... \u0026lt;---\u0026gt; \u0026lt;!-- magic separator, between columns --\u0026gt; # Mid Content Lorem markdownum insigne... \u0026lt;---\u0026gt; \u0026lt;!-- magic separator, between columns --\u0026gt; # Right Content Lorem markdownum insigne... {{\u0026lt; /columns \u0026gt;}} Example Left Content Lorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter! Oculis non ritibus Telethusa protulit, sed sed aere valvis inhaesuro Pallas animam: qui quid, ignes. Miseratus fonte Ditis conubia.  Mid Content Lorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter!  Right Content Lorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter! Oculis non ritibus Telethusa protulit, sed sed aere valvis inhaesuro Pallas animam: qui quid, ignes. Miseratus fonte Ditis conubia.   "});index.add({'id':12,'href':'/shortcodes/details/','title':"Details",'section':"Shortcodes",'content':"Details Details shortcode is a helper for details html5 element. It is going to replace expand shortcode.\nExample {{\u0026lt; details \u0026#34;Title\u0026#34; [open] \u0026gt;}} ## Markdown content Lorem markdownum insigne... {{\u0026lt; /details \u0026gt;}} {{\u0026lt; details title=\u0026#34;Title\u0026#34; open=true \u0026gt;}} ## Markdown content Lorem markdownum insigne... {{\u0026lt; /details \u0026gt;}} Title Markdown content Lorem markdownum insigne\u0026hellip;   "});index.add({'id':13,'href':'/shortcodes/expand/','title':"Expand",'section':"Shortcodes",'content':"Expand Expand shortcode can help to decrease clutter on screen by hiding part of text. Expand content by clicking on it.\nExample Default {{\u0026lt; expand \u0026gt;}} ## Markdown content Lorem markdownum insigne... {{\u0026lt; /expand \u0026gt;}}   Expand ↕  Markdown content Lorem markdownum insigne\u0026hellip;    With Custom Label {{\u0026lt; expand \u0026#34;Custom Label\u0026#34; \u0026#34;...\u0026#34; \u0026gt;}} ## Markdown content Lorem markdownum insigne... {{\u0026lt; /expand \u0026gt;}}   Custom Label ...  Markdown content Lorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter! Oculis non ritibus Telethusa protulit, sed sed aere valvis inhaesuro Pallas animam: qui quid, ignes. Miseratus fonte Ditis conubia.    "});index.add({'id':14,'href':'/shortcodes/hints/','title':"Hints",'section':"Shortcodes",'content':"Hints Hint shortcode can be used as hint/alerts/notification block.\nThere are 3 colors to choose: info, warning and danger.\n{{\u0026lt; hint [info|warning|danger] \u0026gt;}} **Markdown content** Lorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter! Oculis non ritibus Telethusa {{\u0026lt; /hint \u0026gt;}} Example Markdown content\nLorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter! Oculis non ritibus Telethusa  Markdown content\nLorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter! Oculis non ritibus Telethusa  Markdown content\nLorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter! Oculis non ritibus Telethusa  "});index.add({'id':15,'href':'/shortcodes/katex/','title':"Katex",'section':"Shortcodes",'content':"KaTeX KaTeX shortcode let you render math typesetting in markdown document. See KaTeX\nExample {{\u0026lt; katex [display] [class=\u0026#34;text-center\u0026#34;] \u0026gt;}} f(x) = \\int_{-\\infty}^\\infty\\hat f(\\xi)\\,e^{2 \\pi i \\xi x}\\,d\\xi {{\u0026lt; /katex \u0026gt;}}     Display Mode Example Here is some inline example:  \\(\\pi(x)\\)  , rendered in the same line. And below is display example, having display: block  \\[f(x) = \\int_{-\\infty}^\\infty\\hat f(\\xi)\\,e^{2 \\pi i \\xi x}\\,d\\xi\\]  Text continues here.\n"});index.add({'id':16,'href':'/shortcodes/mermaid/','title':"Mermaid",'section':"Shortcodes",'content':"Mermaid Chart Mermaid is library for generating svg charts and diagrams from text.\nExample {{\u0026lt; mermaid [class=\u0026#34;text-center\u0026#34;]\u0026gt;}} sequenceDiagram Alice-\u0026gt;\u0026gt;Bob: Hello Bob, how are you? alt is sick Bob-\u0026gt;\u0026gt;Alice: Not so good :( else is well Bob-\u0026gt;\u0026gt;Alice: Feeling fresh like a daisy end opt Extra response Bob-\u0026gt;\u0026gt;Alice: Thanks for asking end {{\u0026lt; /mermaid \u0026gt;}}     "});index.add({'id':17,'href':'/shortcodes/section/','title':"Section",'section':"Shortcodes",'content':"Section Section renders pages in section as definition list, using title and description.\nExample {{\u0026lt; section \u0026gt;}}   Page1   Page 1   Page2   Page 2   "});index.add({'id':18,'href':'/shortcodes/section/page1/','title':"Page1",'section':"Section",'content':"Page 1 "});index.add({'id':19,'href':'/shortcodes/section/page2/','title':"Page2",'section':"Section",'content':"Page 2 "});index.add({'id':20,'href':'/shortcodes/tabs/','title':"Tabs",'section':"Shortcodes",'content':"Tabs Tabs let you organize content by context, for example installation instructions for each supported platform.\n{{\u0026lt; tabs \u0026#34;uniqueid\u0026#34; \u0026gt;}} {{\u0026lt; tab \u0026#34;MacOS\u0026#34; \u0026gt;}} # MacOS Content {{\u0026lt; /tab \u0026gt;}} {{\u0026lt; tab \u0026#34;Linux\u0026#34; \u0026gt;}} # Linux Content {{\u0026lt; /tab \u0026gt;}} {{\u0026lt; tab \u0026#34;Windows\u0026#34; \u0026gt;}} # Windows Content {{\u0026lt; /tab \u0026gt;}} {{\u0026lt; /tabs \u0026gt;}} Example MacOS MacOS This is tab MacOS content.\nLorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter! Oculis non ritibus Telethusa protulit, sed sed aere valvis inhaesuro Pallas animam: qui quid, ignes. Miseratus fonte Ditis conubia.\nLinux Linux This is tab Linux content.\nLorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter! Oculis non ritibus Telethusa protulit, sed sed aere valvis inhaesuro Pallas animam: qui quid, ignes. Miseratus fonte Ditis conubia.\nWindows Windows This is tab Windows content.\nLorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter! Oculis non ritibus Telethusa protulit, sed sed aere valvis inhaesuro Pallas animam: qui quid, ignes. Miseratus fonte Ditis conubia.\n "});})();